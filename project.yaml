metadata:
  version: 1.0.0
  timestamp: 2025-07-28 13:33:02.867633
  description: YAML snapshot of /Users/richardhightower/src/art_hug_11
  generator: yaml-project
  generator_version: 0.1.0
  author:
  tags:
  custom:
config:
  supported_extensions:
    .py: python
    .sh: bash
    .java: java
    .js: javascript
    .jsx: javascript
    .ts: typescript
    .tsx: typescript
    .html: html
    .css: css
    .md: markdown
    .yml: yaml
    .yaml: yaml
    .json: json
    .txt: text
    .go: go
    .rs: rust
    .rb: ruby
    .php: php
    .c: c
    .cpp: cpp
    .h: c
    .hpp: cpp
    .cs: csharp
    .toml: toml
    .xml: xml
    .sql: sql
    .kt: kotlin
    .swift: swift
    .dart: dart
    .r: r
    .scala: scala
    .pl: perl
    .lua: lua
    .ini: ini
    .cfg: ini
    .properties: properties
  forbidden_dirs:
    - __pycache__
    - node_modules
    - dist
    - cdk.out
    - env
    - venv
    - .venv
    - .idea
    - build
    - .git
    - .svn
    - .hg
    - .DS_Store
    - .vs
    - .vscode
    - target
    - bin
    - obj
    - out
    - Debug
    - Release
    - tmp
    - .tox
    - .pytest_cache
    - __MACOSX
    - .mypy_cache
    - tests
  include_pattern:
  exclude_pattern:
  outfile: /Users/richardhightower/src/art_hug_11/project.yaml
  log_level: INFO
  max_file_size: 204800
  config_dir: .yamlproject
  chunking_enabled: false
  chunk_size: 1048576
  temp_dir:
  backup_enabled: false
  backup_dir:
  metadata_fields:
    - extension
    - size_bytes
    - language
  yaml_format:
    indent: 2
    width: 120
tests:
content:
  files:
    pyproject.toml:
      content: |
        [tool.poetry]
        name = "advanced-techniques"
        version = "0.1.0"
        description = "Advanced Transformer Techniques - Working Examples"
        authors = ["Your Name <you@example.com>"]
        readme = "README.md"
        packages = [{include = "src"}]

        [tool.poetry.dependencies]
        python = "^3.12"
        transformers = "^4.36.0"
        torch = "^2.0.0"
        pyarrow = ">=11.0.0,<18.0.0"
        datasets = "^2.14.0"
        tokenizers = "^0.15.0"
        accelerate = "^0.26.0"
        evaluate = "^0.4.0"
        langdetect = "^1.0.9"
        peft = "^0.7.0"
        scipy = "^1.11.0"
        langchain = "^0.2.0"
        openai = "^1.0.0"
        anthropic = "^0.20.0"
        python-dotenv = "^1.0.0"
        pandas = "^2.1.0"
        numpy = "^1.26.0"

        [tool.poetry.group.dev.dependencies]
        pytest = "^8.0.0"
        black = "^24.0.0"
        ruff = "^0.6.0"
        jupyter = "^1.0.0"
        ipykernel = "^6.29.0"

        [build-system]
        requires = ["poetry-core"]
        build-backend = "poetry.core.masonry.api"

        [tool.black]
        line-length = 88
        target-version = ['py312']

        [tool.ruff]
        line-length = 88
        target-version = "py312"

        [tool.ruff.lint]
        select = ["E", "F", "I", "N", "UP", "B", "C4", "SIM"]
      metadata:
        extension: .toml
        size_bytes: 1004
        language: toml
    README.md:
      content: |
        # Advanced Transformer Techniques

        This project contains working examples for Chapter 11 of the Hugging Face Transformers book.

        ## Overview

        Learn how to implement and understand:

        ## Prerequisites

        - Python 3.12 (managed via pyenv)
        - Poetry for dependency management
        - Go Task for build automation
        - API keys for any required services (see .env.example)

        ## Setup

        1. Clone this repository
        2. Run the setup task:
           ```bash
           task setup
           ```
        3. Copy `.env.example` to `.env` and configure as needed

        ## Project Structure

        ```
        .
        ├── src/
        │   ├── __init__.py
        │   ├── config.py              # Configuration and utilities
        │   ├── main.py                # Entry point with all examples
        │   ├── prompt_engineering.py        # Prompt Engineering implementation
        │   ├── few_shot_learning.py        # Few Shot Learning implementation
        │   ├── chain_of_thought.py        # Chain Of Thought implementation
        │   ├── constitutional_ai.py        # Constitutional Ai implementation
        │   └── utils.py               # Utility functions
        ├── tests/
        │   └── test_examples.py       # Unit tests
        ├── .env.example               # Environment template
        ├── Taskfile.yml               # Task automation
        └── pyproject.toml             # Poetry configuration
        ```

        ## Running Examples

        Run all examples:
        ```bash
        task run
        ```

        Or run individual modules:
        ```bash
        task run-prompt-engineering    # Run prompt engineering
        task run-few-shot-learning    # Run few shot learning
        task run-chain-of-thought    # Run chain of thought
        ```

        ## Available Tasks

        - `task setup` - Set up Python environment and install dependencies
        - `task run` - Run all examples
        - `task test` - Run unit tests
        - `task format` - Format code with Black and Ruff
        - `task clean` - Clean up generated files

        ## Learn More

        - [Hugging Face Documentation](https://huggingface.co/docs)
        - [Transformers Library](https://github.com/huggingface/transformers)
        - [Book Resources](https://example.com/book-resources)
      metadata:
        extension: .md
        size_bytes: 2047
        language: markdown
    Taskfile.yml:
      content: |
        version: '3'

        vars:
          PYTHON_VERSION: 3.12.9

        tasks:
          default:
            desc: "Show available tasks"
            cmds:
              - task --list

          setup:
            desc: "Set up the Python environment and install dependencies"
            cmds:
              - pyenv install -s {{.PYTHON_VERSION}}
              - pyenv local {{.PYTHON_VERSION}}
              - poetry install
              - poetry config virtualenvs.in-project true
              - 'echo "Setup complete! Activate with: source .venv/bin/activate"'

          run:
            desc: "Run all examples"
            cmds:
              - poetry run python src/main.py

          run-prompt-engineering:
            desc: "Run prompt engineering examples"
            cmds:
              - poetry run python src/prompt_engineering.py

          run-few-shot-learning:
            desc: "Run few shot learning examples"
            cmds:
              - poetry run python src/few_shot_learning.py

          run-chain-of-thought:
            desc: "Run chain of thought examples"
            cmds:
              - poetry run python src/chain_of_thought.py

          run-data-curation:
            desc: "Run data curation examples"
            cmds:
              - poetry run python src/data_curation.py

          run-tokenization:
            desc: "Run tokenization examples"
            cmds:
              - poetry run python src/tokenization.py

          run-model-configuration:
            desc: "Run model configuration examples"
            cmds:
              - poetry run python src/model_configuration.py

          run-training:
            desc: "Run training workflow examples"
            cmds:
              - poetry run python src/training_workflow.py

          run-constitutional-ai:
            desc: "Run constitutional AI examples"
            cmds:
              - poetry run python src/constitutional_ai.py

          notebook:
            desc: "Run the tutorial notebook"
            cmds:
              - poetry run jupyter notebook notebooks/tutorial.ipynb

          notebook-lab:
            desc: "Run the tutorial notebook with JupyterLab"
            cmds:
              - poetry run jupyter lab notebooks/tutorial.ipynb

          test:
            desc: "Run all tests"
            cmds:
              - poetry run pytest tests/ -v

          format:
            desc: "Format code with Black and Ruff"
            cmds:
              - poetry run black src/ tests/
              - poetry run ruff check --fix src/ tests/

          clean:
            desc: "Clean up generated files"
            cmds:
              - find . -type d -name "__pycache__" -exec rm -rf {} +
              - find . -type f -name "*.pyc" -delete
              - rm -rf .pytest_cache
              - rm -rf .ruff_cache
              - rm -rf .mypy_cache
      metadata:
        extension: .yml
        size_bytes: 2277
        language: yaml
    ch_11_project.yaml:
      content: |
        metadata:
          version: "1.0.0"
          timestamp: "2025-07-01T22:50:15.158733"
          description: "Chapter 11: Dataset Curation and Training Language Models from Scratch"
          generator: "yaml-project"
          generator_version: "0.1.0"
          author: "Hugging Face Transformers Book - Chapter 11"
          tags:
            - "transformers"
            - "nlp"
            - "chapter-11"
            - "example"

        content:
          files:
            pyproject.toml:
              content: |
                [tool.poetry]
                name = "advanced-techniques"
                version = "0.1.0"
                description = "Advanced Transformer Techniques - Working Examples"
                authors = ["Your Name <you@example.com>"]
                readme = "README.md"
                packages = [{include = "src"}]

                [tool.poetry.dependencies]
                python = "^3.12"
                transformers = "^{latest}"
                langchain = "^{latest}"
                openai = "^{latest}"
                anthropic = "^{latest}"
                python-dotenv = "^1.0.0"
                pandas = "^2.1.0"
                numpy = "^1.26.0"

                [tool.poetry.group.dev.dependencies]
                pytest = "^8.0.0"
                black = "^24.0.0"
                ruff = "^0.6.0"
                jupyter = "^1.0.0"
                ipykernel = "^6.29.0"

                [build-system]
                requires = ["poetry-core"]
                build-backend = "poetry.core.masonry.api"

                [tool.black]
                line-length = 88
                target-version = ['py312']

                [tool.ruff]
                line-length = 88
                target-version = "py312"

                [tool.ruff.lint]
                select = ["E", "F", "I", "N", "UP", "B", "C4", "SIM"]
              metadata:
                extension: .toml
                size_bytes: 1000
                language: toml
            README.md:
              content: |
                # Advanced Transformer Techniques

                This project contains working examples for Chapter 11 of the Hugging Face Transformers book.

                ## Overview

                Learn how to implement and understand:

                ## Prerequisites

                - Python 3.12 (managed via pyenv)
                - Poetry for dependency management
                - Go Task for build automation
                - API keys for any required services (see .env.example)

                ## Setup

                1. Clone this repository
                2. Run the setup task:
                   ```bash
                   task setup
                   ```
                3. Copy `.env.example` to `.env` and configure as needed

                ## Project Structure

                ```
                .
                ├── src/
                │   ├── __init__.py
                │   ├── config.py              # Configuration and utilities
                │   ├── main.py                # Entry point with all examples
                │   ├── prompt_engineering.py        # Prompt Engineering implementation
                │   ├── few_shot_learning.py        # Few Shot Learning implementation
                │   ├── chain_of_thought.py        # Chain Of Thought implementation
                │   ├── constitutional_ai.py        # Constitutional Ai implementation
                │   └── utils.py               # Utility functions
                ├── tests/
                │   └── test_examples.py       # Unit tests
                ├── .env.example               # Environment template
                ├── Taskfile.yml               # Task automation
                └── pyproject.toml             # Poetry configuration
                ```

                ## Running Examples

                Run all examples:
                ```bash
                task run
                ```

                Or run individual modules:
                ```bash
                task run-prompt-engineering    # Run prompt engineering
                task run-few-shot-learning    # Run few shot learning
                task run-chain-of-thought    # Run chain of thought
                ```

                ## Available Tasks

                - `task setup` - Set up Python environment and install dependencies
                - `task run` - Run all examples
                - `task test` - Run unit tests
                - `task format` - Format code with Black and Ruff
                - `task clean` - Clean up generated files

                ## Learn More

                - [Hugging Face Documentation](https://huggingface.co/docs)
                - [Transformers Library](https://github.com/huggingface/transformers)
                - [Book Resources](https://example.com/book-resources)
              metadata:
                extension: .md
                size_bytes: 2000
                language: markdown
            Taskfile.yml:
              content: |
                version: '3'

                vars:
                  PYTHON_VERSION: 3.12.9

                tasks:
                  default:
                    desc: "Show available tasks"
                    cmds:
                      - task --list

                  setup:
                    desc: "Set up the Python environment and install dependencies"
                    cmds:
                      - pyenv install -s {{.PYTHON_VERSION}}
                      - pyenv local {{.PYTHON_VERSION}}
                      - poetry install
                      - poetry config virtualenvs.in-project true
                      - 'echo "Setup complete! Activate with: source .venv/bin/activate"'

                  run:
                    desc: "Run all examples"
                    cmds:
                      - poetry run python src/main.py

                  run-prompt-engineering:
                    desc: "Run prompt engineering examples"
                    cmds:
                      - poetry run python src/prompt_engineering.py

                  run-few-shot-learning:
                    desc: "Run few shot learning examples"
                    cmds:
                      - poetry run python src/few_shot_learning.py

                  run-chain-of-thought:
                    desc: "Run chain of thought examples"
                    cmds:
                      - poetry run python src/chain_of_thought.py

                  test:
                    desc: "Run all tests"
                    cmds:
                      - poetry run pytest tests/ -v

                  format:
                    desc: "Format code with Black and Ruff"
                    cmds:
                      - poetry run black src/ tests/
                      - poetry run ruff check --fix src/ tests/

                  clean:
                    desc: "Clean up generated files"
                    cmds:
                      - find . -type d -name "__pycache__" -exec rm -rf {} +
                      - find . -type f -name "*.pyc" -delete
                      - rm -rf .pytest_cache
                      - rm -rf .ruff_cache
                      - rm -rf .mypy_cache
              metadata:
                extension: .yml
                size_bytes: 1200
                language: yaml
            src/__init__.py:
              content: |
                """
                Chapter 11 Examples: Advanced Transformer Techniques
                """

                __version__ = "0.1.0"
              metadata:
                extension: .py
                size_bytes: 100
                language: python

            src/config.py:
              content: |
                """Configuration module for examples."""
                
                import os
                from pathlib import Path
                from dotenv import load_dotenv
                
                # Load environment variables
                load_dotenv()
                
                # Project paths
                PROJECT_ROOT = Path(__file__).parent.parent
                DATA_DIR = PROJECT_ROOT / "data"
                MODELS_DIR = PROJECT_ROOT / "models"
                
                # Create directories if they don't exist
                DATA_DIR.mkdir(exist_ok=True)
                MODELS_DIR.mkdir(exist_ok=True)
                
                # Model configurations
                DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "bert-base-uncased")
                BATCH_SIZE = int(os.getenv("BATCH_SIZE", "8"))
                MAX_LENGTH = int(os.getenv("MAX_LENGTH", "512"))
                
                # API keys (if needed)
                OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
                ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
                HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
                
                # Device configuration
                import torch
                
                def get_device():
                    """Get the best available device."""
                    if torch.backends.mps.is_available():
                        return "mps"
                    elif torch.cuda.is_available():
                        return "cuda"
                    else:
                        return "cpu"
                        
                DEVICE = get_device()
              metadata:
                extension: .py
                size_bytes: 1000
                language: python

            src/main.py:
              content: |
                """Main entry point for all examples."""
                
                import sys
                from pathlib import Path
                
                # Add src to path
                sys.path.append(str(Path(__file__).parent))
                
                from prompt_engineering import run_prompt_engineering_examples
                from few_shot_learning import run_few_shot_learning_examples
                from chain_of_thought import run_chain_of_thought_examples
                from constitutional_ai import run_constitutional_ai_examples
                
                def print_section(title: str):
                    """Print a formatted section header."""
                    print("\n" + "=" * 60)
                    print(f"  {title}")
                    print("=" * 60 + "\n")
                
                def main():
                    """Run all examples."""
                    print_section("CHAPTER 11: ADVANCED TRANSFORMER TECHNIQUES")
                    print("Welcome! This script demonstrates the concepts from this chapter.")
                    print("Each example builds on the previous concepts.\n")
                    
                    print_section("1. PROMPT ENGINEERING")
                    run_prompt_engineering_examples()
                    
                    print_section("2. FEW SHOT LEARNING")
                    run_few_shot_learning_examples()
                    
                    print_section("3. CHAIN OF THOUGHT")
                    run_chain_of_thought_examples()
                    
                    print_section("CONCLUSION")
                    print("These examples demonstrate the key concepts from this chapter.")
                    print("Try modifying the code to experiment with different approaches!")
                
                if __name__ == "__main__":
                    main()
              metadata:
                extension: .py
                size_bytes: 1500
                language: python
            .env.example:
              content: |
                # Model Configuration
                DEFAULT_MODEL=bert-base-uncased
                BATCH_SIZE=8
                MAX_LENGTH=512
                
                # API Keys (optional, depending on chapter)
                OPENAI_API_KEY=your-openai-key-here
                ANTHROPIC_API_KEY=your-anthropic-key-here
                HUGGINGFACE_TOKEN=your-hf-token-here
                
                # Other Configuration
                LOG_LEVEL=INFO
                CACHE_DIR=~/.cache/transformers
              metadata:
                extension: .example
                size_bytes: 300
                language: text

            .gitignore:
              content: |
                # Python
                __pycache__/
                *.py[cod]
                *$py.class
                *.so
                .Python
                env/
                venv/
                .venv/
                ENV/
                .env
                
                # Poetry
                dist/
                *.egg-info/
                
                # Testing
                .pytest_cache/
                .coverage
                htmlcov/
                .tox/
                
                # IDE
                .idea/
                .vscode/
                *.swp
                *.swo
                .DS_Store
                
                # Project specific
                data/
                models/
                *.log
                .cache/
              metadata:
                extension: .gitignore
                size_bytes: 300
                language: text

            .python-version:
              content: |
                3.12.9
              metadata:
                extension: .python-version
                size_bytes: 7
                language: text
            src/prompt_engineering.py:
              content: |
                """Prompt Engineering implementation."""
                
                from transformers import pipeline, AutoTokenizer, AutoModel
                import torch
                from config import get_device, DEFAULT_MODEL
                
                def run_prompt_engineering_examples():
                    """Run prompt engineering examples."""
                    
                    print(f"Loading model: {DEFAULT_MODEL}")
                    device = get_device()
                    print(f"Using device: {device}")
                    
                    # Example implementation
                    tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)
                    model = AutoModel.from_pretrained(DEFAULT_MODEL)
                    
                    # Example text
                    text = "Hugging Face Transformers make NLP accessible to everyone!"
                    
                    # Tokenize
                    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                    
                    print(f"\nInput text: {text}")
                    print(f"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())}")
                    print(f"Token IDs: {inputs['input_ids'][0].tolist()}")
                    
                    # Get model outputs
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    print(f"\nModel output shape: {outputs.last_hidden_state.shape}")
                    print("Example completed successfully!")
                    
                if __name__ == "__main__":
                    print("=== Prompt Engineering Examples ===\n")
                    run_prompt_engineering_examples()
              metadata:
                extension: .py
                size_bytes: 1200
                language: python
            tests/test_examples.py:
              content: |
                """Unit tests for Chapter 11 examples."""
                
                import pytest
                import sys
                from pathlib import Path
                
                # Add src to path
                sys.path.append(str(Path(__file__).parent.parent / "src"))
                
                from config import get_device
                from prompt_engineering import run_prompt_engineering_examples
                
                def test_device_detection():
                    """Test that device detection works."""
                    device = get_device()
                    assert device in ["cpu", "cuda", "mps"]
                    
                def test_prompt_engineering_runs():
                    """Test that prompt_engineering examples run without errors."""
                    # This is a basic smoke test
                    try:
                        run_prompt_engineering_examples()
                    except Exception as e:
                        pytest.fail(f"prompt_engineering examples failed: {e}")
                        
                def test_imports():
                    """Test that all required modules can be imported."""
                    import transformers
                    import torch
                    import numpy
                    import pandas
                    
                    assert transformers.__version__
                    assert torch.__version__
              metadata:
                extension: .py
                size_bytes: 800
                language: python
      metadata:
        extension: .yaml
        size_bytes: 13855
        language: yaml
    CLAUDE.md:
      content: |-
        # CLAUDE.md

        This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

        ## Commands

        ### Setup and Installation
        - `task setup` - Sets up Python 3.12.9 environment and installs all dependencies via Poetry

        ### Running Examples
        - `task run` - Runs all examples from src/main.py
        - `task run-prompt-engineering` - Runs prompt engineering examples only
        - `task run-few-shot-learning` - Runs few shot learning examples only
        - `task run-chain-of-thought` - Runs chain of thought examples only

        ### Development Commands
        - `task test` - Runs all tests with pytest
        - `task format` - Formats code with Black (line-length: 88) and Ruff
        - `task clean` - Cleans up generated files and caches

        ### Single Test Execution
        - `pytest tests/test_examples.py::test_specific_function` - Run a specific test function
        - `pytest -k "keyword"` - Run tests matching a keyword pattern

        ## Architecture

        This project demonstrates advanced transformer techniques from Chapter 11 of the Hugging Face Transformers book, focusing on dataset curation and training language models from scratch.

        ### Project Structure
        - **src/** - Main source code
          - `config.py` - Configuration utilities with environment variable loading and device selection (CPU/CUDA/MPS)
          - `main.py` - Entry point that orchestrates all examples
          - `prompt_engineering.py` - Implementation of prompt engineering techniques
          
        - **tests/** - Unit tests using pytest
        - **docs/article11.md** - Comprehensive guide on dataset curation (1000+ lines)

        ### Key Patterns
        1. **Environment Configuration**: Uses python-dotenv for API keys (OpenAI, Anthropic, HuggingFace)
        2. **Device Selection**: Automatic detection of available compute (CUDA, MPS, CPU) in config.py
        3. **Task Automation**: Uses Taskfile.yml instead of Makefile for cross-platform compatibility
        4. **Poetry for Dependencies**: Modern Python dependency management with lock file

        ### Technology Stack
        - Python 3.12.9 (managed via pyenv)
        - Hugging Face Transformers 4.53.3
        - LangChain 0.2.17 for LLM application framework
        - PyTorch (via transformers) for deep learning
        - OpenAI and Anthropic clients for API access

        ### Code Style
        - Black formatter with 88 character line length
        - Ruff linter with rules: E, F, I, N, UP, B, C4, SIM
        - Target Python version: 3.12
      metadata:
        extension: .md
        size_bytes: 2301
        language: markdown
    .claude/settings.local.json:
      content: |-
        {
          "permissions": {
            "allow": [
              "Bash(task setup)",
              "Bash(mkdir:*)",
              "Bash(poetry install:*)",
              "Bash(poetry lock:*)",
              "Bash(task run)",
              "Bash(task run-tokenization:*)",
              "Bash(rm:*)",
              "Bash(ls:*)",
              "Bash(find:*)",
              "Bash(python:*)",
              "Bash(grep:*)"
            ],
            "deny": []
          }
        }
      metadata:
        extension: .json
        size_bytes: 343
        language: json
    docs/briefing.md:
      content: |+
        Here’s a well-formatted version of your **briefing document** with clear structure, consistent headings, and bullet points for readability:

        ---

        # 📘 Detailed Briefing Document: Building Custom Language Models

        This document reviews the key ideas, themes, and actionable facts from *"Building Custom Language Models: From Data to AI Solutions."* It covers the full lifecycle—from data curation to training—emphasizing modern techniques and the Hugging Face ecosystem.

        ---

        ## I. Executive Summary

        Building effective custom language models (LLMs) depends critically on **high-quality, domain-specific data**. While full model training from scratch is rare due to cost and complexity, **fine-tuning pre-trained models** using Parameter-Efficient Fine-Tuning (PEFT) on curated datasets is now the standard.

        Key pillars for success:

        * 📊 **High-quality data**
        * ☁️ **Scalable, privacy-aware processing**
        * 🔁 **Reproducible workflows**
        * 📈 **Iterative refinement and monitoring**

        ---

        ## II. Main Themes and Most Important Ideas

        ### 1. **Data as the Foundation ("Garbage in, garbage out")**

        * Model quality is constrained by data quality.
        * Emphasis on **relevance, diversity, and cleanliness**.
        * Quote: *"Even the most sophisticated AI architecture can't salvage a flawed foundation."*

        ### 2. **Fine-Tuning is the Dominant Strategy**

        * Pre-trained models (e.g., GPT, BERT, Llama) are fine-tuned on domain-specific data.
        * Benefits: **faster, cheaper, and task-aligned**.
        * Full training from scratch is reserved for large orgs with specialized needs.

        ### 3. **Comprehensive Data Curation**

        * Goes beyond scraping:

          * Relevant, diverse sources
          * Text cleaning & normalization
          * Deduplication (exact + semantic)
          * Annotation & labeling
          * Tokenization
          * Versioning
        * Hugging Face Datasets is the go-to tool.
        * LLMs are now used in **cleaning and labeling pipelines**.

        ### 4. **Scalable Data Processing**

        * **Streaming and batching** are essential for large datasets.
        * Integration with:

          * Hugging Face Datasets
          * Ray, Spark, Kafka
          * Cloud object stores (e.g., S3, GCS)

        ### 5. **Privacy and Reproducibility**

        * Essential for trust and compliance:

          * **Data versioning**: DVC, LakeFS, Delta Lake
          * **Annotation tracking**
          * **Dataset cards** to document preprocessing
          * **PII redaction**: regex, LLMs, anonymization
          * **Encryption and access control**

        ### 6. **Strategic Model Configuration**

        * Match architecture to task:

          * Encoder-only (e.g., BERT) for understanding
          * Decoder-only (e.g., GPT) for generation
          * Encoder-decoder (e.g., T5, BART) for sequence-to-sequence
        * Key config parameters: `vocab_size`, `max_position_embeddings`, `n_layer`, `n_head`, etc.

        ### 7. **Parameter-Efficient Fine-Tuning (PEFT)**

        * PEFT methods (LoRA, Adapters, Prefix Tuning):

          * Enable fine-tuning large models with fewer trainable parameters
          * Example: Mistral-7B reduced to \~7M trainable parameters using LoRA

        ### 8. **Iterative Training and Evaluation**

        * Continuous monitoring:

          * Metrics: Loss, perplexity, F1, BLEU, ROUGE
          * Tools: TensorBoard, Weights & Biases, Evaluate
        * Use:

          * `EarlyStoppingCallback`
          * `save_steps` for checkpoints
          * Error analysis tools like **Argilla**

        ### 9. **Distributed and Mixed-Precision Training**

        * Hugging Face `accelerate` simplifies multi-GPU training
        * Add-ons: DeepSpeed, FairScale
        * Enable `fp16=True` for mixed precision to reduce memory and speed up training

        ---

        ## III. Key Facts and Practical Implementations

        ### 🛠 Custom LLMs:

        * Provide competitive edge in **domain-specific applications**.

        ### 🤗 Hugging Face Ecosystem:

        * Preferred for:

          * Model access
          * Data management
          * Training orchestration

        ### 🧼 Data Cleaning Techniques:

        * Remove:

          * HTML tags
          * URLs
          * Duplicate entries (exact/semantic)
        * Normalize whitespace
        * Use LLMs for advanced cleaning
        * **Code Example**: Using `re.sub` and `dataset.map()` for transformations

        ### 🧪 Tokenization:

        * Converts text to tokens:

          * Use AutoTokenizer for common models
          * Use SentencePiece/BPE for custom tokenizers

        ### 📂 Data Versioning:

        * Tools:

          * [ ] DVC
          * [ ] LakeFS
          * [ ] Delta Lake

        ### 🔐 PII Redaction:

        * Tools:

          * Regex
          * `presidio-analyzer`
          * Custom LLM pipelines

        ### ⚙️ Model Architectures:

        | Architecture    | Use Case                         |
        | --------------- | -------------------------------- |
        | Encoder-only    | Classification, NER (e.g., BERT) |
        | Decoder-only    | Text/code generation (e.g., GPT) |
        | Encoder-decoder | Translation, summarization (T5)  |

        ### 🔧 Model Config Parameters:

        * `vocab_size`, `n_embd`, `max_position_embeddings`, `n_layer`, `n_head`
        * After tokenizer updates: `model.resize_token_embeddings(len(tokenizer))`

        ### 🧠 PEFT Setup Example:

        ```python
        from transformers import BitsAndBytesConfig, LoraConfig

        bnb_config = BitsAndBytesConfig(load_in_4bit=True)
        peft_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
        ```

        ### 🏗 Training Setup:

        ```python
        from transformers import TrainingArguments

        training_args = TrainingArguments(
            output_dir="./checkpoints",
            evaluation_strategy="steps",
            logging_steps=50,
            save_steps=200,
            per_device_train_batch_size=8,
            num_train_epochs=3,
            fp16=True,
            report_to=["tensorboard", "wandb"]
        )
        ```

        ### 📊 Evaluation & Monitoring:

        * Use Hugging Face `evaluate` library for standardized metrics
        * Log metrics to:

          * TensorBoard
          * Weights & Biases
          * MLflow, Neptune

        ### 🧪 Early Stopping & Checkpointing:

        ```python
        from transformers import EarlyStoppingCallback

        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        ```

        ### 🚀 Distributed Training:

        * Use:

          ```bash
          accelerate config
          accelerate launch train.py
          ```
        * For large models: DeepSpeed or FairScale with ZeRO optimizations

        ---

        ## IV. Conclusion

        This briefing distills practical insights for building custom LLMs:

        * 🔍 Start with **clean, high-quality data**
        * 🧠 Focus on **fine-tuning**, not training from scratch
        * 🧰 Leverage **modern tools** like Hugging Face, PEFT, Accelerate
        * 🔐 Ensure **privacy, reproducibility**, and compliance
        * 📈 Track, monitor, and iterate your training

        By adopting these best practices, teams can build reliable, high-performing, domain-adapted LLMs that deliver real business value.

        ---

        Let me know if you'd like a version exported as PDF or tailored for slides/briefing decks.

        ---

        # Detailed Briefing Document: Building Custom Language Models
        This briefing document reviews the main themes, important ideas, and key facts presented in the provided source, "Building Custom Language Models From Data to AI Solutions." It focuses on the complete lifecycle of developing custom language models, from data curation to training and refinement, emphasizing modern techniques and the Hugging Face ecosystem.

        ## I. Executive Summary
        Building effective custom language models (LLMs) hinges critically on high-quality, domain-specific data. While training models from scratch is resource-intensive and less common for most business applications, understanding the foundational principles of data curation, model configuration, and iterative training is essential. The modern workflow heavily emphasizes fine-tuning pre-trained models using efficient methods like Parameter-Efficient Fine-Tuning (PEFT) on meticulously prepared data. Key pillars for successful LLM development include data quality, scalable and privacy-aware processing, reproducible workflows, and iterative model refinement using robust monitoring and evaluation.

        ## II. Main Themes and Most Important Ideas
        The source highlights several core themes crucial for developing custom language models:

        Data as the Foundation ("Garbage in, garbage out"): The quality, relevance, diversity, and freshness of the dataset are paramount. "Even the most sophisticated AI architecture can't salvage a flawed foundation." High-quality data is "the first—and most critical—step in building effective language models."
        The Dominance of Fine-Tuning: For most practical applications, fine-tuning a pre-trained model (e.g., GPT, BERT, Llama) on domain-specific data is the recommended approach. This is "efficient, cost-effective, and enables rapid adaptation to unique business, user, or privacy requirements." Training from scratch is reserved for "organizations with massive datasets and compute resources, or highly specialized applications demanding complete control."
        Modern Data Curation is Comprehensive: Curation goes beyond simple collection, demanding "selecting relevant, diverse sources strategically," "cleaning and standardizing text meticulously," "removing duplicates and noise," "annotating and labeling," "tokenizing," and "versioning and tracking your data for reproducibility." The Hugging Face Datasets library is a cornerstone for scalable, memory-efficient data handling. LLMs themselves are increasingly used for "advanced data cleaning, deduplication, and even initial annotation."
        Scalable and Efficient Data Processing: For large datasets, "streaming and batching" are essential. Hugging Face Datasets supports this, allowing processing "one record at a time, or in manageable batches, keeping only a small portion in memory." Integration with cloud storage and distributed frameworks (Ray, Spark, Kafka) is crucial for production.
        Privacy and Reproducibility are Non-Negotiable: "In professional AI projects, you must know exactly what data went into your model and how it was processed." This requires "version control for data" (e.g., DVC, LakeFS, Delta Lake), "annotation tracking," and documenting all preprocessing steps in "dataset cards." For sensitive data, "PII detection and removal," "anonymization," "differential privacy," and "access controls and encryption" are mandatory.
        Strategic Model Configuration: Choosing the correct model architecture (encoder-only, decoder-only, encoder-decoder) for the task is vital. "Modern workflows overwhelmingly favor fine-tuning pre-trained models." When fine-tuning, key parameters like vocab_size and max_position_embeddings must be carefully aligned.
        Parameter-Efficient Fine-Tuning (PEFT) is Standard: Methods like LoRA, Prefix Tuning, and Adapters are "standards for large models and can dramatically reduce compute and memory requirements." PEFT can reduce trainable parameters significantly (e.g., Mistral-7B from 7B to ~7M).
        Iterative Training and Evaluation: Training is an iterative process. "Monitor loss, perplexity, and task-specific metrics with robust tools like TensorBoard, Weights & Biases, and the Evaluate library." "Early stopping" prevents overtraining, and "checkpointing" protects progress. "Error analysis" through manual inspection and systematic tools (like Argilla) drives continuous improvement.
        Distributed Training for Scale: For large models and datasets, "multi-GPU and distributed training enable you to train faster and scale to larger models." Hugging Face Accelerate simplifies this, often integrated with DeepSpeed or FairScale for advanced memory optimizations (ZeRO optimizations, gradient checkpointing).

        ## III. Key Facts and Practical Implementations
        Custom Language Models: Provide a "critical competitive advantage" for "specific domains and tasks."
        Hugging Face Ecosystem: The primary toolkit recommended for its "efficiency, scalability, and model quality."
        Data Cleaning: Involves removing HTML tags, URLs, normalizing whitespace, deduplication (exact and semantic), and often leverages LLMs for advanced tasks.
        Example: Python code snippet demonstrates re.sub and dataset.map() with Hugging Face Datasets for cleaning.
        Tokenization: Converts raw text into model-friendly units. Custom tokenizers (e.g., SentencePiece Unigram, BPE) are crucial for "specialized or multilingual data." Hugging Face AutoTokenizer is the standard API.
        Data Versioning: Tools like DVC, LakeFS, or Databricks Delta Lake provide Git-like tracking for datasets.
        PII Redaction: Automated tools (e.g., presidio-analyzer, LLMs) are used for scanning and replacing sensitive information like emails, phone numbers, and names.
        Example: Regex-based Python function for basic PII redaction.
        Model Architectures:Encoder-only (BERT): Understanding tasks (classification, NER).
        Decoder-only (GPT): Generative tasks (text, code).
        Encoder-decoder (T5, BART): Sequence-to-sequence (translation, summarization).
        Model Configuration Parameters: vocab_size, max_position_embeddings, n_embd, n_layer, n_head, use_cache.
        Always ensure vocab_size matches the tokenizer's vocabulary.
        model.resize_token_embeddings(len(tokenizer)) is critical when adding new tokens.
        PEFT Example (Mistral-7B): Demonstrated using BitsAndBytesConfig for 4-bit quantization and LoraConfig to apply LoRA, drastically reducing trainable parameters.
        Training Setup: Uses Hugging Face Trainer and TrainingArguments to define output_dir, evaluation_strategy, logging_steps, save_steps, per_device_train_batch_size, num_train_epochs.
        Experiment Tracking: report_to=["tensorboard", "wandb"] enables seamless logging with popular tools. MLflow, Neptune, and Hugging Face Hub are also supported.
        Evaluation Metrics: Beyond loss and perplexity, task-specific metrics like Accuracy, F1-score, BLEU, ROUGE, and human-centric metrics are important. Hugging Face evaluate library provides standardized computation.
        Early Stopping and Checkpointing: EarlyStoppingCallback(early_stopping_patience=N) in Trainer prevents overfitting, while save_steps in TrainingArguments saves model progress.
        Distributed Training: Hugging Face accelerate simplifies multi-GPU and multi-node training. accelerate config and accelerate launch train.py are key commands. Integration with DeepSpeed or FairScale is recommended for very large models.
        Mixed-Precision Training: fp16=True in TrainingArguments (or PyTorch's native AMP) accelerates training and reduces memory usage.

        # IV. Conclusion
        The provided source offers a comprehensive and practical guide to building custom language models. Its emphasis on data quality, modern fine-tuning techniques (especially PEFT), scalable processing, and robust experimental workflows aligns with current best practices in the AI field. By mastering these foundations, practitioners can effectively transform raw data into powerful, domain-specific AI solutions, preparing them for advanced challenges in model deployment and responsible AI development.

      metadata:
        extension: .md
        size_bytes: 14487
        language: markdown
    docs/faq.md:
      content: |
        Here is your content **cleanly formatted** into a structured, readable layout while preserving all your original wording exactly:

        ---

        # **Building Custom Language Models: Key Concepts & Insights**

        ---

        ### **1. Why is data quality considered the most critical factor in building effective language models?**

        Data quality is paramount because it directly dictates the model's performance and accuracy; "garbage in, garbage out" is a fundamental truth in AI. A model trained on messy, biased, or irrelevant data will inevitably reflect those flaws. High-quality, domain-specific data enables the model to understand nuanced contexts, industry-specific terms, and unique requirements, transforming it from a general assistant into a specialist. For instance, a financial company's model would be greatly enhanced by a dataset of financial documents rather than generic text. Curating relevant, diverse, clean, and appropriately labeled data is the foundational first step that ensures the model learns effectively and generalizes well.

        ---

        ### **2. What are the key steps involved in modern data curation for language models, and what tools are commonly used?**

        Modern data curation is a comprehensive process that goes beyond simple collection. The key steps include:

        * **Source Selection:** Strategically choosing relevant and diverse data sources that match the target use case (e.g., legal documents for a legal AI).
        * **Text Cleaning:** Meticulously removing noise such as HTML tags, URLs, boilerplate text, inconsistent spacing, and potentially sensitive or offensive content.
        * **Deduplication:** Eliminating exact and semantic duplicates to prevent model overfitting and reduce redundancy.
        * **Annotation/Labeling:** For many tasks, adding high-quality labels (e.g., sentiment, intent) often involving human-in-the-loop workflows to capture subtleties.
        * **Tokenization:** Splitting text into model-friendly units (tokens) and building a custom vocabulary tailored to the domain.
        * **Versioning and Tracking:** Documenting and tracking every change to the dataset for reproducibility and auditability.

        Tools like the Hugging Face datasets library are essential for scalable, memory-efficient data loading and transformation, supporting streaming and batch processing. For human-in-the-loop annotation and versioning, tools like Argilla are recommended. LLMs themselves are increasingly used for advanced cleaning, deduplication, and even synthetic data generation.

        ---

        ### **3. What is the difference between training a language model from scratch and fine-tuning a pre-trained model, and when would you choose each approach?**

        Training from scratch involves building a model with randomly initialized weights and training it on a new dataset from the ground up. This approach is highly resource-intensive in terms of compute and data. It is typically chosen by organizations with:

        * Massive, unique datasets (e.g., a new language).
        * Specific requirements for a fully custom architecture.
        * The need for complete control over every aspect of the model, including data residency and privacy, where no pre-trained model suffices.
        * Foundational research purposes.

        Fine-tuning a pre-trained model involves taking an existing model (like BERT, GPT, or Llama) that has already learned general language patterns from vast, diverse datasets, and then adapting it to a specific domain or task using a smaller, domain-specific dataset. This is the overwhelmingly preferred and more efficient approach for most practical applications because it:

        * Leverages existing knowledge, dramatically reducing compute requirements.
        * Is cost-effective and enables rapid adaptation.
        * Allows for seamless inclusion of industry-specific vocabulary.
        * Facilitates filtering sensitive content and meeting privacy requirements.

        For example, a healthcare provider would fine-tune a pre-trained model on anonymized clinical notes to understand medical jargon, rather than training a model from scratch.

        ---

        ### **4. How do modern workflows handle large-scale datasets and ensure data privacy and reproducibility?**

        Modern workflows use several strategies to manage large-scale data and uphold privacy and reproducibility:

        * **Streaming and Batch Processing:** Tools like Hugging Face datasets enable processing data one record at a time or in manageable batches, keeping only small portions in memory. This is crucial for handling terabyte-sized datasets without exhausting system resources and allows direct processing from cloud storage.
        * **Versioning Control for Data:** Tools such as DVC (Data Version Control), LakeFS, or Databricks Delta Lake track every change to the dataset, similar to how Git tracks code. This ensures that the exact data used for any model version can be reproduced.
        * **Experiment Tracking:** Platforms like MLflow, Weights & Biases (W\&B), and Hugging Face Hub log configurations, metrics, and model artifacts, linking data versions to specific training runs.
        * **Data Privacy and Security:** This involves detecting and removing or masking Personally Identifiable Information (PII) using automated tools (e.g., presidio, LLM-powered detection) or regular expressions. Anonymization, differential privacy, synthetic data generation, strict access controls, and encryption (at rest and in transit) are also critical, especially for regulated data (GDPR, HIPAA).

        These practices ensure transparency, traceability, and compliance, which are essential for robust and responsible AI development.

        ---

        ### **5. What are Parameter-Efficient Fine-Tuning (PEFT) methods, and why are they important for modern LLM development?**

        Parameter-Efficient Fine-Tuning (PEFT) methods are a set of techniques (e.g., LoRA, QLoRA, Prefix Tuning, Adapters) that allow for adapting large language models (LLMs) to new tasks with significantly reduced computational and memory overhead. Instead of fine-tuning the entire model (which can have billions of parameters), PEFT methods only update a small subset of the model's parameters, or introduce a small number of new, trainable parameters.

        Their importance stems from several factors:

        * **Reduced Compute & Memory:** They dramatically cut down the GPU memory and computational power required for fine-tuning, making it feasible to adapt large models on consumer-grade GPUs.
        * **Faster Training:** Training only a small fraction of parameters is much quicker.
        * **Smaller Checkpoints:** The resulting fine-tuned models are much smaller, as they only store the small set of adapter weights instead of the full model.
        * **Maintaining Performance:** Despite updating fewer parameters, PEFT methods often achieve performance comparable to full fine-tuning.

        For instance, applying LoRA to a 7-billion-parameter model can reduce the trainable parameters from billions to just a few million, enabling efficient fine-tuning.

        ---

        ### **6. How are models configured and initialized for training, especially in the context of using pre-trained models?**

        Model configuration and initialization involve several key steps using libraries like Hugging Face Transformers:

        * **Architecture Selection:** Choosing the appropriate model architecture (encoder-only for understanding tasks, decoder-only for generation, encoder-decoder for sequence-to-sequence tasks) based on the specific NLP task.
        * **Hyperparameter Setting:** Defining core parameters such as `vocab_size` (matching the tokenizer's vocabulary), `max_position_embeddings` (maximum sequence length), `n_embd` (embedding dimension), `n_layer` (number of transformer layers), and `n_head` (attention heads).
        * **Loading Pre-trained Models:** The standard and most efficient practice is to load a pre-trained model (e.g., `AutoModelForCausalLM.from_pretrained("model_name")`). These models already have learned weights from massive datasets, providing a strong foundation.
        * **Tokenizer Alignment:** Ensuring the model's `vocab_size` matches that of the tokenizer. If new tokens are added to the tokenizer, the model's embedding layer must be resized (`model.resize_token_embeddings(len(tokenizer))`) to accommodate them.
        * **Initialization (when training from scratch):** In the rare case of training from scratch, Hugging Face and PyTorch use robust random initialization schemes for transformer layers. However, for domain adaptation, PEFT methods or pre-trained models are generally preferred over custom embedding initialization.

        Modern workflows also emphasize configuration-driven training (using YAML or JSON files) and parameter-efficient fine-tuning (PEFT) for efficient adaptation.

        ---

        ### **7. What are the essential practices for monitoring, evaluating, and improving a language model during and after training?**

        Effective training involves continuous monitoring, evaluation, and iterative refinement:

        * **Monitoring Metrics during Training:** Track Training Loss (how well the model fits training data), Validation Loss (how well it generalizes), and Perplexity (for language modeling). For specific tasks, also monitor Accuracy, Precision, Recall, F1-score (for classification), or BLEU/ROUGE (for generation). Tools like Hugging Face Trainer API, TensorBoard, and Weights & Biases (W\&B) provide seamless logging and visualization.
        * **Early Stopping:** Implement callbacks (e.g., `EarlyStoppingCallback` in Hugging Face Trainer) to halt training when validation metrics stop improving for a set number of evaluations, preventing overfitting and saving compute resources.
        * **Checkpointing:** Regularly save the model's progress (`save_steps` in `TrainingArguments`) to disk. This allows recovery from interruptions and enables selection of the best performing model based on validation metrics.
        * **Error Analysis and Iteration:** After training, go beyond quantitative metrics. Manually sample model outputs and compare them to ground truth to identify specific failure modes (e.g., struggles with rare terms, specific contexts). Use these insights to iteratively:

          * Clean or augment data for weak spots.
          * Adjust hyperparameters (learning rate, batch size).
          * Tweak fine-tuning strategies or even model architecture.

        Tools like Argilla can facilitate systematic human-in-the-loop error analysis.

        * **Distributed Training:** For large models and datasets, leverage Hugging Face Accelerate, DeepSpeed, or FSDP for multi-GPU and multi-node training, enabling faster and more memory-efficient scaling. Mixed-precision training (FP16/BF16) is also standard for efficiency.

        These practices ensure that the training process is efficient, robust, and leads to a high-performing, reliable model.

        ---

        ### **8. What are the key takeaways for building custom language models in today's AI landscape?**

        The key takeaways for building custom language models in the current AI landscape are:

        * **Data Quality is Paramount:** The quality, relevance, diversity, and cleanliness of your data are the single most important factor for model performance. Invest heavily in meticulous data curation, cleaning, labeling (often human-in-the-loop), and domain-specific tokenization.
        * **Fine-tuning Pre-trained Models is the Norm:** While training from scratch is still relevant for research or highly specialized domains, most practical applications in 2025 leverage the knowledge embedded in powerful pre-trained models (like Llama, Mistral) and fine-tune them. This approach is more efficient, cost-effective, and delivers strong results.
        * **Efficiency and Scalability are Crucial:** For large datasets, adopt modern techniques like data streaming and batch processing (e.g., with Hugging Face datasets). For large models, utilize distributed training frameworks (Hugging Face Accelerate, DeepSpeed) and mixed-precision training (FP16/BF16).
        * **Privacy and Reproducibility are Non-Negotiable:** Implement robust PII detection and redaction, stringent access controls, and encryption for sensitive data. Crucially, track every data change and experiment with version control systems (DVC, LakeFS) and experiment tracking platforms (MLflow, Weights & Biases) to ensure auditability and the ability to reproduce results.
        * **Parameter-Efficient Fine-Tuning (PEFT) is a Game-Changer:** Methods like LoRA and QLoRA are highly recommended for adapting large models, significantly reducing compute and memory requirements while maintaining performance.
        * **Iterative Improvement is Essential:** Monitor training and validation metrics closely, use early stopping to prevent overfitting, and rigorously analyze model errors (both quantitative and qualitative) to continuously refine your data, hyperparameters, and fine-tuning strategies.

        ---

        Let me know if you’d like a PDF, slide version, or a Notion-friendly export.
      metadata:
        extension: .md
        size_bytes: 12746
        language: markdown
    docs/code_listings.md:
      content: |-
        ### 1. Python Environment Setup

        *   **Introduction**: This code snippet details **how to set up your development environment** with the necessary Python version and libraries to begin building custom language models. It addresses a fundamental prerequisite for any AI project: having the correct tools installed and configured.
        *   **High-level Description**: The code provides multiple methods for setting up a Python 3.12.9 environment and installing key libraries like `datasets`, `transformers`, `tokenizers`, `torch`, and `accelerate`. It recommends using `pyenv` for Python version management and `poetry` for dependency management, but also offers `conda` and `pip` alternatives for flexibility.
        *   **Step-by-step Explanation**:
            *   `# Using pyenv (recommended for Python version management)`: A comment indicating `pyenv` as the recommended tool for managing Python versions.
            *   `pyenv install 3.12.9`: Installs Python version 3.12.9 using `pyenv`.
            *   `pyenv local 3.12.9`: Sets the **local Python version for the current directory** to 3.12.9.
            *   `# Verify Python version`: A comment indicating the next step is to verify the Python version.
            *   `python --version # Should show Python 3.12.9`: Runs the `python --version` command to **confirm the active Python version**.
            *   `# Install with poetry (recommended)`: A comment indicating `poetry` as the recommended tool for dependency management.
            *   `poetry new dataset-curation-project`: **Creates a new `poetry` project** named `dataset-curation-project`.
            *   `cd dataset-curation-project`: Changes the current directory into the newly created project.
            *   `poetry env use 3.12.9`: Configures `poetry` to use the specified Python version for the project's virtual environment.
            *   `poetry add datasets transformers tokenizers torch accelerate`: **Adds the required Python packages** (`datasets`, `transformers`, `tokenizers`, `torch`, `accelerate`) to the project's dependencies using `poetry`.
            *   `# Or use mini-conda`: A comment indicating an alternative installation method using `conda`.
            *   `conda create -n dataset-curation python=3.12.9`: **Creates a new `conda` environment** named `dataset-curation` with Python 3.12.9.
            *   `conda activate dataset-curation`: **Activates the newly created `conda` environment**.
            *   `pip install datasets transformers tokenizers torch accelerate`: Installs the required packages using `pip` within the `conda` environment.
            *   `# Or use pip with pyenv`: A comment indicating another alternative using `pip` in conjunction with `pyenv`.
            *   `pyenv install 3.12.9`: Installs Python version 3.12.9 using `pyenv`.
            *   `pyenv local 3.12.9`: Sets the local Python version for the current directory to 3.12.9.
            *   `pip install datasets transformers tokenizers torch accelerate`: Installs the required packages using `pip` directly after setting the Python version with `pyenv`.

        ### 2. Basic Data Cleaning with Hugging Face Datasets

        *   **Introduction**: This code demonstrates a **fundamental step in data curation**: cleaning raw text. It specifically shows how to use the Hugging Face `Datasets` library for initial data preparation, which is crucial because "Garbage in, garbage out" remains a core truth in AI, and high-quality, clean data is the "first—and most critical—step" in building effective language models.
        *   **High-level Description**: This Python script loads a dataset (assumed to be `customer_logs.csv`), defines a function to remove HTML tags and normalize whitespace within the text, and then applies this cleaning function across the entire dataset using the `map` method of the Hugging Face `Datasets` library.
        *   **Step-by-step Explanation**:
            *   `import re`: Imports the **regular expression module** (`re`) for pattern-based text manipulation.
            *   `from datasets import load_dataset`: Imports the `load_dataset` function from the Hugging Face `datasets` library, used to load various dataset formats efficiently.
            *   `dataset = load_dataset("csv", data_files="customer_logs.csv")`: **Loads a dataset** from a CSV file named `customer_logs.csv`. The `datasets` library supports various formats.
            *   `def clean_text(example):`: Defines a Python function `clean_text` that takes an `example` (a single record from the dataset) as input.
            *   `text = re.sub(r'<.*?>', '', example["text"])`: **Removes HTML tags** from the `text` field using a regular expression, replacing them with an empty string.
            *   `text = re.sub(r'\\s+', ' ', text)`: **Replaces multiple whitespace characters** (including spaces, tabs, newlines) with a single space, standardizing spacing.
            *   `text = text.strip()`: **Removes any leading or trailing whitespace** from the `text`.
            *   `return {"text": text}`: Returns a dictionary with the cleaned text, ready to update the dataset.
            *   `cleaned_dataset = dataset.map(clean_text)`: **Applies the `clean_text` function to every example** in the `dataset`. The `map` function is highly efficient for large datasets.
            *   `print(cleaned_dataset["train"]["text"])`: Prints the cleaned text of the first example from the "train" split of the `cleaned_dataset`, showing the result.

        ### 3. Scalable Text Cleaning and Deduplication with Hugging Face Datasets

        *   **Introduction**: This code expands on data cleaning by demonstrating **scalable text cleaning and deduplication** techniques using the Hugging Face `Datasets` library. It's a critical part of ensuring data quality, as removing noise, duplicates, and applying normalization prevents models from learning from corrupted or redundant examples, which is essential for preparing "rich and well-tended data" that "grows robust AI".
        *   **High-level Description**: The script loads a snapshot of the English Wikipedia dataset, applies Unicode normalization, removes HTML tags and URLs, normalizes whitespace, and then performs deduplication based on the text content. It leverages the `map` method with parallel processing and the `unique` method for efficiency.
        *   **Step-by-step Explanation**:
            *   `import datasets`: Imports the core `datasets` library.
            *   `import unicodedata`: Imports the `unicodedata` module for Unicode character normalization.
            *   `import re`: Imports the `re` module for regular expressions.
            *   `wiki = datasets.load_dataset("wikipedia", "20220301.en", split="train")`: **Loads a specific version of the English Wikipedia dataset** (snapshot from March 1, 2022) into a `Dataset` object, specifically the "train" split.
            *   `def clean_text(example):`: Defines a function `clean_text` that takes an `example` (a dictionary representing a data entry).
            *   `text = unicodedata.normalize('NFKC', example['text'])`: **Performs Unicode normalization** using the NFKC form to standardize characters.
            *   `text = re.sub(r'<.*?>', '', text)`: **Removes HTML tags** from the text.
            *   `text = re.sub(r'https?://\\S+', '', text)`: **Removes URLs** (http or https) from the text.
            *   `text = re.sub(r'\\s+', ' ', text)`: **Normalizes all sequences of whitespace** into a single space.
            *   `text = text.strip()`: **Removes any leading or trailing whitespace**.
            *   `return {"text": text}`: Returns the cleaned text in a dictionary format.
            *   `wiki = wiki.map(clean_text, num_proc=4)`: **Applies the `clean_text` function to the `wiki` dataset** in parallel using 4 processes (`num_proc=4`) for scalability.
            *   `wiki = wiki.unique("text")`: **Removes duplicate entries** from the dataset based on the content of the `text` column, improving model quality.

        ### 4. Automated Language Detection and Filtering

        *   **Introduction**: This code snippet demonstrates how to **automatically detect and filter text data by language**, ensuring that your dataset matches your target language. This is particularly important for maintaining consistency in multilingual projects or when building a model for a specific language.
        *   **High-level Description**: The Python function uses the `langdetect` library to identify the language of each text entry. The `filter` method of the Hugging Face `Datasets` library then retains only the entries detected as English, leveraging parallel processing for efficiency.
        *   **Step-by-step Explanation**:
            *   `from langdetect import detect`: Imports the `detect` function from the `langdetect` library for language identification.
            *   `def filter_english(example):`: Defines a function `filter_english` that takes an `example` (a data entry).
            *   `try:`: Starts a `try` block to handle potential errors during language detection.
            *   `return detect(example['text']) == 'en'`: Attempts to **detect the language of the `text` field**. If it's 'en' (English), it returns `True`.
            *   `except:`: Catches any exceptions that occur (e.g., if language detection fails for very short text).
            *   `return False`: If an exception occurs, it returns `False`, filtering out the example.
            *   `wiki = wiki.filter(filter_english, num_proc=4)`: **Applies the `filter_english` function to the `wiki` dataset**. Only examples for which the function returns `True` are kept. `num_proc=4` enables parallel processing.

        ### 5. Training a Custom Tokenizer with Hugging Face Transformers (SentencePiece)

        *   **Introduction**: This code addresses the crucial step of **tokenization and vocabulary creation**, particularly for specialized or multilingual data. While pre-trained tokenizers work for general English, training a custom tokenizer can "boost performance dramatically" by tailoring the model's "slicing bread" (tokens) to how "the filling (meaning) fits" your domain. This ensures your model learns from the best possible foundation.
        *   **High-level Description**: The script demonstrates how to train a custom SentencePiece Unigram tokenizer using the `tokenizers` library. It initializes a Unigram model, sets a pre-tokenizer for whitespace, defines trainer parameters (like vocabulary size), trains the tokenizer on a specified text corpus, and then saves it. Finally, it loads the trained tokenizer into a `PreTrainedTokenizerFast` object for use with Hugging Face models.
        *   **Step-by-step Explanation**:
            *   `from transformers import AutoTokenizer, PreTrainedTokenizerFast`: Imports `AutoTokenizer` and `PreTrainedTokenizerFast` from `transformers`.
            *   `from tokenizers import trainers, Tokenizer, models, pre_tokenizers, processors`: Imports necessary classes from the `tokenizers` library for tokenizer building.
            *   `files = ["./data/cleaned_corpus.txt"]`: **Defines the input file(s)** containing cleaned text for tokenizer training.
            *   `tokenizer = Tokenizer(models.Unigram())`: **Initializes a new tokenizer with the `Unigram` model**, flexible for domain-specific tasks.
            *   `tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()`: Sets a **pre-tokenizer that splits text by whitespace** before the main tokenization.
            *   `trainer = trainers.UnigramTrainer(vocab_size=30000, special_tokens=["", "", "", ""])`: **Initializes a `UnigramTrainer`**.
                *   `vocab_size=30000`: Sets the **target vocabulary size**.
                *   `special_tokens=["", "", "", ""]`: Placeholder for special tokens.
            *   `tokenizer.train(files, trainer)`: **Trains the tokenizer** using the specified input `files` and `trainer`.
            *   `tokenizer.save("./tokenizer-unigram.json")`: **Saves the trained tokenizer** to a JSON file.
            *   `hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")`: **Loads the saved tokenizer into a `PreTrainedTokenizerFast` object** for Hugging Face `transformers` integration.

        ### 6. Using Your Trained Tokenizer with Hugging Face Transformers

        *   **Introduction**: After training a custom tokenizer (as in the previous example), this code demonstrates **how to integrate and use it** with the Hugging Face `transformers` library. This is the bridge that allows your language model to understand text in your specific domain, ensuring important terms are correctly handled and not awkwardly fragmented.
        *   **High-level Description**: The script loads a previously trained custom tokenizer from its saved file. It then uses this loaded tokenizer to tokenize a domain-specific sentence, showcasing how the tokenizer breaks down text into model-friendly pieces.
        *   **Step-by-step Explanation**:
            *   `from transformers import PreTrainedTokenizerFast`: Imports `PreTrainedTokenizerFast` for loading fast tokenizers.
            *   `hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")`: **Loads the custom tokenizer** that was previously trained and saved.
            *   `print(hf_tokenizer.tokenize("myocardial infarction"))`: **Uses the loaded tokenizer to convert the input string** "myocardial infarction" into a list of tokens, showing how it handles domain-specific terms.

        ### 7. Listing Available Wikipedia Dataset Versions & Streaming and Batch Processing with 🤗 Datasets

        *   **Introduction**: This code addresses the challenge of **scaling data processing for massive datasets**, a critical aspect of modern AI workflows. It highlights how to efficiently handle large text corpora like Wikipedia by leveraging streaming and batching with the Hugging Face `Datasets` library, thus avoiding the need to load the entire dataset into memory.
        *   **High-level Description**: The first part lists available versions of the Wikipedia dataset. The second part demonstrates how to load a large dataset in streaming mode, enabling it to be processed in manageable batches without fully loading into memory. It then applies a simple batch processing function and iterates over the processed data.
        *   **Step-by-step Explanation**:
            *   `from datasets import get_dataset_config_names`: Imports `get_dataset_config_names` to discover dataset versions.
            *   `print(get_dataset_config_names('wikipedia'))`: **Prints a list of all available configuration names** (versions by date) for the Wikipedia dataset.
            *   `from datasets import load_dataset`: Imports `load_dataset`.
            *   `def process_batch(batch):`: Defines a function `process_batch` for batch processing.
            *   `return {"processed_text": [t[:200] for t in batch["text"]]}`: Within `process_batch`, **truncates each text entry in the batch to its first 200 characters**.
            *   `streamed_dataset = load_dataset('wikipedia', '20240101.en', split='train', streaming=True)`: **Loads the English Wikipedia dataset in streaming mode**. `streaming=True` reads data one sample at a time, avoiding full download.
            *   `processed = streamed_dataset.map(process_batch, batched=True, batch_size=1000)`: **Applies `process_batch` to the streamed dataset** in chunks of 1000 examples (`batch_size=1000`) for efficiency.
            *   `for i, example in enumerate(processed):`: **Iterates over the processed streamed dataset**.
            *   `print(example["processed_text"])`: Prints the `processed_text` from each example.
            *   `if i >= 2: break`: Limits the output to the first three processed examples for quick checks.

        ### 8. Tracking Dataset Versions with DVC

        *   **Introduction**: This code provides a practical example of **how to implement version control for your datasets** using DVC (Data Version Control). This practice is paramount for **reproducibility and transparency** in professional AI projects, allowing teams to know "exactly what data went into your model and how it was processed".
        *   **High-level Description**: The series of bash commands demonstrates the basic workflow of DVC: initializing a DVC repository, adding a raw dataset to DVC tracking, committing the dataset changes with Git, and then adding a new version of a processed dataset (e.g., cleaned data) to DVC, mirroring code versioning practices for data.
        *   **Step-by-step Explanation**:
            *   `$ dvc init`: **Initializes a DVC repository** within your current Git project.
            *   `$ dvc add data/raw_corpus.txt`: **Adds the `raw_corpus.txt` file (raw dataset) to DVC tracking**. DVC creates a `.dvc` metadata file.
            *   `$ git add data/raw_corpus.txt.dvc .gitignore`: **Adds the DVC metadata file** and updated `.gitignore` rules to Git's staging area.
            *   `$ git commit -m "Add raw corpus to DVC tracking"`: **Commits the DVC metadata file** to your Git repository, associating a specific data version with a code commit.
            *   `# After cleaning or labeling, add the new version`: A comment indicating the next logical step.
            *   `$ dvc add data/cleaned_corpus.txt`: **Adds a new version of the data** (`cleaned_corpus.txt`) to DVC tracking.
            *   `$ git add data/cleaned_corpus.txt.dvc`: Adds the DVC metadata file for the `cleaned_corpus.txt` to Git's staging area.
            *   `$ git commit -m "Add cleaned corpus version"`: **Commits the DVC metadata for the cleaned data** to Git, linking this data version to the current state of your code.

        ### 9. Simple PII Redaction Example (Regex-Based)

        *   **Introduction**: This code provides a basic example of **ensuring data privacy by redacting Personally Identifiable Information (PII)**. Protecting sensitive data is not just optional, but a "legal and ethical requirement," especially when dealing with user feedback or regulated fields. This is a crucial step in preparing data for responsible AI development.
        *   **High-level Description**: The Python function `redact_pii` uses regular expressions to find and replace common patterns for emails, phone numbers, and simple name formats with generic placeholder tokens like `[EMAIL]`, `[PHONE]`, and `[NAME]`. It then demonstrates this redaction on a sample sentence.
        *   **Step-by-step Explanation**:
            *   `import re`: Imports the **regular expression module** (`re`).
            *   `def redact_pii(text):`: Defines a Python function `redact_pii` that takes a `text` string.
            *   `text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)`: **Redacts email addresses** by replacing a common email pattern with `[EMAIL]`.
            *   `text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)`: **Redacts phone numbers** by replacing a common phone pattern with `[PHONE]`.
            *   `text = re.sub(r'Mr\\.\\s+\\w+|Ms\\.\\s+\\w+|Dr\\.\\s+\\w+', '[NAME]', text)`: **Redacts names** (basic pattern matching titles and a word) with `[NAME]`.
            *   `return text`: Returns the text with PII redacted.
            *   `sample = "Contact Dr. Smith at dr.smith@example.com or 555-123-4567."`: Defines a sample string for testing.
            *   `print(redact_pii(sample))`: Calls `redact_pii` and prints the redacted output.

        ### 10. Configuring a GPT-2 Model from Scratch (Modern API)

        *   **Introduction**: This code demonstrates how to **configure and initialize a GPT-2 model from scratch** using Hugging Face's `transformers` library. While modern workflows predominantly favor fine-tuning pre-trained models, understanding how to configure a model from its foundational parameters is important for foundational research or highly specialized domains. It shows how to **match the model's architecture to its task** (decoder-only for generative tasks like text generation).
        *   **High-level Description**: The Python script defines a `GPT2Config` object, setting key architectural hyperparameters such as vocabulary size, maximum position embeddings, embedding dimension, number of layers, and attention heads. It then uses this configuration to initialize a `GPT2LMHeadModel` (a GPT-2 model with a language modeling head) and includes an assertion to verify the vocabulary size alignment.
        *   **Step-by-step Explanation**:
            *   `from transformers import GPT2Config, GPT2LMHeadModel`: Imports `GPT2Config` (for defining model architecture) and `GPT2LMHeadModel` (the GPT-2 model with a language modeling head).
            *   `config = GPT2Config(`: **Initializes a configuration object** for a GPT-2 model.
            *   `vocab_size=30000,`: Sets the **vocabulary size** to 30,000, which must match the tokenizer's vocabulary.
            *   `max_position_embeddings=512,`: Defines the **maximum sequence length** the model can handle.
            *   `n_embd=768,`: Specifies the **embedding dimension**.
            *   `n_layer=12,`: Sets the **number of transformer layers**.
            *   `n_head=12,`: Defines the **number of attention heads**.
            *   `use_cache=True`: Enables the **key-value cache** for faster generation during inference.
            *   `)`: Closes `GPT2Config` initialization.
            *   `model = GPT2LMHeadModel(config)`: **Initializes the GPT-2 language model** with the defined `config` and randomly initialized weights.
            *   `assert config.vocab_size == model.transformer.wte.weight.shape, "Vocab size mismatch!"`: **Verifies that the model's embedding matrix size matches the configured vocabulary size**, a crucial sanity check.

        ### 11. Loading and Adapting a Pre-trained GPT-2 Model

        *   **Introduction**: This code demonstrates the **most common and efficient practice** in modern NLP: **starting from a strong pre-trained model and fine-tuning it** on domain-specific data. This approach leverages the vast general language knowledge already captured by models like GPT-2, significantly reducing compute requirements and accelerating adaptation to unique business needs.
        *   **High-level Description**: The script loads a pre-trained GPT-2 tokenizer and model directly from the Hugging Face Model Hub. It then shows how to add new, domain-specific tokens to the tokenizer and, crucially, how to resize the model's token embeddings to accommodate these new tokens, ensuring the model can learn their representations during fine-tuning.
        *   **Step-by-step Explanation**:
            *   `from transformers import GPT2TokenizerFast, GPT2LMHeadModel`: Imports `GPT2TokenizerFast` and `GPT2LMHeadModel`.
            *   `tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")`: **Loads the pre-trained GPT-2 tokenizer**.
            *   `model = GPT2LMHeadModel.from_pretrained("gpt2")`: **Loads the pre-trained GPT-2 language model** with its learned weights.
            *   `new_tokens = ["", ""]`: Defines a list `new_tokens` for **new, domain-specific tokens**.
            *   `num_added = tokenizer.add_tokens(new_tokens)`: **Adds `new_tokens` to the tokenizer's vocabulary**.
            *   `if num_added > 0:`: Checks if any new tokens were added.
            *   `model.resize_token_embeddings(len(tokenizer))`: If new tokens were added, **resizes the model's input token embeddings layer** to match the new vocabulary size, allowing the model to learn representations for new tokens.
            *   `# Model and tokenizer are now ready for domain-specific fine-tuning`: A comment indicating readiness for fine-tuning.

        ### 12. Quick PEFT Example with Mistral-7B

        *   **Introduction**: This code exemplifies the power of **Parameter-Efficient Fine-Tuning (PEFT)**, specifically LoRA (Low-Rank Adaptation), which has become a standard for adapting large language models (LLMs). PEFT methods dramatically **reduce compute and memory requirements** for fine-tuning, making it feasible to adapt models like Mistral-7B on more modest hardware while "maintaining strong performance".
        *   **High-level Description**: The script first sets up a Python environment. Then, it loads the Mistral-7B model in a highly memory-efficient 4-bit quantized format using `BitsAndBytesConfig`. It then configures a `LoraConfig` object with specific parameters (like rank and alpha) targeting key attention layers. Finally, it applies this LoRA configuration to the model, showcasing a massive reduction in trainable parameters.
        *   **Step-by-step Explanation**:
            *   `pyenv install 3.12.9`: Installs Python 3.12.9.
            *   `pyenv local 3.12.9`: Sets local Python version.
            *   `poetry add transformers peft bitsandbytes accelerate`: Adds required libraries for PEFT and quantization.
            *   `from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig`: Imports necessary classes for models, tokenizers, and 4-bit quantization.
            *   `from peft import LoraConfig, get_peft_model, TaskType`: Imports PEFT-specific classes.
            *   `bnb_config = BitsAndBytesConfig(`: **Initializes a `BitsAndBytesConfig`** for 4-bit quantization.
                *   `load_in_4bit=True,`: Loads model weights in 4-bit precision.
                *   `bnb_4bit_compute_dtype="float16",`: Sets computation dtype to `float16`.
                *   `bnb_4bit_quant_type="nf4",`: Uses NF4 quantization type.
                *   `bnb_4bit_use_double_quant=True`: Enables double quantization.
            *   `model = AutoModelForCausalLM.from_pretrained(`: **Loads the Mistral-7B model from Hugging Face Hub** with the defined `quantization_config`.
            *   `tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")`: **Loads the corresponding tokenizer**.
            *   `peft_config = LoraConfig(`: **Initializes a `LoraConfig`** for LoRA parameters.
                *   `task_type=TaskType.CAUSAL_LM,`: Specifies the task type.
                *   `inference_mode=False,`: Configures for fine-tuning.
                *   `r=8,`: Sets the **LoRA rank**.
                *   `lora_alpha=32,`: Scaling factor for LoRA updates.
                *   `lora_dropout=0.1,`: Dropout for LoRA layers.
                *   `target_modules=["q_proj", "v_proj"]`: Specifies **which layers LoRA should be applied to**.
            *   `model = get_peft_model(model, peft_config)`: **Wraps the loaded Mistral model with the LoRA configuration**, transforming it into a PEFT model.
            *   `model.print_trainable_parameters()`: **Prints trainable parameters**, showing the massive reduction (e.g., ~0.1% of total).

        ### 13. Launching Distributed Training with Accelerate

        *   **Introduction**: This code demonstrates the **simplicity of launching distributed training** using the Hugging Face `Accelerate` library. As models and datasets grow, distributed training across multiple GPUs or machines becomes "essential for real-world projects and production-grade deployment" to train faster and scale to larger models.
        *   **High-level Description**: The commands show the two main steps for using `Accelerate`: first, interactively configuring the hardware setup (like number of GPUs, backend), and then launching your Python training script, where `Accelerate` will automatically handle device placement and data parallelism with "minimal script changes".
        *   **Step-by-step Explanation**:
            *   `accelerate config`: This **command-line utility initiates an interactive setup process**. It prompts the user to define their hardware configuration (e.g., number of GPUs, backend, precision) and saves these settings.
            *   `accelerate launch train.py`: This **command executes your training script (`train.py`) in a distributed manner** based on the saved `accelerate` configuration. `Accelerate` automatically handles spreading the model and data across specified devices and managing gradient synchronization, simplifying distributed training.

        ### 14. Logging Training Metrics with Trainer API and Experiment Tracking

        *   **Introduction**: This code focuses on **effective training monitoring and experiment tracking**, which are vital for understanding a model's learning progress and catching issues early. By integrating with tools like TensorBoard and Weights & Biases (W&B), it facilitates **visualization and comparison of experiments at scale**, moving beyond just loss and perplexity to include richer metrics for robust evaluation.
        *   **High-level Description**: The Python script configures `TrainingArguments` for a Hugging Face `Trainer`, setting parameters for output directory, evaluation strategy, logging intervals, batch size, epochs, and crucially, enabling reporting to TensorBoard and Weights & Biases. It then initializes the `Trainer` with a model and datasets, and starts the training process, with instructions for visualizing the logged metrics.
        *   **Step-by-step Explanation**:
            *   `from transformers import Trainer, TrainingArguments`: Imports `Trainer` (high-level API for training) and `TrainingArguments` (to define hyperparameters).
            *   `training_args = TrainingArguments(`: **Initializes a `TrainingArguments` object**.
            *   `output_dir="./results",`: Specifies the **directory for saving outputs**.
            *   `evaluation_strategy="steps",`: Sets **evaluation strategy** to fixed step intervals.
            *   `eval_steps=500,`: Sets **evaluation interval** to every 500 training steps.
            *   `logging_steps=100,`: Sets **logging interval** to every 100 steps.
            *   `save_steps=500,`: Sets **checkpoint interval** to every 500 steps, crucial for resuming training.
            *   `per_device_train_batch_size=2,`: Sets **batch size per GPU**.
            *   `num_train_epochs=3,`: Defines the **number of training epochs**.
            *   `report_to=["tensorboard", "wandb"],`: **Enables integration with experiment tracking tools** like TensorBoard and Weights & Biases.
            *   `trainer = Trainer(`: **Initializes the `Trainer` object** with the model, arguments, and datasets.
            *   `trainer.train()`: **Starts the training process**.
            *   `# To visualize in TensorBoard: # tensorboard --logdir ./results`: Instructions to launch TensorBoard.
            *   `# For Weights & Biases, login with wandb and view runs in the dashboard.`: Instructions for W&B.

        ### 15. Using the Hugging Face Evaluate Library

        *   **Introduction**: This code highlights how to use the Hugging Face `evaluate` library for **standardized metric computation across various NLP tasks**. Effective evaluation is crucial for judging model performance beyond just loss, encompassing task-specific metrics like accuracy, F1-score, BLEU, and ROUGE.
        *   **High-level Description**: The script demonstrates how to load specific evaluation metrics (accuracy, F1, BLEU) using the `load` function from the `evaluate` library. It then provides a general example of how these loaded metric objects can be used within an evaluation loop to compute scores by comparing model predictions against ground truth references.
        *   **Step-by-step Explanation**:
            *   `from evaluate import load`: Imports the `load` function from the `evaluate` library.
            *   `accuracy = load("accuracy")`: **Loads the "accuracy" metric**.
            *   `f1 = load("f1")`: **Loads the "f1" metric**.
            *   `bleu = load("bleu")`: **Loads the "bleu" metric**.
            *   `predictions = [...] # Model outputs`: Placeholder for model outputs.
            *   `references = [...] # Ground truth labels`: Placeholder for ground truth labels.
            *   `result = accuracy.compute(predictions=predictions, references=references)`: **Computes the accuracy score** by comparing predictions to references.
            *   `print(result)`: Prints the calculated accuracy result.

        ### 16. Adding Early Stopping Callback

        *   **Introduction**: This code demonstrates the implementation of **early stopping**, a vital technique to **prevent overfitting and conserve computational resources** during training. By halting training when validation performance no longer improves, it ensures that your model doesn't waste time learning noise from the training data, leading to better generalization and efficiency.
        *   **High-level Description**: The script shows how to integrate the `EarlyStoppingCallback` into the Hugging Face `Trainer`. This callback monitors a specified metric (by default, validation loss) and automatically stops the training process if that metric does not improve for a predefined number of evaluations, known as "patience".
        *   **Step-by-step Explanation**:
            *   `from transformers import EarlyStoppingCallback`: Imports the `EarlyStoppingCallback` class.
            *   `trainer = Trainer(`: Continues the `Trainer` initialization.
                *   `model=model,`: The model to be trained.
                *   `args=training_args,`: The `TrainingArguments`.
                *   `train_dataset=train_dataset,`: The training dataset.
                *   `eval_dataset=eval_dataset,`: The evaluation dataset, crucial for early stopping.
                *   `callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]`: **Adds the `EarlyStoppingCallback`**. `early_stopping_patience=3` means training will stop if there's no improvement on the monitored validation metric for 3 consecutive evaluations.

        ### 17. Sampling Model Outputs for Error Analysis

        *   **Introduction**: This code provides a practical method for **error analysis by sampling and reviewing model outputs**. After training, it's essential to look beyond just numerical metrics and manually inspect where the model fails, especially for "real errors" that require qualitative assessment. This helps in identifying specific weaknesses in your model or data.
        *   **High-level Description**: The Python script loads a fine-tuned text generation model using the Hugging Face `pipeline` API. It then defines a list of domain-specific prompts and iterates through them, generating text for each. Finally, it prints both the original prompt and the model's generated output, allowing for human review and error identification.
        *   **Step-by-step Explanation**:
            *   `from transformers import pipeline`: Imports the `pipeline` function.
            *   `text_generator = pipeline("text-generation", model="./results/checkpoint-1500")`: **Initializes a text generation pipeline**, loading a fine-tuned model from a specific checkpoint.
            *   `prompts = [`: **Defines a list of input prompts**.
            *   `"In accordance with the contract, the party of the first part shall",`: A sample legal prompt.
            *   `"The diagnosis was confirmed by the following procedure:"`: A sample medical prompt.
            *   `for prompt in prompts:`: **Iterates through each prompt**.
            *   `output = text_generator(prompt, max_length=50, num_return_sequences=1)`: **Generates text** for the current `prompt`, limiting length and sequences.
            *   `print(f"Prompt: {prompt}\\nGenerated: {output['generated_text']}\\n")`: **Prints the original prompt and the model's generated text** for review.

        ### 18. Sample Data Cleaning Pipeline with Hugging Face Datasets

        *   **Introduction**: This code provides another example of a **data cleaning pipeline** using the Hugging Face `Datasets` library. It reinforces the concept that **data quality is the "foundation"** for building effective language models. This pipeline demonstrates how to efficiently clean raw text data, which is optimized for scalability and "integrates seamlessly with modern LLM workflows".
        *   **High-level Description**: The Python script defines a simple cleaning function to remove HTML tags and normalize whitespace. It then loads a Wikipedia dataset in streaming mode and applies this cleaning function across the dataset using the `map` method, showcasing a scalable and memory-efficient approach to data preparation.
        *   **Step-by-step Explanation**:
            *   `import re`: Imports the `re` module.
            *   `from datasets import load_dataset`: Imports `load_dataset`.
            *   `def clean_text(example):`: Defines a `clean_text` function.
            *   `example['text'] = re.sub(r'<.*?>', '', example['text'])`: **Removes HTML tags** from the text.
            *   `example['text'] = re.sub(r'\\s+', ' ', example['text'])`: **Normalizes multiple whitespace characters** into a single space.
            *   `return example`: Returns the modified example.
            *   `dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)`: **Loads a Wikipedia dataset in streaming mode**, enabling memory-efficient processing for large datasets.
            *   `cleaned_dataset = dataset.map(clean_text)`: **Applies the `clean_text` function to every example** in the dataset.
            *   `# For deduplication, use .unique or filter as needed`: A comment reminding about further steps like deduplication.

        ### 19. Streaming a Large Dataset with Hugging Face Datasets

        *   **Introduction**: This code explicitly demonstrates **streaming capabilities for large datasets** using the Hugging Face `Datasets` library. This technique is "essential for scalability" because it allows processing data in batches without loading everything into memory, thereby managing large data efficiently.
        *   **High-level Description**: The Python script loads a large dataset (Wikipedia) in streaming mode. It then iterates over the dataset, printing a small portion of each example's text, showcasing how data can be processed on-the-fly without consuming excessive memory, even for terabyte-scale datasets.
        *   **Step-by-step Explanation**:
            *   `from datasets import load_dataset`: Imports `load_dataset`.
            *   `dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)`: **Loads the English Wikipedia dataset in streaming mode**. `streaming=True` is key for processing data sample by sample.
            *   `for i, example in enumerate(dataset):`: **Starts a loop to iterate through the dataset examples** as they are streamed.
            *   `print(example['text'][:100])`: **Prints the first 100 characters of the `text` field** for each example.
            *   `if i >= 2: break`: Stops the loop after printing the first three examples for demonstration purposes.

        ### 20. Configuring a Modern LLM (e.g., Llama-2) for Fine-Tuning

        *   **Introduction**: This code demonstrates **how to configure and load modern Large Language Models (LLMs)** like Llama-2 for fine-tuning. While GPT-2 provided a classic example earlier, current projects often leverage state-of-the-art architectures, and this code shows how to access their configurations using Hugging Face's `AutoConfig` and `AutoModelForCausalLM` APIs.
        *   **High-level Description**: The Python script first loads the configuration of a pre-trained Llama-2 model using `AutoConfig`. It then initializes a model from this configuration. The comments highlight that for most practical tasks, it's recommended to load the model directly with pre-trained weights for fine-tuning, leveraging existing knowledge for better results and efficiency.
        *   **Step-by-step Explanation**:
            *   `from transformers import AutoConfig, AutoModelForCausalLM`: Imports `AutoConfig` (for auto-loading model configurations) and `AutoModelForCausalLM` (for auto-loading causal language models).
            *   `config = AutoConfig.from_pretrained("meta-llama/Llama-2-7b-hf")`: **Loads the configuration of the "Llama-2-7b-hf" model**. This defines the model's architecture.
            *   `model = AutoModelForCausalLM.from_config(config)`: **Initializes a new model from the loaded `config`**, but with randomly initialized weights (i.e., not yet trained).
            *   `# For most tasks, you will load from pre-trained weights:`: A crucial comment indicating the **preferred method**: loading models with pre-trained weights for fine-tuning for efficiency and better results.
            *   `# model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")`: The **commented-out line showing how to load the model directly with its pre-trained weights**.

        ### 21. Trainer Setup with Early Stopping, Checkpointing, and Mixed Precision

        *   **Introduction**: This comprehensive code snippet demonstrates a robust and production-ready **`Trainer` setup**, incorporating critical features for efficient and reliable model training. It combines **early stopping to prevent overfitting**, **checkpointing to save progress**, and **mixed precision (fp16) for speed and memory efficiency**, while also integrating with experiment tracking. This configuration is key for achieving "robust, reliable performance" through iterative improvement.
        *   **High-level Description**: The Python script defines `TrainingArguments` with parameters for output management, evaluation frequency, checkpointing, logging, batch size, epochs, and mixed precision. It also enables reporting to experiment tracking platforms. Then, it initializes a `Trainer` with the model, arguments, datasets, and an `EarlyStoppingCallback`, and finally initiates the training process.
        *   **Step-by-step Explanation**:
            *   `from transformers import Trainer, TrainingArguments, EarlyStoppingCallback`: Imports the `Trainer`, `TrainingArguments`, and `EarlyStoppingCallback` classes.
            *   `training_args = TrainingArguments(`: **Initializes a `TrainingArguments` object**.
            *   `output_dir="./results",`: **Directory for saving outputs**.
            *   `evaluation_strategy="steps",`: Sets **evaluation strategy** to fixed step intervals.
            *   `eval_steps=500,`: Sets **evaluation interval** to every 500 steps.
            *   `save_steps=500,`: Sets **checkpoint interval** to every 500 steps.
            *   `logging_steps=100,`: Sets **logging interval** to every 100 steps.
            *   `per_device_train_batch_size=4,`: Defines the **batch size per GPU**.
            *   `num_train_epochs=3,`: Sets the **total number of training epochs**.
            *   `report_to=["wandb"],`: **Enables integration with Weights & Biases** for experiment tracking.
            *   `fp16=True,`: **Enables mixed-precision training (using 16-bit floating point)** for speed and memory efficiency.
            *   `load_best_model_at_end=True`: After training, **loads the model weights corresponding to the best evaluation performance**.
            *   `trainer = Trainer(`: **Initializes the `Trainer` object** with the model, arguments, datasets, and callbacks.
            *   `callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]`: **Adds the `EarlyStoppingCallback`** to stop training if no improvement is observed for 3 consecutive evaluation steps.
            *   `trainer.train()`: **Starts the training process**.
      metadata:
        extension: .md
        size_bytes: 41322
        language: markdown
    docs/study_guide.md:
      content: |
        Here is your original content **beautifully formatted** without altering the substance, ensuring it's easy to navigate, reference, and study:

        ---

        # 📘 **Building Custom Language Models: A Comprehensive Study Guide**

        ---

        ## I. **Quiz**

        **Instructions:** Answer each question in 2–3 sentences.

        1. **What is the primary reason why data quality is considered the "first and most critical step" in building effective language models?**
        2. **Explain the key difference between fine-tuning a pre-trained model and training a model entirely from scratch, and when each approach is typically preferred.**
        3. **Name three common types of "noise" that modern NLP cleaning processes aim to remove from raw text data.**
        4. **How does the Hugging Face Datasets library facilitate scalable text cleaning and deduplication, especially for large datasets?**
        5. **What is the purpose of "human-in-the-loop" data labeling, and why is it important even with automated labeling capabilities?**
        6. **Briefly describe the role of tokenization in preparing text data for language models and mention one popular tokenization algorithm.**
        7. **Explain the concept of data streaming in the context of large-scale data processing for language models.**
        8. **Why is version control for data (e.g., using DVC) considered essential in professional AI projects?**
        9. **When configuring a language model, what is the significance of the `vocab_size` parameter, and what must it always match?**
        10. **What is Parameter-Efficient Fine-Tuning (PEFT), and what primary benefit does it offer when adapting large language models?**

        ---

        ## II. **Answer Key**

        1. **Data quality** is critical because a flawed foundation, containing messy, biased, or irrelevant content, will directly lead to a model reflecting those flaws. High-quality, domain-specific data enables the model to truly understand and specialize in its intended area, outperforming generic models.

        2. **Fine-tuning** involves adapting a strong pre-trained model to specific domain data, which is efficient and cost-effective for most practitioners. **Training from scratch** means building a model entirely new, typically reserved for organizations with massive datasets, significant compute resources, or highly specialized applications requiring complete control over the architecture.

        3. Three common types of **"noise"** are:

           * HTML tags
           * URLs
           * Inconsistent spacing/newlines
             Other examples include boilerplate text, emojis, code snippets, and offensive content.

        4. The **Hugging Face Datasets library** enables scalable processing by supporting **streaming and batch operations**. It reads data in chunks instead of all at once and applies functions like `map()` and `unique()` efficiently, even on large datasets.

        5. **Human-in-the-loop** data labeling helps capture nuanced cases such as sarcasm, legal ambiguity, or rare anomalies. This human oversight improves **accuracy, reliability, and bias mitigation** beyond what automated tools can guarantee.

        6. **Tokenization** breaks text into model-understandable units like words, subwords, or characters. A popular tokenization algorithm is **Byte-Pair Encoding (BPE)**, useful for handling rare or compound words.

        7. **Data streaming** processes one record at a time or in small batches, ideal for large datasets that don’t fit in memory. It supports efficient, scalable NLP workflows without requiring full dataset downloads.

        8. **Version control for data** (e.g., via DVC) ensures that every change is tracked, enabling reproducibility, audits, and consistent model results. It is foundational for collaborative and compliant ML projects.

        9. The `vocab_size` parameter specifies how many unique tokens a model can recognize. It **must match** the tokenizer's vocabulary size to avoid misalignment between token embeddings and input processing.

        10. **PEFT** refers to strategies like LoRA, Prefix Tuning, or Adapters that allow training only a small subset of a model’s parameters. This makes it possible to adapt large models with **lower compute costs and memory usage**.

        ---

        ## III. **Essay Format Questions**

        1. **Discuss the complete lifecycle** of building a custom language model, from raw data to a production-ready system, highlighting the interconnectedness of each stage.

        2. **Analyze the "Garbage in, garbage out" principle** in the context of language model training. Provide specific examples of how poor data quality can manifest in model flaws and detail the various modern data curation techniques used to mitigate these issues.

        3. **Compare and contrast fine-tuning vs. training from scratch.** Include considerations for resource allocation, typical use cases, and the benefits and drawbacks of each approach in an AI-driven world.

        4. **Explain the critical importance of scalability, privacy, and data versioning** in modern workflows. Describe how tools like Hugging Face Datasets, PII redaction methods, and DVC address these challenges.

        5. **Detail the process of configuring and initializing a language model**, including architecture selection and PEFT methods. How do these decisions influence training efficiency and model outcomes?

        ---

        ## IV. **Glossary of Key Terms**

        | **Term**                                     | **Definition**                                                           |
        | -------------------------------------------- | ------------------------------------------------------------------------ |
        | **Accelerate**                               | Hugging Face library for simplified distributed/mixed-precision training |
        | **Annotation**                               | Adding metadata or labels (e.g., sentiment, NER) to raw text             |
        | **Apache Arrow**                             | In-memory columnar format used by Datasets for efficiency                |
        | **Argilla**                                  | Human-in-the-loop annotation tool with audit trails                      |
        | **Attention Heads (`n_head`)**               | Number of attention mechanisms per transformer layer                     |
        | **Batch Processing**                         | Handling data in groups rather than one-by-one                           |
        | **Byte-Pair Encoding (BPE)**                 | Tokenization method merging frequent byte pairs                          |
        | **Checkpointing**                            | Saving model state during training for resumption or recovery            |
        | **Common Crawl**                             | Web-scale dataset often used to train general-purpose LMs                |
        | **DVC**                                      | Data Version Control for managing large dataset revisions                |
        | **Early Stopping**                           | Halting training when validation loss ceases improving                   |
        | **Embedding Dimension (`n_embd`)**           | Size of token vectors capturing semantic meaning                         |
        | **Encoder-Decoder**                          | Model for sequence-to-sequence tasks (e.g., T5, BART)                    |
        | **Encoder-Only**                             | Models for understanding tasks (e.g., BERT)                              |
        | **Experiment Tracking**                      | Recording hyperparams, metrics, and versions (e.g., MLflow, W\&B)        |
        | **Fine-Tuning**                              | Adapting a pre-trained model to domain-specific data                     |
        | **FSDP**                                     | Fully Sharded Data Parallel training across devices                      |
        | **Gradient Checkpointing**                   | Saves memory by recomputing activations in backward pass                 |
        | **Hugging Face Datasets**                    | Library for processing large NLP datasets                                |
        | **Human-in-the-loop**                        | Involving people for quality control in AI tasks                         |
        | **KV Cache**                                 | Reuses previous attention states during generation                       |
        | **LakeFS**                                   | Git-style versioning for cloud-based data storage                        |
        | **Learning Rate**                            | Determines how fast model weights update during training                 |
        | **LoRA**                                     | PEFT method using trainable low-rank matrices                            |
        | **Max Position Embeddings**                  | Defines model’s input sequence length capacity                           |
        | **Mixed-Precision Training**                 | Uses 16-bit + 32-bit floats to reduce memory/training time               |
        | **MLflow**                                   | Platform for experiment and model lifecycle management                   |
        | **Model Architecture Selection**             | Choosing BERT, GPT, T5 based on task                                     |
        | **Number of Transformer Layers (`n_layer`)** | Depth of transformer (stacked blocks)                                    |
        | **Optimizer**                                | Algorithm for updating weights (e.g., AdamW)                             |
        | **PEFT**                                     | Parameter-efficient fine-tuning methods (e.g., LoRA, Adapters)           |
        | **Perplexity**                               | Measure of prediction uncertainty in LMs                                 |
        | **PII**                                      | Personal Identifiable Information (e.g., name, phone, email)             |
        | **Pre-trained Model**                        | Already trained on a large dataset for general understanding             |
        | **Reproducibility**                          | Ability to consistently replicate results and experiments                |
        | **ROUGE**                                    | Metric for summarization and translation quality                         |
        | **Semantic Deduplication**                   | Removing near-duplicate text via semantic similarity                     |
        | **SentencePiece Unigram**                    | Robust tokenizer for multilingual/rare domain tasks                      |
        | **Streaming**                                | Processing data in memory-efficient chunks                               |
        | **Synthetic Data Generation**                | Creating artificial data for augmentation or privacy                     |
        | **TensorBoard**                              | Tool for visualizing ML metrics, histograms, and loss                    |
        | **Tokenization**                             | Splitting raw text into tokens for model input                           |
        | **Train from Scratch**                       | Full training with no pre-existing weights                               |
        | **Vocab Size (`vocab_size`)**                | Total number of unique tokens model can understand                       |
        | **Weights & Biases**                         | ML experiment tracker for metrics and visualization                      |
        | **WordPiece**                                | BERT-style tokenizer based on word segmentation                          |
        | **ZeRO Optimizations**                       | Sharding optimizer states and model weights for scale                    |

        ---

        Let me know if you'd like this exported to **PDF**, a **Notion doc**, or a **study flashcard format**!
      metadata:
        extension: .md
        size_bytes: 11419
        language: markdown
    docs/article11i.md:
      content: |-
        # 🚀 Building Custom Language Models: From Raw Data to Production AI

        In today's rapidly evolving AI landscape, the ability to create custom language models tailored to specific domains represents a **critical competitive advantage**. This comprehensive guide walks you through the complete lifecycle of building language models—from curating high-quality datasets to training and deploying powerful AI systems that deliver real business value.

        Whether you're developing specialized models for healthcare, finance, legal services, or any domain requiring nuanced understanding, this chapter provides the **practical knowledge and code examples** you need to succeed. We'll explore modern techniques using the Hugging Face ecosystem that balance efficiency, scalability, and model quality.

        ## 📑 Table of Contents

        1. [What You'll Master](#-what-youll-master)
        2. [Dataset Curation and Training Language Models](#-dataset-curation-and-training-language-models-from-scratch)
        3. [Introduction: From Raw Data to Custom Language Models](#-introduction-from-raw-data-to-custom-language-models)
           - [Setting Up Your Environment](#️-setting-up-your-environment)
           - [Why Data Quality Matters](#-why-data-quality-matters)
           - [The Value of Fine-Tuning](#-the-value-of-fine-tuning-and-custom-training)
        4. [Preparing and Curating High-Quality Datasets](#-preparing-and-curating-high-quality-datasets)
           - [Data Quality Checklist](#-data-quality-checklist)
           - [Scalable Text Cleaning](#-scalable-text-cleaning-and-deduplication)
           - [Human-in-the-Loop Labeling](#️-human-in-the-loop-data-labeling)
           - [Tokenization and Vocabulary](#-tokenization-and-vocabulary-creation)
        5. [Scaling Data Processing and Streaming](#-scaling-data-processing-and-streaming)
           - [Large-Scale Data Handling](#-handling-large-scale-data-with-streaming)
           - [Versioning and Reproducibility](#-annotation-versioning-and-reproducibility)
           - [Privacy and Security](#-ensuring-data-privacy-and-security)
        6. [Configuring and Initializing a Model](#️-configuring-and-initializing-a-model)
           - [Architecture Selection](#️-choosing-model-architecture-and-hyperparameters)
           - [Modern Model Configuration](#-configuring-a-gpt-2-model-modern-api)
           - [Parameter-Efficient Fine-Tuning](#-parameter-efficient-fine-tuning-with-modern-models)
           - [Distributed Training](#️-training-with-multiple-gpus-and-distributed-setups)
        7. [Training, Evaluation, and Iteration](#-training-evaluation-and-iteration)
           - [Monitoring Metrics](#-monitoring-loss-and-metrics)
           - [Early Stopping and Checkpointing](#-early-stopping-and-checkpointing)
           - [Error Analysis](#-error-analysis-and-iterative-improvement)
        8. [Summary and Key Takeaways](#-summary-and-key-takeaways)
        9. [Glossary](#-glossary)

        ## 📋 What You'll Master

        - **Data curation fundamentals**: selecting, cleaning, and preparing domain-specific text
        - **Scalable processing techniques** for handling massive datasets efficiently  
        - **Privacy protection and data versioning** for responsible AI development
        - **Modern model architecture selection** and configuration strategies
        - **Training workflows** with distributed computing and experiment tracking
        - **Parameter-efficient fine-tuning methods** for adapting large models
        - **Evaluation, error analysis**, and iterative improvement techniques

        By the end of this chapter, you'll have both the theoretical understanding and practical skills to transform raw text into powerful, domain-specific language models that deliver real business value. Let's begin your journey toward AI mastery! 🎯

        ## 🧠 Dataset Curation and Training Language Models from Scratch

        ```mermaid
        mindmap
          root((Dataset Curation & Training LLMs))
            Data Quality Foundation
              Source Selection
              Cleaning & Deduplication
              Labeling & Annotation
              Domain Relevance
            Modern Processing
              Streaming & Batching
              Cloud Integration
              Privacy & Compliance
              Versioning & Reproducibility
            Model Configuration
              Architecture Selection
              Parameter-Efficient Methods
              Pre-trained vs Scratch
              Distributed Training
            Training Workflow
              Monitoring & Metrics
              Early Stopping
              Checkpointing
              Iterative Improvement
            Production Ready
              Experiment Tracking
              Error Analysis
              Human-in-the-Loop
              Deployment Pipeline
        ```

        ## 🌟 Introduction: From Raw Data to Custom Language Models

        To build a great language model, you need great data. Picture your dataset as ingredients for a gourmet meal—the fresher and more carefully chosen, the better the final dish. Even the most sophisticated AI architecture can't salvage a flawed foundation. **Ready to transform raw text into powerful AI?** 🚀

        This chapter explores why curating high-quality datasets remains the first—and most critical—step in building effective language models. In today's AI landscape, most practitioners start with a strong pre-trained model (available on the Hugging Face Model Hub) and fine-tune it on their domain-specific data. This approach proves efficient, cost-effective, and enables rapid adaptation to unique business, user, or privacy requirements.

        ### 🛠️ Setting Up Your Environment

        ```bash
        # Using pyenv (recommended for Python version management)
        pyenv install 3.12.9  # Use Python 3.12.9 as per project requirements
        pyenv local 3.12.9

        # Verify Python version
        python --version  # Should show Python 3.12.9

        # Install with poetry (recommended)
        poetry new dataset-curation-project
        cd dataset-curation-project
        poetry env use 3.12.9
        poetry add datasets transformers tokenizers torch accelerate@^0.26.0

        # Or use mini-conda
        conda create -n dataset-curation python=3.12.9
        conda activate dataset-curation
        pip install datasets transformers tokenizers torch "accelerate>=0.26.0,<0.27.0"

        # Or use pip with pyenv
        pyenv install 3.12.9
        pyenv local 3.12.9
        pip install datasets transformers tokenizers torch "accelerate>=0.26.0,<0.27.0"
        ```

        > **💡 Pro Tip**: This project uses Python 3.12.9 as configured in the pyproject.toml file. Ensure you use this specific version for consistency with the development environment and Poetry lock file.

        > **⚠️ Note on accelerate**: This project requires accelerate version ^0.26.0. Earlier versions may cause compatibility issues with certain model configurations and distributed training setups.

        ### 🔐 API Key Configuration

        Before running examples that require external APIs, set up your environment variables:

        ```bash
        # Copy the example environment file
        cp .env.example .env

        # Edit .env and add your actual API keys
        # The .env.example file includes placeholders for:
        # - OPENAI_API_KEY (optional - for OpenAI examples)
        # - ANTHROPIC_API_KEY (optional - for Claude examples)  
        # - HUGGINGFACE_TOKEN (optional - for private model access)
        ```

        The project includes a `.env.example` file that shows all available environment variables. Copy this file to `.env` and replace the placeholder values with your actual API keys. The `.env` file is gitignored to keep your credentials secure.

        ### 🔑 Environment Configuration and API Keys

        For working with modern LLMs and cloud services, you'll need to configure API keys and select the appropriate compute device:

        ```python
        import os
        from pathlib import Path
        from typing import Optional, Literal
        from dotenv import load_dotenv
        import warnings

        # Load environment variables
        load_dotenv()

        # API keys (if needed)
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
        HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

        def validate_api_key(key_name: str, key_value: Optional[str], required: bool = False) -> bool:
            """
            Validate an API key with proper error handling.
            
            Args:
                key_name: Name of the API key (for error messages)
                key_value: The actual API key value
                required: Whether the key is required for operation
                
            Returns:
                bool: True if valid, False otherwise
            """
            if not key_value:
                if required:
                    raise ValueError(f"{key_name} is required but not set in environment variables")
                else:
                    warnings.warn(f"{key_name} not found in environment variables", UserWarning)
                    return False
            
            # Check for placeholder values
            if key_value.lower() in ["your-api-key-here", "placeholder", "xxx", "todo"]:
                if required:
                    raise ValueError(f"{key_name} contains a placeholder value")
                warnings.warn(f"{key_name} contains a placeholder value", UserWarning)
                return False
            
            return True

        # Device configuration for PyTorch
        import torch

        def get_device() -> Literal["mps", "cuda", "cpu"]:
            """Get the best available device for PyTorch computation."""
            if torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            elif torch.cuda.is_available():
                return "cuda"  # NVIDIA GPU
            else:
                return "cpu"

        DEVICE = get_device()
        print(f"Using device: {DEVICE}")
        ```

        ### 🎯 Why Data Quality Matters

        "Garbage in, garbage out" remains a core truth in AI. If your dataset contains messy, biased, or irrelevant content, your model will reflect those flaws with painful accuracy. Picture a financial company using a generic model that misses industry-specific terms or context. By curating a dataset of financial documents, you enable your model to actually understand your domain—transforming it from a general assistant into a specialist.

        ### ⚖️ Detecting and Mitigating Bias in Your Data

        Beyond accuracy, ensuring fairness and ethical AI requires proactive bias detection. Models trained on biased data perpetuate and amplify societal inequalities. Modern tools like **fairlearn** and **AI Fairness 360** help identify and mitigate various forms of bias—demographic, representation, and historical—before they become embedded in your model.

        ```python
        from typing import Any, List
        import pandas as pd

        # Example: Analyzing model predictions for bias
        def analyze_bias(y_true: List[int], y_pred: List[int], sensitive_features: List[str]) -> dict:
            """
            Analyze predictions for potential bias across sensitive groups.
            
            Args:
                y_true: True labels
                y_pred: Predicted labels
                sensitive_features: Sensitive attributes for each sample
                
            Returns:
                dict: Bias analysis results
            """
            try:
                from fairlearn.metrics import MetricFrame
                from sklearn.metrics import accuracy_score
            except ImportError:
                print("Install fairlearn and scikit-learn for bias analysis:")
                print("pip install fairlearn scikit-learn")
                return {}
            
            # Create metric frame for bias analysis
            metric_frame = MetricFrame(
                metrics=accuracy_score,
                y_true=y_true,
                y_pred=y_pred,
                sensitive_features=sensitive_features
            )
            
            # Display disparities
            print("Performance by group:")
            print(metric_frame.by_group)
            
            # Calculate disparity ratio
            disparity = metric_frame.difference(method='ratio')
            print(f"\nDisparity ratio: {disparity:.2f}")
            
            return metric_frame

        # Example usage with demographic data
        df = pd.DataFrame({
            'text': ['...'],  # Your text data
            'label': [0, 1, 0, 1],  # True labels
            'predicted': [0, 1, 1, 1],  # Model predictions
            'demographic': ['A', 'B', 'A', 'B']  # Sensitive attribute
        })

        analyze_bias(df['label'], df['predicted'], df['demographic'])
        ```

        **Key considerations for bias mitigation:**
        - 📊 Audit your data sources for representation gaps
        - 🔄 Rebalance datasets to ensure fair representation
        - 🎯 Use targeted data augmentation for underrepresented groups
        - 📈 Monitor fairness metrics throughout training
        - 🤝 Involve diverse stakeholders in data curation decisions

        ### 💪 The Value of Fine-Tuning and Custom Training

        Pre-trained models like GPT-4, Claude, or Llama 3 trained on vast, general data. But sometimes, you need a model that speaks your language—literally. Fine-tuning empowers you to:

        - ✅ Include rare or industry-specific vocabulary seamlessly
        - ✅ Filter out sensitive or irrelevant content precisely
        - ✅ Meet strict privacy or compliance requirements confidently
        - ✅ Rapidly adapt to new domains with limited resources

        **Example:** A healthcare provider can fine-tune a pre-trained model on anonymized clinical notes using Hugging Face's Trainer API, ensuring it understands medical jargon while respecting patient privacy completely.

        ### 🔄 What Modern Data Curation Involves

        Curation transcends merely collecting files. It demands:

        - **Selecting** relevant, diverse sources strategically
        - **Cleaning** and standardizing text meticulously
        - **Removing** duplicates and noise (including semantic deduplication)
        - **Annotating** and labeling (with tools like Argilla for human-in-the-loop workflows)
        - **Tokenizing** (splitting text into model-friendly pieces) and building vocabulary that fits your domain
        - **Versioning** and tracking your data for reproducibility

        Modern workflows often leverage the Hugging Face Datasets library for scalable, memory-efficient data loading and transformation. For large-scale or streaming data, Datasets supports processing data on-the-fly, making it possible to curate web-scale corpora without exhausting memory.

        ### 🤖 Synthetic Data Generation for Enhanced Training

        In data-scarce domains or when dealing with privacy constraints, synthetic data generation provides a powerful augmentation strategy. Modern LLMs can generate high-quality training examples that maintain semantic coherence while expanding dataset diversity. This approach proves especially valuable for rare edge cases, underrepresented classes, or sensitive domains where real data is limited.

        ```python
        import random
        from typing import List, Dict, Optional

        def generate_synthetic_examples(prompt_template: str, num_examples: int = 100, 
                                       categories: Optional[List[str]] = None, 
                                       max_length: int = 150) -> List[Dict[str, str]]:
            """
            Generate synthetic training examples using LLM-based augmentation.
            
            Args:
                prompt_template: Template string for prompts
                num_examples: Number of examples to generate
                categories: Optional list of categories to use
                max_length: Maximum length of generated text
                
            Returns:
                List of synthetic examples with text and category
            """
            try:
                from transformers import pipeline
                # Initialize text generation pipeline with a modern model
                generator = pipeline("text-generation", model="meta-llama/Llama-2-7b-hf")
            except ImportError:
                print("Warning: transformers not available for synthetic data generation")
                return []
            except Exception as e:
                print(f"Warning: Could not load model: {e}")
                return []
            
            synthetic_data = []
            
            for _ in range(num_examples):
                # Vary the prompt for diversity
                if categories:
                    category = random.choice(categories)
                    prompt = prompt_template.format(category=category)
                else:
                    prompt = prompt_template
                    
                # Generate synthetic example
                result = generator(
                    prompt,
                    max_length=max_length,
                    temperature=0.8,  # Control randomness
                    do_sample=True,
                    top_p=0.9
                )
                
                synthetic_data.append({
                    'text': result[0]['generated_text'],
                    'category': category if categories else 'general'
                })
            
            return synthetic_data

        # Example: Generate customer support queries
        prompt_template = "Generate a realistic customer support query about {category}:"
        categories = ["billing", "technical issues", "account access", "feature requests"]

        synthetic_examples = generate_synthetic_examples(
            prompt_template, 
            num_examples=50, 
            categories=categories
        )

        # Combine with real data for enhanced training
        print(f"Generated {len(synthetic_examples)} synthetic examples")
        ```

        **Best practices for synthetic data:**
        - 🎯 Validate synthetic examples against real data distributions
        - 🔄 Mix synthetic and real data (typically 20-30% synthetic)
        - 📊 Monitor model performance on held-out real data
        - 🛡️ Ensure synthetic data doesn't leak sensitive patterns
        - 📝 Document synthetic data generation for reproducibility

        For advanced synthetic data techniques and quality evaluation, see Article 8.

        ### 🧹 Basic Data Cleaning with Hugging Face Datasets

        ```python
        import re
        from typing import Dict, Any

        # Handle import issues gracefully
        try:
            from datasets import load_dataset, Dataset
            HAS_DATASETS = True
        except (ImportError, AttributeError) as e:
            print(f"Warning: datasets library import issue: {e}")
            HAS_DATASETS = False

        def clean_text(example: Dict[str, Any]) -> Dict[str, str]:
            """Clean text by removing HTML and normalizing whitespace."""
            # Remove HTML tags
            text = re.sub(r'<.*?>', '', example["text"])
            # Replace multiple spaces/newlines with a single space
            text = re.sub(r'\s+', ' ', text)
            # Strip leading/trailing whitespace
            text = text.strip()
            return {"text": text}

        if HAS_DATASETS:
            # Create sample data for demonstration
            sample_data = {
                "text": [
                    "<p>Customer complaint: Product <b>broken</b></p>   Multiple   spaces!",
                    "<div>Great service!</div>\n\n\nExtra newlines",
                    "Normal text without HTML"
                ]
            }
            
            dataset = Dataset.from_dict(sample_data)
            cleaned_dataset = dataset.map(clean_text)
            
            print(cleaned_dataset[0]["text"])  # Output: Cleaned text sample
        else:
            print("Install datasets library: pip install datasets")
        ```

        **How this works:**

        1. Loads your dataset using Hugging Face Datasets (supports CSV, JSON, Parquet, and streaming)
        2. Removes HTML tags like `<p>` completely
        3. Replaces extra spaces or newlines with a single space
        4. Trims spaces from the start and end

        This represents a starting point. In production, you may employ LLM-assisted cleaning for more complex tasks—such as detecting semantic duplicates, flagging outliers, or even auto-labeling data.

        ### 🏆 Why Invest in Modern Data Curation?

        High-quality data becomes your competitive edge. Custom-curated and well-annotated datasets allow your models to:

        - 📈 Outperform generic models in specialized tasks dramatically
        - 🛡️ Reduce errors in critical business processes significantly
        - 🔒 Ensure privacy and regulatory compliance completely
        - 🌍 Enable support for rare languages or unique domains effectively
        - ⚡ Adapt quickly to new requirements using fine-tuning or continual learning

        Tools like Hugging Face Datasets and Argilla (for collaborative annotation and versioning) now represent standards for scalable, reproducible, and team-based data workflows.

        Mastering data curation—and knowing when to fine-tune versus train from scratch—lets you build models that fit your needs, not just what's available off the shelf. **Ever wondered what unique data could give your model an unstoppable edge?** 🤔

        **🎯 Try This**: Identify three unique data sources in your domain that generic models might miss. How could these transform your AI capabilities?

        ## 📊 Preparing and Curating High-Quality Datasets

        Great language models start with great data. Picture your dataset as soil—rich and well-tended data grows robust AI. In this section, you'll master the essentials: how to select, clean, label, and tokenize text using up-to-date tools and best practices, so your model learns from the best possible foundation.

        ```mermaid
        flowchart TB
            subgraph DataCuration[Dataset Curation Pipeline]
                RawData[Raw Text Data<br>Wikipedia, Domain Docs]
                Selection[Source Selection<br>Relevance & Diversity]
                Cleaning[Text Cleaning<br>HTML, URLs, Noise]
                Dedup[Deduplication<br>Exact & Semantic]
                Labeling[Annotation<br>Human-in-the-Loop]
                Tokenization[Tokenization<br>Custom Vocabulary]

                RawData -->|Evaluate| Selection
                Selection -->|Process| Cleaning
                Cleaning -->|Remove Duplicates| Dedup
                Dedup -->|Add Labels| Labeling
                Labeling -->|Prepare for Model| Tokenization
            end

            Tokenization -->|Ready| TrainingData[Training-Ready Dataset]

            classDef default fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            class RawData,Selection,Cleaning,Dedup,Labeling,Tokenization,TrainingData default
        ```

        ### 📑 Data Quality Checklist

        | Criterion | Key Questions | Why It Matters |
        | --- | --- | --- |
        | **Relevance** | Does the text match your target use case? | Ensures model learns domain-specific patterns |
        | **Diversity** | Is there a mix of topics, styles, and authors? | Prevents bias and improves generalization |
        | **Quality** | Is the text well-formed and free of noise? | Reduces training on corrupted examples |
        | **Freshness** | Are you using the latest available data? | Prevents model drift and outdated knowledge |

        ### 🧽 Scalable Text Cleaning and Deduplication

        ```python
        import datasets
        import unicodedata
        import re

        # Load a recent English Wikipedia snapshot
        wiki = datasets.load_dataset("wikipedia", "20240101.en", split="train")

        # Unicode normalization and basic cleaning
        def clean_text(example):
            text = unicodedata.normalize('NFKC', example['text'])  # Unicode normalization
            text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
            text = re.sub(r'https?://\S+', '', text)  # Remove URLs
            text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
            text = text.strip()
            return {"text": text}

        # Apply cleaning
        wiki = wiki.map(clean_text, num_proc=4)

        # Remove duplicates
        wiki = wiki.unique("text")
        ```

        This example uses Hugging Face Datasets for efficient, parallel cleaning and deduplication. It applies Unicode normalization, removes HTML tags, URLs, and normalizes whitespace. For large-scale projects, always use `.map()` and `.unique()` for performance and reproducibility.

        ### 🏷️ Human-in-the-Loop Data Labeling

        For many tasks, clean data isn't enough—you need high-quality labels. While some models (like large language models) can learn from raw text, most business applications require supervised, labeled examples—such as sentiment, intent, or domain-specific categories.

        **Best practices for human annotation:**

        - ✍️ Write clear, detailed instructions and provide examples for annotators
        - 👥 Use multiple annotators per example to catch mistakes and reduce bias
        - 🔄 Regularly review disagreements, update guidelines, and retrain annotators as needed
        - 🔒 Ensure privacy: Mask or remove PII before annotation, especially in sensitive domains

        ### 🔤 Tokenization and Vocabulary Creation

        Clean, labeled data stands almost ready—but models can't use raw text. They need tokens: units like words, subwords, or characters. Tokenization bridges the gap, and modern workflows rely on integrated Hugging Face APIs for both standard and custom tokenizers.

        **Popular tokenization algorithms:**

        - **SentencePiece Unigram:** Flexible and robust for multilingual and domain-specific tasks
        - **Byte-Pair Encoding (BPE):** Splits rare words into subwords, balancing vocabulary size and coverage
        - **WordPiece:** Used in BERT; similar to BPE but merges differently

        ### 🛠️ Training a Custom Tokenizer with Hugging Face

        ```python
        from transformers import AutoTokenizer, PreTrainedTokenizerFast
        from tokenizers import trainers, Tokenizer, models, pre_tokenizers, processors

        # Example: Train a Unigram tokenizer using SentencePiece
        from tokenizers import Tokenizer, models, pre_tokenizers, trainers

        # Load your cleaned text file(s)
        files = ["./data/cleaned_corpus.txt"]

        # Initialize a Unigram model
        tokenizer = Tokenizer(models.Unigram())
        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

        # Configure trainer with special tokens
        trainer = trainers.UnigramTrainer(
            vocab_size=30000, 
            special_tokens=[
                "<pad>",    # Padding token
                "<unk>",    # Unknown token
                "<s>",      # Beginning of sequence (BOS)
                "</s>"      # End of sequence (EOS)
            ]
        )
        tokenizer.train(files, trainer)

        tokenizer.save("./tokenizer-unigram.json")

        # Load into Hugging Face for use with models
        hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")
        ```

        > **💡 Note**: Each special token serves a specific purpose:
        > - `<pad>`: Used for padding sequences to equal length
        > - `<unk>`: Replaces out-of-vocabulary tokens
        > - `<s>`: Marks the start of a sequence
        > - `</s>`: Marks the end of a sequence

        ### ✅ Using Your Trained Tokenizer

        ```python
        from transformers import PreTrainedTokenizerFast

        # Load the trained tokenizer
        hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")

        # Tokenize a domain-specific sentence
        print(hf_tokenizer.tokenize("myocardial infarction"))
        # Output: ['myocardial', 'infarction']  # Example output
        ```

        Test your tokenizer on real, domain-specific sentences to ensure important terms aren't split awkwardly. For example, in medical data, 'myocardial infarction' should not fragment into meaningless subwords.

        **🔬 Experiment**: Take 10 technical terms from your domain and tokenize them. Are any split inappropriately? This reveals whether you need a custom tokenizer.

        ## 🚀 Scaling Data Processing and Streaming

        As your projects grow, so do your datasets—sometimes reaching terabytes or more. Loading all this data at once resembles trying to cook every dish in a restaurant simultaneously: the kitchen will grind to a halt. Instead, you need smart, scalable workflows to keep things running smoothly.

        ```mermaid
        stateDiagram-v2
            [*] --> DataDiscovery
            DataDiscovery --> StreamingSetup: Dataset Located
            StreamingSetup --> BatchProcessing: Streaming Enabled
            BatchProcessing --> DataValidation: Batches Processed
            DataValidation --> VersionControl: Quality Verified
            VersionControl --> PrivacyCheck: Changes Tracked
            PrivacyCheck --> ProductionReady: PII Removed
            ProductionReady --> [*]

            BatchProcessing --> ErrorHandling: Processing Error
            ErrorHandling --> BatchProcessing: Retry
            PrivacyCheck --> DataValidation: Privacy Issue Found

            style DataDiscovery fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style StreamingSetup fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style BatchProcessing fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style DataValidation fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333
            style VersionControl fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style PrivacyCheck fill:#ffcdd2,stroke:#e53935,stroke-width:1px,color:#333333
            style ProductionReady fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333
            style ErrorHandling fill:#ffcdd2,stroke:#e53935,stroke-width:1px,color:#333333
        ```

        ### 🌊 Handling Large-Scale Data with Streaming

        When working with huge datasets—think the size of Wikipedia or larger—downloading everything to your machine just isn't practical. This is where streaming and batching come into play. Data streaming lets you process one record at a time, or in manageable batches, keeping only a small portion in memory.

        ### 📋 Listing Available Wikipedia Dataset Versions

        ```python
        from datasets import get_dataset_config_names

        # List all available Wikipedia dumps (by date)
        print(get_dataset_config_names('wikipedia'))
        ```

        ### 🔄 Streaming and Batch Processing

        ```python
        from datasets import load_dataset

        def process_batch(batch):
            # Example batch processing (e.g., truncating text)
            return {"processed_text": [t[:200] for t in batch["text"]]}

        # Always use the latest Wikipedia config (e.g., '20240101.en')
        streamed_dataset = load_dataset('wikipedia', '20240101.en', split='train', streaming=True)

        # Efficiently process data in batches of 1000
        processed = streamed_dataset.map(process_batch, batched=True, batch_size=1000)

        # Iterate over the first processed batch
        for i, example in enumerate(processed):
            print(example["processed_text"])
            if i >= 2:
                break
        ```

        **Key takeaway:** Modern streaming and batching let you process huge datasets efficiently, with minimal memory or storage requirements. For production-scale needs, leverage cloud storage and distributed frameworks for seamless scalability.

        ### 📦 Annotation, Versioning, and Reproducibility

        In professional AI projects, you must know exactly what data went into your model and how it was processed. Consider this as keeping a precise recipe—so you (or your team) can always recreate results or explain decisions.

        ### 🔖 Tracking Dataset Versions with DVC

        ```bash
        # Initialize DVC in your project
        $ dvc init

        # Add your raw dataset to DVC tracking
        $ dvc add data/raw_corpus.txt

        # Commit the change (with metadata)
        $ git add data/raw_corpus.txt.dvc .gitignore
        $ git commit -m "Add raw corpus to DVC tracking"

        # After cleaning or labeling, add the new version
        $ dvc add data/cleaned_corpus.txt
        $ git add data/cleaned_corpus.txt.dvc
        $ git commit -m "Add cleaned corpus version"
        ```

        For cloud-native projects, LakeFS and Delta Lake integrate with S3 or Azure Blob, offering Git-like semantics for data versioning at scale.

        ### 🔐 Ensuring Data Privacy and Security

        When your data includes personal or confidential information, privacy transcends optional—it's a legal and ethical requirement. Protecting sensitive data proves as important as any step in your pipeline, and modern privacy tools make this easier and more robust than ever.

        **Main strategies:**

        - 🔍 **PII detection and removal**: Use automated tools to scan for names, emails, and phone numbers
        - 🎭 **Anonymization**: Replace sensitive details with tokens or hash values
        - 🛡️ **Differential privacy**: Apply mathematical guarantees to prevent individual identification
        - 🔒 **Access controls**: Store data securely with encryption at rest and in transit

        ### 🚨 Advanced PII Redaction with Transformer Models

        ```python
        # Basic regex approach (for comparison)
        import re

        def basic_redact_pii(text: str) -> str:
            """Basic PII redaction using regex patterns."""
            # Apply patterns in specific order to avoid conflicts
            
            # SSN pattern (xxx-xx-xxxx) - do this before phone numbers
            text = re.sub(
                r'\b\d{3}-\d{2}-\d{4}\b',
                '[SSN]',
                text
            )
            
            # Credit card patterns (basic - 16 digits with optional spaces/dashes)
            text = re.sub(
                r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
                '[CREDIT_CARD]',
                text
            )
            
            # Improved email pattern
            text = re.sub(
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                '[EMAIL]',
                text
            )
            
            # Phone patterns
            text = re.sub(
                r'(\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b',
                '[PHONE]',
                text
            )
            
            # Name patterns with titles
            text = re.sub(
                r'\b(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.|Rev\.)\s+[A-Z][a-z]+\s+[A-Z][a-z]+\b',
                '[NAME]',
                text
            )
            
            return text

        # Modern transformer-based approach using presidio
        from presidio_analyzer import AnalyzerEngine
        from presidio_anonymizer import AnonymizerEngine

        # Initialize with transformer model support
        analyzer = AnalyzerEngine()
        anonymizer = AnonymizerEngine()

        def advanced_redact_pii(text):
            # Analyze text for PII entities
            results = analyzer.analyze(
                text=text,
                language='en',
                entities=["PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER", "CREDIT_CARD"]
            )
            
            # Anonymize detected entities
            anonymized = anonymizer.anonymize(
                text=text,
                analyzer_results=results
            )
            
            return anonymized.text

        # Example usage
        sample = "Contact Dr. Smith at dr.smith@example.com or 555-123-4567."
        print(f"Basic: {basic_redact_pii(sample)}")
        print(f"Advanced: {advanced_redact_pii(sample)}")
        ```

        > **⚠️ Important**: The basic regex patterns have low recall and miss many edge cases. For production use, always prefer transformer-based approaches like presidio-analyzer or LLM-powered detection for multilingual and context-dependent PII.

        ## ⚙️ Configuring and Initializing a Model

        With your dataset ready, it's time to transform raw data into a working language model. This section walks you through four key steps: selecting the right architecture, setting core hyperparameters, initializing your model, and preparing for scalable, efficient training.

        ```mermaid
        classDiagram
            class ModelConfiguration {
                +architecture_type: str
                +vocab_size: int
                +max_position_embeddings: int
                +n_embd: int
                +n_layer: int
                +n_head: int
                +use_cache: bool
                +select_architecture()
                +set_hyperparameters()
                +validate_config()
            }

            class PreTrainedModel {
                +model_name: str
                +config: ModelConfiguration
                +from_pretrained()
                +resize_token_embeddings()
                +save_pretrained()
            }

            class TrainingSetup {
                +learning_rate: float
                +batch_size: int
                +optimizer: AdamW
                +mixed_precision: bool
                +distributed: bool
                +setup_training()
                +enable_peft()
            }

            class TokenizerIntegration {
                +tokenizer: PreTrainedTokenizer
                +vocab_size: int
                +add_tokens()
                +validate_vocab_size()
            }

            ModelConfiguration "1" -- "1" PreTrainedModel : configures
            PreTrainedModel "1" -- "1" TrainingSetup : prepares for
            PreTrainedModel "1" -- "1" TokenizerIntegration : integrates with
            TrainingSetup "1" -- "*" PEFT : can use

            class PEFT {
                +method: str
                +lora_rank: int
                +apply_peft()
            }
        ```

        ### 🏗️ Choosing Model Architecture and Hyperparameters

        Start by matching your model architecture to your task:

        - **Encoder-only (e.g., BERT):** For understanding tasks like classification or NER
        - **Decoder-only (e.g., GPT):** For generative tasks such as text, code, or story generation
        - **Encoder-decoder (e.g., T5, BART):** For sequence-to-sequence tasks like translation

        ### 📊 Key Configuration Parameters

        | Parameter | Description | Typical Values |
        | --- | --- | --- |
        | **vocab_size** | Must match tokenizer output | 30K-50K (custom), 50K+ (general) |
        | **max_position_embeddings** | Maximum tokens per input | 512-2048 (standard), 4K-8K (long) |
        | **n_embd** | Embedding dimension | 768 (base), 1024-2048 (large) |
        | **n_layer** | Number of transformer layers | 12 (base), 24-48 (large) |
        | **n_head** | Attention heads | 12 (base), 16-32 (large) |
        | **use_cache** | Enable KV cache for generation | True (inference), False (training) |

        ### 🔧 Configuring a GPT-2 Model (Modern API)

        ```python
        from transformers import GPT2Config, GPT2LMHeadModel

        # Use modern config parameter names
        config = GPT2Config(
            vocab_size=30000,                # Match your tokenizer's vocab size
            max_position_embeddings=512,     # Max sequence length
            n_embd=768,                      # Embedding size
            n_layer=12,                      # Number of transformer layers
            n_head=12,                       # Number of attention heads
            use_cache=True                   # Enable caching for faster generation
        )

        model = GPT2LMHeadModel(config)

        # Sanity check: vocab size should match embedding matrix
        assert config.vocab_size == model.transformer.wte.weight.shape[0], "Vocab size mismatch!"
        ```

        > **⚠️ Important**: Always use `max_position_embeddings` (not the deprecated `n_positions`) for setting sequence length in configs.

        ### 🎯 Loading and Adapting Pre-trained Models

        ```python
        from transformers import GPT2TokenizerFast, GPT2LMHeadModel

        tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
        model = GPT2LMHeadModel.from_pretrained("gpt2")

        # If you add new tokens, resize embeddings
        new_tokens = ["<new_token1>", "<new_token2>"]
        num_added = tokenizer.add_tokens(new_tokens)
        if num_added > 0:
            model.resize_token_embeddings(len(tokenizer))

        # Model and tokenizer are now ready for domain-specific fine-tuning
        ```

        ### 🚄 Parameter-Efficient Fine-Tuning with Modern Models

        ```python
        # Using pyenv for Python 3.12.9
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Install with poetry
        poetry add transformers peft bitsandbytes accelerate

        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        from peft import LoraConfig, get_peft_model, TaskType

        # Load Llama-3 or Gemma-2 in 4-bit for memory efficiency
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype="float16",
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )

        # Example with Llama-3-8B (adjust model name to latest version)
        model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Meta-Llama-3-8B",  # Or "google/gemma-2-7b"
            quantization_config=bnb_config,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

        # Configure LoRA for efficient fine-tuning
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # LoRA rank
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"]  # Target attention layers
        )

        # Apply LoRA to the model
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()  # Shows only ~0.1% params are trainable!
        ```

        This example loads Llama-3-8B in 4-bit quantization and applies LoRA, reducing trainable parameters from 8B to just ~8M—a 1000x reduction! Perfect for fine-tuning on consumer GPUs while maintaining strong performance.

        **🔧 QLoRA Alternative**: For even more memory savings, use QLoRA which quantizes the base model to 4-bit while keeping LoRA adapters in fp16:

        ```python
        # QLoRA configuration for extreme efficiency
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=4,  # Even smaller rank for QLoRA
            lora_alpha=16,
            lora_dropout=0.05,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # All attention
            bias="none"
        )
        ```

        ### 🖥️ Training with Multiple GPUs and Distributed Setups

        As your model or dataset grows, a single GPU may not suffice. Multi-GPU and distributed training enable you to train faster and scale to larger models.

        ```bash
        accelerate config      # Set up your hardware interactively
        accelerate launch train.py
        ```

        **How it works:**

        - `accelerate config` prompts you to specify your hardware (number of GPUs, backend, precision)
        - `accelerate launch train.py` runs your training script with distributed setup

        For large-scale or memory-intensive training, integrate DeepSpeed or FairScale with Accelerate. These frameworks enable:

        - ⚡ ZeRO optimizations for memory efficiency
        - 💾 Gradient checkpointing and sharded training
        - 🏗️ Support for extremely large models (billions of parameters)

        ## 📈 Training, Evaluation, and Iteration

        With your data ready and model configured, it's time to train. Consider training like baking: monitor progress, check results, and adjust your recipe as you go.

        > **📓 Jupyter Notebook**: For an interactive walkthrough of these concepts, check out the [Building Custom Language Models notebook](/Users/richardhightower/src/art_hug_11/notebooks/building_custom_language_models.ipynb) that demonstrates data curation, model configuration, and training workflows with executable examples.

        ### 📊 Monitoring Loss and Metrics

        Effective training starts with careful monitoring. Just as a chef watches the oven, you need to watch your model's training and validation metrics.

        ### 📊 Key Metrics to Monitor

        | Metric | Task Type | Description | Example Use Case |
        |--------|-----------|-------------|------------------|
        | **Training Loss** | All | Measures model fit on training data | Should decrease steadily |
        | **Validation Loss** | All | Indicates generalization ability | Rising = overfitting |
        | **Perplexity** | Language Modeling | How well model predicts next token | Lower is better (e.g., 20-50) |
        | **Accuracy** | Classification | Percentage of correct predictions | Intent detection, sentiment |
        | **F1 Score** | Classification | Harmonic mean of precision/recall | Imbalanced datasets |
        | **BLEU** | Translation/Generation | N-gram overlap with references | Machine translation quality |
        | **ROUGE** | Summarization | Recall-oriented overlap measure | Text summarization tasks |
        | **BERTScore** | Generation | Semantic similarity using BERT | Modern alternative to BLEU |
        | **HELM** | General LLM | Holistic evaluation across tasks | Comprehensive model assessment |

        **💡 Pro Tip**: For production models, combine automated metrics with human evaluation for nuanced quality assessment.

        ### 🔬 Logging with Modern Experiment Tracking

        ```python
        from transformers import Trainer, TrainingArguments

        training_args = TrainingArguments(
            output_dir="./results",          
            evaluation_strategy="steps",     
            eval_steps=500,                  
            logging_steps=100,               
            save_steps=500,                  
            per_device_train_batch_size=2,   
            num_train_epochs=3,              
            report_to=["tensorboard", "wandb"], # Modern experiment tracking
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset
        )
        trainer.train()
        ```

        ### 📏 Using the Hugging Face Evaluate Library

        ```python
        from evaluate import load

        # Load metrics appropriate for your task
        accuracy = load("accuracy")
        f1 = load("f1")
        bleu = load("bleu")

        # Example usage in your evaluation loop:
        predictions = [...]  # Model outputs
        references = [...]   # Ground truth labels
        result = accuracy.compute(predictions=predictions, references=references)
        print(result)
        ```

        ### 🛑 Early Stopping and Checkpointing

        Don't waste resources by training longer than needed. Early stopping halts training when your model stops improving on the validation set.

        ```python
        from transformers import EarlyStoppingCallback

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        ```

        ### 🔍 Error Analysis and Iterative Improvement

        No model achieves perfection on the first try. The key to improvement involves iteration: analyze errors, adjust, and retrain.

        ```python
        from transformers import pipeline

        # Load your fine-tuned model
        text_generator = pipeline("text-generation", model="./results/checkpoint-1500")

        prompts = [
            "In accordance with the contract, the party of the first part shall",
            "The diagnosis was confirmed by the following procedure:"
        ]

        for prompt in prompts:
            output = text_generator(prompt, max_length=50, num_return_sequences=1)
            print(f"Prompt: {prompt}\nGenerated: {output[0]['generated_text']}\n")
        ```

        For systematic error analysis and human-in-the-loop annotation, consider using tools like Argilla. These platforms help teams label, review, and track errors across large datasets, accelerating the improvement cycle.

        ### ⚠️ Common Pitfalls and Solutions

        Avoid these frequent training issues to save time and compute:

        | Pitfall | Symptoms | Solution |
        |---------|----------|----------|
        | **OOM Errors** | CUDA out of memory | Reduce batch size, enable gradient accumulation, use mixed precision |
        | **Tokenizer Mismatch** | Unexpected tokens, errors | Verify vocab_size matches, check special tokens alignment |
        | **Learning Rate Issues** | Loss explosion or no progress | Use warmup, try different schedulers, start with 2e-5 |
        | **Data Leakage** | Unrealistic high performance | Ensure train/val/test splits are clean, check for duplicates |
        | **Checkpoint Bloat** | Disk space issues | Save only best models, delete intermediate checkpoints |
        | **Import Errors** | Missing dependencies | Use try/except blocks for optional libraries |
        | **API Key Issues** | Authentication failures | Validate keys before use, handle missing keys gracefully |

        **🚀 Quick Debugging Checklist:**
        - ✅ Print model and data shapes before training
        - ✅ Test with a tiny subset first (10-100 examples)
        - ✅ Monitor GPU memory with `nvidia-smi -l 1`
        - ✅ Use gradient clipping for stability
        - ✅ Enable anomaly detection in development: `torch.autograd.set_detect_anomaly(True)`
        - ✅ Handle import errors gracefully for optional dependencies
        - ✅ Validate API keys and environment variables early

        ## 🎯 Summary and Key Takeaways

        You've just completed a critical step in your AI journey: learning how to curate data and train custom language models using the latest Hugging Face tools. Let's recap the essentials:

        ### 1. 📊 Data Quality: The Foundation

        Great models start with great data. Choose sources that reflect your target domain. Clean out noise, duplicates, and irrelevant material.

        ```python
        import re
        from datasets import load_dataset

        def clean_text(example):
            example['text'] = re.sub(r'<.*?>', '', example['text'])  # Remove HTML
            example['text'] = re.sub(r'\s+', ' ', example['text'])   # Normalize whitespace
            return example

        dataset = load_dataset('wikipedia', '20240101.en', split='train', streaming=True)
        cleaned_dataset = dataset.map(clean_text)
        ```

        ### 2. 🔒 Efficient Processing, Privacy, and Reproducibility

        Big datasets demand efficient workflows. Streaming enables you to process data in batches without loading everything into memory.

        ```python
        from datasets import load_dataset

        dataset = load_dataset('wikipedia', '20240101.en', split='train', streaming=True)
        for i, example in enumerate(dataset):
            print(example['text'][:100])  # Show first 100 characters
            if i >= 2:
                break
        ```

        ### 3. 🚀 Model Configuration and Iterative Training

        Model setup matters deeply. Most modern projects use architectures like Llama-2, Mistral, or Falcon for LLM training and fine-tuning.

        ```python
        from transformers import AutoConfig, AutoModelForCausalLM

        config = AutoConfig.from_pretrained("meta-llama/Llama-2-7b-hf")
        model = AutoModelForCausalLM.from_config(config)
        # For most tasks, load from pre-trained weights:
        # model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
        ```

        ### 4. 🔗 Connecting to Advanced Skills

        Everything here prepares you for what's next: advanced fine-tuning (including LoRA/QLoRA), dataset management, and secure deployment. These foundations help you build, adapt, and scale models that deliver real business value.

        ### 📚 Key Takeaways

        - ✅ **Data quality drives results**
        - ✅ **Efficient, private, and reproducible workflows enable safe scaling**
        - ✅ **Modern model selection and parameter-efficient fine-tuning prove the norm**
        - ✅ **Experiment tracking and config-driven training ensure reliability**
        - ✅ **Iterate and refine for best performance**

        ### 📖 Glossary

        - **Tokenization:** Splitting text into model-ready pieces (tokens)
        - **Streaming:** Loading data in batches instead of all at once
        - **Checkpointing:** Saving your model's progress during training
        - **Early stopping:** Halting training when improvement stalls
        - **Parameter-efficient fine-tuning (PEFT):** Techniques like LoRA/QLoRA that update only a small subset of model parameters
        - **Human-in-the-loop:** Involving people in labeling or reviewing data
        - **Experiment tracking:** Logging runs, configs, and metrics with tools like MLflow, W&B, or HF Hub

        ### 🏃 Running the Examples

        This project includes a comprehensive set of examples demonstrating all the concepts covered in this guide. Use the Taskfile commands to run specific examples:

        ```bash
        # Run all examples
        task run

        # Run specific topic examples
        task run-prompt-engineering      # Prompt engineering techniques
        task run-few-shot-learning       # Few-shot learning examples
        task run-chain-of-thought        # Chain of thought reasoning
        task run-data-curation          # Data curation workflows
        task run-tokenization           # Custom tokenizer training
        task run-model-configuration    # Model configuration examples
        task run-training               # Training workflow demonstrations
        task run-constitutional-ai      # Constitutional AI examples

        # Development commands
        task test                       # Run all tests
        task format                     # Format code with Black and Ruff
        task clean                      # Clean up generated files

        # Jupyter notebooks
        task notebook                   # Open tutorial notebook
        task notebook-lab              # Open with JupyterLab
        ```

        ### 🔧 Troubleshooting Common Issues

        #### Accelerate Version Compatibility
        If you encounter issues with model loading or distributed training:
        ```bash
        # Ensure you have the correct accelerate version
        poetry show accelerate  # Should show ^0.26.0
        # Or reinstall
        poetry add accelerate@^0.26.0
        ```

        #### API Key Configuration Issues
        If examples fail with authentication errors:
        1. Verify `.env` file exists: `ls -la .env`
        2. Check API keys are set: `grep -v "your-.*-key-here" .env`
        3. Ensure no placeholder values remain in `.env`
        4. Restart your shell or run: `source .env`

        #### Bitsandbytes GPU Warning on macOS
        If you see warnings about bitsandbytes on macOS:
        ```
        # This is expected - bitsandbytes doesn't support Metal/MPS
        # The code will automatically fall back to CPU mode
        # For Apple Silicon, models will use MPS when available
        ```

        To suppress the warning, set:
        ```bash
        export BITSANDBYTES_NOWELCOME=1
        ```

        #### Memory Issues
        For out-of-memory errors:
        1. Reduce batch size in training configs
        2. Use gradient accumulation
        3. Enable mixed precision training
        4. Consider using parameter-efficient methods (LoRA/QLoRA)

        #### Import Errors
        If you encounter missing dependencies:
        ```bash
        # Reinstall all dependencies
        poetry install

        # Or for specific issues:
        poetry add [package_name]
        ```

        ### 🚀 Looking Ahead

        You now know how to curate data and train language models from scratch using modern, scalable tools. Next, you'll unlock advanced fine-tuning, parameter-efficient adaptation, and best practices for deploying models securely and efficiently. Review your own process—what can you improve? Keep building. Your AI journey continues! 🎉
      metadata:
        extension: .md
        size_bytes: 50231
        language: markdown
    docs/article11.md:
      content: |-
        # Building Custom Language Models: From Raw Data to AI Solutions

        In today's AI-driven world, the ability to create custom language models tailored to specific domains and tasks represents a critical competitive advantage. This comprehensive guide walks you through the complete lifecycle of building language models from the ground up—from curating high-quality datasets to training and refining powerful AI systems.

        Whether you're developing specialized models for healthcare, finance, legal services, or any domain requiring nuanced understanding, this chapter provides the practical knowledge and code examples you need to succeed. We'll explore modern techniques using the Hugging Face ecosystem that balance efficiency, scalability, and model quality.

        In this chapter, we'll cover:

        - Data curation fundamentals: selecting, cleaning, and preparing domain-specific text
        - Scalable processing techniques for handling massive datasets efficiently
        - Privacy protection and data versioning for responsible AI development
        - Modern model architecture selection and configuration strategies
        - Training workflows with distributed computing and experiment tracking
        - Parameter-efficient fine-tuning methods for adapting large models
        - Evaluation, error analysis, and iterative improvement techniques

        By the end of this chapter, you'll have both the theoretical understanding and practical skills to transform raw text into powerful, domain-specific language models that deliver real business value. Let's begin your journey toward AI mastery.

        # Dataset Curation and Training Language Models from Scratch - Article 11

        ```mermaid
        mindmap
          root((Dataset Curation & Training LLMs))
            Data Quality Foundation
              Source Selection
              Cleaning & Deduplication
              Labeling & Annotation
              Domain Relevance
            Modern Processing
              Streaming & Batching
              Cloud Integration
              Privacy & Compliance
              Versioning & Reproducibility
            Model Configuration
              Architecture Selection
              Parameter-Efficient Methods
              Pre-trained vs Scratch
              Distributed Training
            Training Workflow
              Monitoring & Metrics
              Early Stopping
              Checkpointing
              Iterative Improvement
            Production Ready
              Experiment Tracking
              Error Analysis
              Human-in-the-Loop
              Deployment Pipeline

        ```

        **Step-by-Step Explanation:**

        - Root node centers on **Dataset Curation & Training LLMs**
        - Branch covers **Data Quality Foundation** with cleaning and labeling essentials
        - Branch details **Modern Processing** including streaming and compliance
        - Branch explains **Model Configuration** from architecture to distributed setups
        - Branch shows **Training Workflow** with monitoring and iteration
        - Branch highlights **Production Ready** features for real-world deployment

        ## Introduction: From Raw Data to Custom Language Models

        To build a great language model, you need great data. Picture your dataset as ingredients for a gourmet meal—the fresher and more carefully chosen, the better the final dish. Even the most sophisticated AI architecture can't salvage a flawed foundation. **Ready to transform raw text into powerful AI?**

        This chapter explores why curating high-quality datasets remains the first—and most critical—step in building effective language models. In today's AI landscape, most practitioners start with a strong pre-trained model (available on the Hugging Face Model Hub) and fine-tune it on their domain-specific data. This approach proves efficient, cost-effective, and enables rapid adaptation to unique business, user, or privacy requirements. Training a model entirely from scratch typically serves organizations with massive datasets and compute resources, or highly specialized applications demanding complete control.

        Let's break down the key ideas, step by step.

        ### Setting Up Your Environment

        ```bash
        # Using pyenv (recommended for Python version management)
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Verify Python version
        python --version  # Should show Python 3.12.9

        # Install with poetry (recommended)
        poetry new dataset-curation-project
        cd dataset-curation-project
        poetry env use 3.12.9
        poetry add datasets transformers tokenizers torch accelerate

        # Or use mini-conda
        conda create -n dataset-curation python=3.12.9
        conda activate dataset-curation
        pip install datasets transformers tokenizers torch accelerate

        # Or use pip with pyenv
        pyenv install 3.12.9
        pyenv local 3.12.9
        pip install datasets transformers tokenizers torch accelerate

        ```

        ### Why Data Quality Matters

        "Garbage in, garbage out" remains a core truth in AI. If your dataset contains messy, biased, or irrelevant content, your model will reflect those flaws with painful accuracy. Picture a financial company using a generic model that misses industry-specific terms or context. By curating a dataset of financial documents, you enable your model to actually understand your domain—transforming it from a general assistant into a specialist.

        ### The Value of Fine-Tuning and Custom Training

        Pre-trained models like GPT-3, BERT, or Llama 3 trained on vast, general data. But sometimes, you need a model that speaks your language—literally. Fine-tuning empowers you to:

        - Include rare or industry-specific vocabulary seamlessly
        - Filter out sensitive or irrelevant content precisely
        - Meet strict privacy or compliance requirements confidently
        - Rapidly adapt to new domains with limited resources

        Training from scratch still matters for certain scenarios: when you require a fully custom architecture, need control over every aspect of the model, or must guarantee data residency and privacy.

        **Example:** A healthcare provider can fine-tune a pre-trained model on anonymized clinical notes using Hugging Face's Trainer API, ensuring it understands medical jargon while respecting patient privacy completely.

        ### What Modern Data Curation Involves

        Curation transcends merely collecting files. It demands:

        - Selecting relevant, diverse sources strategically
        - Cleaning and standardizing text meticulously
        - Removing duplicates and noise (including semantic deduplication)
        - Annotating and labeling (with tools like Argilla for human-in-the-loop workflows)
        - Tokenizing (splitting text into model-friendly pieces) and building vocabulary that fits your domain
        - Versioning and tracking your data for reproducibility

        Modern workflows often leverage the Hugging Face Datasets library for scalable, memory-efficient data loading and transformation. For large-scale or streaming data, Datasets supports processing data on-the-fly, making it possible to curate web-scale corpora without exhausting memory.

        Increasingly, LLMs themselves handle advanced data cleaning, deduplication, and even initial annotation—automating the removal of outliers, semantic duplicates, and low-quality samples. Synthetic data generation with LLMs or diffusion models also provides a common strategy to augment datasets and improve model robustness, especially in low-resource or highly specialized domains.

        Let's examine a simple example. Suppose your customer service logs contain HTML tags, inconsistent spacing, and duplicates. Here's a basic cleaning function in Python using the Hugging Face Datasets library:

        ### Basic Data Cleaning with Hugging Face Datasets

        ```python
        import re
        from datasets import load_dataset

        dataset = load_dataset("csv", data_files="customer_logs.csv")

        def clean_text(example):
            # Remove HTML tags
            text = re.sub(r'<.*?>', '', example["text"])
            # Replace multiple spaces/newlines with a single space
            text = re.sub(r'\\s+', ' ', text)
            # Strip leading/trailing whitespace
            text = text.strip()
            return {"text": text}

        cleaned_dataset = dataset.map(clean_text)

        print(cleaned_dataset["train"][0]["text"])  # Output: Cleaned text sample

        ```

        **How this works:**

        1. Loads your dataset using Hugging Face Datasets (supports CSV, JSON, Parquet, and streaming)
        2. Removes HTML tags like `<p>` completely
        3. Replaces extra spaces or newlines with a single space
        4. Trims spaces from the start and end

        This represents a starting point. In production, you may employ LLM-assisted cleaning for more complex tasks—such as detecting semantic duplicates, flagging outliers, or even auto-labeling data.

        ### Why Invest in Modern Data Curation?

        High-quality data becomes your competitive edge. Custom-curated and well-annotated datasets allow your models to:

        - Outperform generic models in specialized tasks dramatically
        - Reduce errors in critical business processes significantly
        - Ensure privacy and regulatory compliance completely
        - Enable support for rare languages or unique domains effectively
        - Adapt quickly to new requirements using fine-tuning or continual learning

        Tools like Hugging Face Datasets and Argilla (for collaborative annotation and versioning) now represent standards for scalable, reproducible, and team-based data workflows.

        Mastering data curation—and knowing when to fine-tune versus train from scratch—lets you build models that fit your needs, not just what's available off the shelf. **Ever wondered what unique data could give your model an unstoppable edge?**

        ### What's Next?

        Next, we'll dive into practical steps for preparing and curating high-quality datasets: selecting sources, cleaning (with both code and LLM-based methods), labeling with modern annotation tools, and building custom tokenizers. We'll also explore streaming and versioning strategies for large-scale data, and introduce synthetic data augmentation.

        For more on tokenization, see Article 5. For privacy, advanced curation, and human-in-the-loop annotation, see Articles 11 and 12.

        ## Preparing and Curating High-Quality Datasets

        Great language models start with great data. Picture your dataset as soil—rich and well-tended data grows robust AI. In this section, you'll master the essentials: how to select, clean, label, and tokenize text using up-to-date tools and best practices, so your model learns from the best possible foundation.

        Regardless of your use case—legal analysis, customer support, or scientific summarization—the quality, diversity, and freshness of your dataset directly shape your results. We'll break down each step of the modern data curation process.

        ```mermaid
        flowchart TB
            subgraph DataCuration[Dataset Curation Pipeline]
                RawData[Raw Text Data<br>Wikipedia, Domain Docs]
                Selection[Source Selection<br>Relevance & Diversity]
                Cleaning[Text Cleaning<br>HTML, URLs, Noise]
                Dedup[Deduplication<br>Exact & Semantic]
                Labeling[Annotation<br>Human-in-the-Loop]
                Tokenization[Tokenization<br>Custom Vocabulary]

                RawData -->|Evaluate| Selection
                Selection -->|Process| Cleaning
                Cleaning -->|Remove Duplicates| Dedup
                Dedup -->|Add Labels| Labeling
                Labeling -->|Prepare for Model| Tokenization
            end

            Tokenization -->|Ready| TrainingData[Training-Ready Dataset]

            classDef default fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            class RawData,Selection,Cleaning,Dedup,Labeling,Tokenization,TrainingData default

        ```

        **Step-by-Step Explanation:**

        - `Raw Text Data` includes sources like Wikipedia and domain documents
        - `Source Selection` evaluates relevance and diversity
        - `Text Cleaning` removes HTML, URLs, and noise
        - `Deduplication` eliminates exact and semantic duplicates
        - `Annotation` adds labels with human-in-the-loop processes
        - `Tokenization` creates custom vocabulary for your domain
        - Final output is a `Training-Ready Dataset`

        ### Selecting and Cleaning Raw Text Data

        Start by choosing data that matches your application. For general models, Common Crawl and Wikipedia remain foundational—ensure you use recent snapshots to avoid model staleness. For specialized tasks, seek out up-to-date, domain-specific corpora (e.g., legal documents, medical texts, business logs).

        ### Data Quality Checklist

        | Criterion | Key Questions | Why It Matters |
        | --- | --- | --- |
        | **Relevance** | Does the text match your target use case? | Ensures model learns domain-specific patterns |
        | **Diversity** | Is there a mix of topics, styles, and authors? | Prevents bias and improves generalization |
        | **Quality** | Is the text well-formed and free of noise? | Reduces training on corrupted examples |
        | **Freshness** | Are you using the latest available data? | Prevents model drift and outdated knowledge |

        Modern NLP cleaning surpasses simple regex. Expect to encounter boilerplate (headers/footers), duplicates, HTML tags, URLs, emojis, code snippets, and potentially sensitive or offensive content. Leverage scalable tools to handle these challenges across large datasets.

        ### Scalable Text Cleaning and Deduplication with Hugging Face Datasets

        ```python
        import datasets
        import unicodedata
        import re

        # Load a recent English Wikipedia snapshot
        wiki = datasets.load_dataset("wikipedia", "20220301.en", split="train")

        # Unicode normalization and basic cleaning
        def clean_text(example):
            text = unicodedata.normalize('NFKC', example['text'])  # Unicode normalization
            text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
            text = re.sub(r'https?://\\S+', '', text)  # Remove URLs
            text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace
            text = text.strip()
            return {"text": text}

        # Apply cleaning
        wiki = wiki.map(clean_text, num_proc=4)

        # Remove duplicates
        wiki = wiki.unique("text")

        ```

        This example uses Hugging Face Datasets for efficient, parallel cleaning and deduplication. It applies Unicode normalization, removes HTML tags, URLs, and normalizes whitespace. For large-scale projects, always use `.map()` and `.unique()` for performance and reproducibility.

        ### Automated Language Detection and Filtering

        ```python
        from langdetect import detect

        def filter_english(example):
            try:
                return detect(example['text']) == 'en'
            except:
                return False

        # Filter for English language only
        wiki = wiki.filter(filter_english, num_proc=4)

        ```

        Language detection (e.g., with `langdetect` or `fasttext`) ensures your dataset matches your target language. For multilingual projects, use automated filtering to maintain consistency.

        For privacy and compliance, it's now standard to remove or mask personally identifiable information (PII) using Named Entity Recognition (NER) or regular expressions. Libraries like spaCy or Presidio help automate this process.

        **Tip:** Always check random samples after cleaning and filtering. This helps verify that your pipeline hasn't removed too much—or left in unwanted content.

        For versioning and reproducibility, consider using DVC (Data Version Control) or Hugging Face Datasets' built-in versioning features to track changes in your dataset over time.

        **Key takeaway:** Modern, scalable cleaning pipelines—paired with versioning—prove essential for robust model training.

        ### Human-in-the-Loop Data Labeling

        For many tasks, clean data isn't enough—you need high-quality labels. While some models (like large language models) can learn from raw text, most business applications require supervised, labeled examples—such as sentiment, intent, or domain-specific categories.

        Automated labeling runs fast, but people catch subtleties that machines miss: sarcasm, legal nuance, or rare edge cases. Human-in-the-loop annotation ensures your dataset reflects real-world needs and reduces hidden biases.

        **Example:** Building a support chatbot to flag 'urgent' messages. Automated rules might miss context, but human annotators spot urgency even when it's subtle. Modern tools like Argilla (see Article 12) support collaborative labeling, track annotator disagreements, and provide audit trails for reproducibility.

        Best practices for human annotation:

        - Write clear, detailed instructions and provide examples for annotators
        - Use multiple annotators per example to catch mistakes and reduce bias
        - Regularly review disagreements, update guidelines, and retrain annotators as needed
        - Ensure privacy: Mask or remove PII before annotation, especially in sensitive domains

        Synthetic data augmentation—generating labeled examples using LLMs or data generation tools—can supplement human-annotated data, especially for rare classes or low-resource domains. See Article 8 for practical augmentation workflows.

        Investing in quality labels pays off—especially in high-stakes or nuanced domains. For advanced annotation workflows and continuous feedback loops, see Article 12.

        ### Tokenization and Vocabulary Creation

        Clean, labeled data stands almost ready—but models can't use raw text. They need tokens: units like words, subwords, or characters. Tokenization bridges the gap, and modern workflows rely on integrated Hugging Face APIs for both standard and custom tokenizers.

        Picture tokenization as slicing bread. How you cut (tokenize) determines how well the filling (meaning) fits. For general English, pre-trained tokenizers (e.g., BERT, GPT-2) work well. For specialized or multilingual data, training a custom tokenizer—often with SentencePiece Unigram or BPE—can boost performance dramatically.

        Popular tokenization algorithms:

        - **SentencePiece Unigram:** Flexible and robust for multilingual and domain-specific tasks. Widely used in T5 and recent LLMs
        - **Byte-Pair Encoding (BPE):** Splits rare words into subwords, balancing vocabulary size and coverage
        - **WordPiece:** Used in BERT; similar to BPE but merges differently

        For most workflows, use Hugging Face's `AutoTokenizer` and the `transformers` library for seamless integration with models and pipelines. If you need to train a custom tokenizer, leverage the `tokenizers` or `sentencepiece` integration.

        ### Training a Custom Tokenizer with Hugging Face Transformers (SentencePiece)

        ```python
        from transformers import AutoTokenizer, PreTrainedTokenizerFast
        from tokenizers import trainers, Tokenizer, models, pre_tokenizers, processors

        # Example: Train a Unigram tokenizer using SentencePiece
        from tokenizers import Tokenizer, models, pre_tokenizers, trainers

        # Load your cleaned text file(s)
        files = ["./data/cleaned_corpus.txt"]

        # Initialize a Unigram model
        tokenizer = Tokenizer(models.Unigram())
        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
        trainer = trainers.UnigramTrainer(vocab_size=30000, special_tokens=["<pad>", "<unk>", "<s>", "</s>"])
        tokenizer.train(files, trainer)

        tokenizer.save("./tokenizer-unigram.json")

        # Load into Hugging Face for use with models
        hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")

        ```

        This code trains a Unigram tokenizer (SentencePiece-style) on your cleaned data, builds a vocabulary of 30,000 tokens, and saves it for use with Hugging Face models. For most use cases, prefer SentencePiece/Unigram for new projects unless you have a specific architecture requirement.

        ### Using Your Trained Tokenizer with Hugging Face Transformers

        ```python
        from transformers import PreTrainedTokenizerFast

        # Load the trained tokenizer
        hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file="./tokenizer-unigram.json")

        # Tokenize a domain-specific sentence
        print(hf_tokenizer.tokenize("myocardial infarction"))
        # Output: ['myocardial', 'infarction']  # Example output

        ```

        Test your tokenizer on real, domain-specific sentences to ensure important terms aren't split awkwardly. For example, in medical data, 'myocardial infarction' should not fragment into meaningless subwords.

        For large-scale and streaming data, Hugging Face Datasets supports tokenization pipelines using `.map()` and can process data on-the-fly, making it easy to scale up efficiently.

        **Key takeaway:** A well-designed tokenizer and vocabulary—built with modern Hugging Face APIs—reduces unknown tokens and helps your model understand your domain.

        For a deep dive into tokenization strategies and debugging, see Article 5.

        ### Key Takeaways

        - Select data that fits your task, remains as diverse and recent as possible, and matches your language/domain needs
        - Clean thoroughly using scalable pipelines to remove noise, bias, and sensitive information
        - Use human annotators and modern labeling tools for high-quality, reliable labels
        - Tokenize and build a vocabulary tailored to your data and domain using current Hugging Face APIs
        - Track dataset versions for reproducibility and auditability

        Solid, modern data preparation forms the bedrock of successful model training. For advanced data workflows, augmentation, and annotation strategies, see Articles 8 and 12.

        ## Scaling Data Processing and Streaming

        As your projects grow, so do your datasets—sometimes reaching terabytes or more. Loading all this data at once resembles trying to cook every dish in a restaurant simultaneously: the kitchen will grind to a halt. Instead, you need smart, scalable workflows to keep things running smoothly. In this section, you'll master how to process massive datasets with streaming and batching, track every change for reproducibility, and protect sensitive data throughout your pipeline using the latest tools and best practices.

        ```mermaid
        stateDiagram-v2
            [*] --> DataDiscovery
            DataDiscovery --> StreamingSetup: Dataset Located
            StreamingSetup --> BatchProcessing: Streaming Enabled
            BatchProcessing --> DataValidation: Batches Processed
            DataValidation --> VersionControl: Quality Verified
            VersionControl --> PrivacyCheck: Changes Tracked
            PrivacyCheck --> ProductionReady: PII Removed
            ProductionReady --> [*]

            BatchProcessing --> ErrorHandling: Processing Error
            ErrorHandling --> BatchProcessing: Retry
            PrivacyCheck --> DataValidation: Privacy Issue Found

            style DataDiscovery fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style StreamingSetup fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style BatchProcessing fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style DataValidation fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333
            style VersionControl fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style PrivacyCheck fill:#ffcdd2,stroke:#e53935,stroke-width:1px,color:#333333
            style ProductionReady fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333
            style ErrorHandling fill:#ffcdd2,stroke:#e53935,stroke-width:1px,color:#333333


        ```

        **Step-by-Step Explanation:**

        - System starts at `DataDiscovery` to locate datasets
        - Moves to `StreamingSetup` once dataset is found
        - `BatchProcessing` handles data in manageable chunks
        - `DataValidation` ensures quality standards
        - `VersionControl` tracks all changes
        - `PrivacyCheck` removes sensitive information
        - System reaches `ProductionReady` state when complete
        - Error paths allow retry and privacy remediation

        ### Handling Large-Scale Data with Streaming

        When working with huge datasets—think the size of Wikipedia or larger—downloading everything to your machine just isn't practical. This is where streaming and batching come into play. Data streaming lets you process one record at a time, or in manageable batches, keeping only a small portion in memory. Picture it as a conveyor belt: you handle each item as it passes, never piling up a mountain of plates.

        The Hugging Face Datasets library makes streaming and batching large datasets straightforward. With streaming enabled, you can iterate over massive datasets efficiently, even on a laptop. You can also stream directly from cloud storage (like S3, GCS, or Azure Blob) by specifying the appropriate dataset path and credentials—a common practice in production environments.

        ### Listing Available Wikipedia Dataset Versions

        ```python
        from datasets import get_dataset_config_names

        # List all available Wikipedia dumps (by date)
        print(get_dataset_config_names('wikipedia'))

        ```

        ### Streaming and Batch Processing with 🤗 Datasets

        ```python
        from datasets import load_dataset

        def process_batch(batch):
            # Example batch processing (e.g., truncating text)
            return {"processed_text": [t[:200] for t in batch["text"]]}

        # Always use the latest Wikipedia config (e.g., '20240101.en')
        streamed_dataset = load_dataset('wikipedia', '20240101.en', split='train', streaming=True)

        # Efficiently process data in batches of 1000
        processed = streamed_dataset.map(process_batch, batched=True, batch_size=1000)

        # Iterate over the first processed batch
        for i, example in enumerate(processed):
            print(example["processed_text"])
            if i >= 2:
                break

        ```

        Let's break it down:

        1. `get_dataset_config_names` lets you discover the latest dataset versions, so you're always working with up-to-date data
        2. `streaming=True` tells the library to read data one sample at a time, directly from the source—no full download required
        3. Batch processing with `batched=True` and `batch_size=1000` lets you efficiently process chunks of data, ideal for tokenization, filtering, or augmentation
        4. This streaming approach works for datasets of any size and can read directly from cloud storage, making it ideal for both local and cloud workflows

        For even larger-scale or distributed workflows, consider integrating with frameworks like Ray or Spark, or using event streaming platforms (like Kafka or Pulsar) to handle real-time or enterprise-scale data pipelines. Internally, Hugging Face Datasets leverages Apache Arrow for efficient memory management and can interoperate with these distributed systems.

        **Key takeaway:** Modern streaming and batching let you process huge datasets efficiently, with minimal memory or storage requirements. For production-scale needs, leverage cloud storage and distributed frameworks for seamless scalability.

        ### Annotation, Versioning, and Reproducibility

        In professional AI projects, you must know exactly what data went into your model and how it was processed. Consider this as keeping a precise recipe—so you (or your team) can always recreate results or explain decisions. This proves especially critical as datasets and teams scale.

        Here's how to keep your data process transparent and repeatable:

        - **Version control for data:** Use tools like DVC (Data Version Control) to track every change, just like Git tracks code. For cloud-native or petabyte-scale workflows, consider LakeFS or Databricks Delta Lake, which offer data versioning directly on object storage
        - **Annotation tracking:** If you label or edit data, log who did what, when, and why, ideally with automated metadata
        - **Reproducibility:** Record all preprocessing steps, random seeds, and environment details so anyone can reproduce your results

        ### Tracking Dataset Versions with DVC

        ```bash
        # Initialize DVC in your project
        $ dvc init

        # Add your raw dataset to DVC tracking
        $ dvc add data/raw_corpus.txt

        # Commit the change (with metadata)
        $ git add data/raw_corpus.txt.dvc .gitignore
        $ git commit -m "Add raw corpus to DVC tracking"

        # After cleaning or labeling, add the new version
        $ dvc add data/cleaned_corpus.txt
        $ git add data/cleaned_corpus.txt.dvc
        $ git commit -m "Add cleaned corpus version"

        ```

        What happens here?

        1. `dvc init` sets up DVC for your project
        2. `dvc add` tracks your dataset, creating a metadata file
        3. Use `git` to commit both code and data versions
        4. When you update your data, repeat the process. You can always roll back or audit any version

        For cloud-native projects, LakeFS and Delta Lake integrate with S3 or Azure Blob, offering Git-like semantics for data versioning at scale.

        For full transparency, document every preprocessing step and annotation guideline in a **dataset card**—a structured summary of your data, including its source, cleaning steps, and known limitations. See Article 14 for best practices on dataset cards and responsible data sharing.

        **Summary:** Track, document, and version your data and annotations to ensure your work remains reproducible and trustworthy.

        ### Ensuring Data Privacy and Security

        When your data includes personal or confidential information, privacy transcends optional—it's a legal and ethical requirement. Imagine your dataset contains user feedback: what happens if names or emails leak? Protecting sensitive data proves as important as any step in your pipeline, and modern privacy tools make this easier and more robust than ever.

        The main strategies:

        - **PII detection and removal:** Use automated tools to scan for names, emails, and phone numbers. For production, transformer-based or LLM-powered PII detection (such as with `presidio-analyzer` using transformer models, or OpenAI's GPT-4o via API) can provide higher recall, especially for multilingual or context-dependent data
        - **Anonymization:** Replace sensitive details with tokens (like `[EMAIL]` or `[NAME]`) or hash values
        - **Differential privacy and synthetic data:** For highly sensitive use cases, consider applying differential privacy (using tools like TensorFlow Privacy or OpenDP) or generating synthetic data to minimize disclosure risk
        - **Access controls and encryption:** Store sensitive data in secure, access-controlled locations. Use encryption at rest and in transit, especially in the cloud

        ### Simple PII Redaction Example (Regex-Based)

        ```python
        import re

        def redact_pii(text):
            # Basic email pattern
            text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)
            # Basic phone pattern
            text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)
            # Basic name pattern (not robust)
            text = re.sub(r'Mr\\.\\s+\\w+|Ms\\.\\s+\\w+|Dr\\.\\s+\\w+', '[NAME]', text)
            return text

        sample = "Contact Dr. Smith at dr.smith@example.com or 555-123-4567."
        print(redact_pii(sample))  # Output: Contact [NAME] at [EMAIL] or [PHONE].

        ```

        This function uses regular expressions to redact emails, phone numbers, and some names. **Note:** these patterns remain basic. For production, use dedicated libraries like `presidio` or `scrubadub`, which now support transformer-based models for more robust and multilingual detection. For the highest accuracy, LLM-based approaches (using models from Hugging Face or OpenAI) can detect nuanced or context-dependent PII.

        For advanced privacy needs, consider differential privacy techniques (see TensorFlow Privacy or OpenDP) or synthetic data generation, which can help you share insights without exposing real user data.

        Always combine automated redaction with spot checks, especially for high-stakes applications. Stay up to date with data protection laws like GDPR, HIPAA, or CCPA, and consult your compliance team when handling regulated data. For deployment and operational security, see Article 15.

        **Summary:** Protect privacy by detecting and redacting sensitive information using the latest tools, securing access, and following all relevant regulations. For advanced use cases, leverage transformer-based PII detection and differential privacy techniques.

        ## Configuring and Initializing a Model

        With your dataset ready, it's time to transform raw data into a working language model. This section walks you through four key steps: selecting the right architecture, setting core hyperparameters, initializing your model, and preparing for scalable, efficient training. All guidance reflects the latest Hugging Face and PyTorch ecosystem best practices.

        ```mermaid
        classDiagram
            class ModelConfiguration {
                +architecture_type: str
                +vocab_size: int
                +max_position_embeddings: int
                +n_embd: int
                +n_layer: int
                +n_head: int
                +use_cache: bool
                +select_architecture()
                +set_hyperparameters()
                +validate_config()
            }

            class PreTrainedModel {
                +model_name: str
                +config: ModelConfiguration
                +from_pretrained()
                +resize_token_embeddings()
                +save_pretrained()
            }

            class TrainingSetup {
                +learning_rate: float
                +batch_size: int
                +optimizer: AdamW
                +mixed_precision: bool
                +distributed: bool
                +setup_training()
                +enable_peft()
            }

            class TokenizerIntegration {
                +tokenizer: PreTrainedTokenizer
                +vocab_size: int
                +add_tokens()
                +validate_vocab_size()
            }

            ModelConfiguration "1" -- "1" PreTrainedModel : configures
            PreTrainedModel "1" -- "1" TrainingSetup : prepares for
            PreTrainedModel "1" -- "1" TokenizerIntegration : integrates with
            TrainingSetup "1" -- "*" PEFT : can use

            class PEFT {
                +method: str
                +lora_rank: int
                +apply_peft()
            }

        ```

        **Step-by-Step Explanation:**

        - `ModelConfiguration` manages architecture selection and hyperparameters
        - `PreTrainedModel` loads and configures models from Hugging Face
        - `TrainingSetup` handles training configuration including PEFT
        - `TokenizerIntegration` ensures vocabulary alignment
        - `PEFT` provides parameter-efficient fine-tuning options

        ### Choosing Model Architecture and Hyperparameters

        Start by matching your model architecture to your task:

        - **Encoder-only (e.g., BERT):** For understanding tasks like classification or named entity recognition
        - **Decoder-only (e.g., GPT):** For generative tasks such as text, code, or story generation
        - **Encoder-decoder (e.g., T5, BART):** For sequence-to-sequence tasks like translation or summarization

        (See Article 4 for a deeper dive into these architectures.)

        Modern workflows overwhelmingly favor **fine-tuning pre-trained models** over training from scratch. Pre-trained weights capture general language knowledge, dramatically reducing compute requirements and improving performance. Only train from scratch if you have unique data, a new language, or conduct foundational research.

        ### Key Configuration Parameters

        | Parameter | Description | Typical Values |
        | --- | --- | --- |
        | **vocab_size** | Must match tokenizer output | 30K-50K (custom), 50K+ (general) |
        | **max_position_embeddings** | Maximum tokens per input | 512-2048 (standard), 4K-8K (long) |
        | **n_embd** | Embedding dimension | 768 (base), 1024-2048 (large) |
        | **n_layer** | Number of transformer layers | 12 (base), 24-48 (large) |
        | **n_head** | Attention heads | 12 (base), 16-32 (large) |
        | **use_cache** | Enable KV cache for generation | True (inference), False (training) |

        **Tip:** If resources remain limited, start with a smaller model or use parameter-efficient fine-tuning (see below).

        ### Configuring a GPT-2 Model from Scratch (Modern API)

        ```python
        from transformers import GPT2Config, GPT2LMHeadModel

        # Use modern config parameter names
        config = GPT2Config(
            vocab_size=30000,                # Match your tokenizer's vocab size
            max_position_embeddings=512,     # Max sequence length (preferred)
            n_embd=768,                      # Embedding size
            n_layer=12,                      # Number of transformer layers
            n_head=12,                       # Number of attention heads
            use_cache=True                   # Enable caching for faster generation
        )

        model = GPT2LMHeadModel(config)

        # Sanity check: vocab size should match embedding matrix
        assert config.vocab_size == model.transformer.wte.weight.shape[0], "Vocab size mismatch!"

        ```

        However, the standard and most efficient practice involves starting from a pre-trained model and fine-tuning it for your data. This leverages the extensive knowledge already learned by the model.

        ### Loading and Adapting a Pre-trained GPT-2 Model

        ```python
        from transformers import GPT2TokenizerFast, GPT2LMHeadModel

        tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
        model = GPT2LMHeadModel.from_pretrained("gpt2")

        # If you add new tokens, resize embeddings
        new_tokens = ["<new_token1>", "<new_token2>"]
        num_added = tokenizer.add_tokens(new_tokens)
        if num_added > 0:
            model.resize_token_embeddings(len(tokenizer))

        # Model and tokenizer are now ready for domain-specific fine-tuning

        ```

        Key points:

        - Use `max_position_embeddings` (not `n_positions`) for setting sequence length in configs
        - Always ensure `vocab_size` matches your tokenizer's vocabulary (use `tokenizer.vocab_size`)
        - When adding new tokens, resize the model's embeddings accordingly

        **Parameter-efficient fine-tuning (PEFT)** methods such as LoRA, Prefix Tuning, and Adapters now represent standards for large models and can dramatically reduce compute and memory requirements. See Article 12 for practical guidance.

        ### Quick PEFT Example with Mistral-7B

        ```python
        # Using pyenv for Python 3.12.9
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Install with poetry
        poetry add transformers peft bitsandbytes accelerate

        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        from peft import LoraConfig, get_peft_model, TaskType

        # Load Mistral-7B in 4-bit for memory efficiency
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype="float16",
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )

        model = AutoModelForCausalLM.from_pretrained(
            "mistralai/Mistral-7B-v0.1",
            quantization_config=bnb_config,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

        # Configure LoRA for efficient fine-tuning
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # LoRA rank
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"]  # Target attention layers
        )

        # Apply LoRA to the model
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()  # Shows only ~0.1% params are trainable!

        ```

        This example loads Mistral-7B in 4-bit quantization and applies LoRA, reducing trainable parameters from 7B to just ~7M—a 1000x reduction! Perfect for fine-tuning on consumer GPUs while maintaining strong performance.

        Other critical hyperparameters (set during training):

        - **Learning rate:** Use a scheduler and start conservatively; too high can destabilize training
        - **Batch size:** Larger batches improve stability but require more memory
        - **Optimizer:** Use `torch.optim.AdamW`, not the legacy `transformers.optimization.AdamW`

        We'll cover these in detail in the training loop section. For now, keep them in mind as you design your model.

        Summary checklist:

        - [ ]  Select the right architecture for your task
        - [ ]  Use pre-trained weights and fine-tune unless you have a specific reason to train from scratch
        - [ ]  Set hyperparameters using modern config parameters (`max_position_embeddings`, etc.)
        - [ ]  Double-check that `vocab_size` matches your tokenizer

        ### Initializing Weights and Embeddings

        Initialization means setting starting values for all model weights and embeddings. In modern workflows, **fine-tuning pre-trained models** remains the norm: weights already contain knowledge from massive datasets. Random initialization only serves training from scratch, which proves rare outside research or highly novel domains.

        When training from scratch, Hugging Face Transformers and PyTorch use robust random initialization schemes (like Xavier or Kaiming) for transformer layers. For most applications, this suffices. For highly specialized vocabularies or domains, consider parameter-efficient fine-tuning (PEFT) or, in rare cases, seeding embeddings with domain-specific vectors. However, adapters and LoRA now prove preferable to manual embedding initialization.

        ### Inspecting Model Parameters and Embedding Shapes

        ```python
        print(model)  # Print a summary of the model architecture

        # Check the shape of the embedding matrix
        print("Embedding matrix shape:", model.transformer.wte.weight.shape)

        # Sanity check: Should be (vocab_size, n_embd)
        assert model.transformer.wte.weight.shape == (model.config.vocab_size, model.config.n_embd), "Embedding shape mismatch!"

        ```

        Checklist before training:

        - Use `print(model)` to review architecture and dimensions
        - Ensure the embedding matrix shape equals `(vocab_size, n_embd)` and matches your config and tokenizer
        - If you add new tokens to the tokenizer, always call `model.resize_token_embeddings(len(tokenizer))`

        **Advanced tip:** For highly specialized domains, parameter-efficient fine-tuning (e.g., LoRA, adapters) allows you to adapt large models to new vocabularies or tasks efficiently. Directly seeding embeddings with vectors (Word2Vec, FastText) rarely proves necessary in 2025, as PEFT and pre-trained models deliver more effective and robust results.

        Summary checklist:

        - [ ]  Confirm model and embedding shapes before training
        - [ ]  Prefer PEFT or pre-trained models for domain adaptation
        - [ ]  Only customize embedding initialization for rare, highly specialized cases

        ### Training with Multiple GPUs and Distributed Setups

        As your model or dataset grows, a single GPU may not suffice. Multi-GPU and distributed training enable you to train faster and scale to larger models. This proves essential for real-world projects and production-grade deployment.

        The Hugging Face Accelerate library simplifies distributed training across multiple GPUs and nodes. It manages device placement, data parallelism, and supports mixed-precision training. For even larger models or more advanced memory optimizations, consider integrating **DeepSpeed** or **FairScale**—both widely support ZeRO optimizations, gradient checkpointing, and efficient scaling.

        ### Launching Distributed Training with Accelerate

        ```bash
        accelerate config      # Set up your hardware interactively
        accelerate launch train.py

        ```

        How it works:

        - `accelerate config` prompts you to specify your hardware (number of GPUs, backend, precision) and saves these settings
        - `accelerate launch train.py` runs your training script with distributed setup, requiring minimal script changes

        For large-scale or memory-intensive training, integrate DeepSpeed or FairScale with Accelerate. These frameworks enable:

        - ZeRO optimizations for memory efficiency
        - Gradient checkpointing and sharded training
        - Support for extremely large models (billions of parameters)

        See Article 17 for advanced distributed training workflows.

        For mixed-precision training, use PyTorch's native AMP (`torch.cuda.amp`)—now standard for speeding up training and reducing memory usage. Accelerate can enable AMP automatically based on your config.

        **Tip:** Monitor GPU and CPU usage with `nvidia-smi`, cloud dashboards, or integrate with modern monitoring tools like Weights & Biases or TensorBoard for observability and debugging.

        **Business example:** A healthcare chatbot with a domain-specific model may run on a single GPU. Training a general-purpose LLM for enterprise search, however, requires distributed training to complete in a reasonable timeframe and control cloud costs.

        Summary checklist:

        - [ ]  Use Accelerate for distributed training; add DeepSpeed or FairScale for large-scale efficiency
        - [ ]  Enable mixed-precision with AMP for faster, memory-efficient training
        - [ ]  Monitor hardware and training metrics with modern tools
        - [ ]  Start with small-scale runs and scale up as needed

        ## Training, Evaluation, and Iteration

        With your data ready and model configured, it's time to train. Consider training like baking: monitor progress, check results, and adjust your recipe as you go. In this section, you'll master how to monitor your model's learning, protect your progress, and refine your model through practical iteration—using modern tools such as experiment trackers, distributed training libraries, and advanced evaluation metrics.

        ### Monitoring Loss, Validation Metrics, and Modern Experiment Tracking

        Effective training starts with careful monitoring. Just as a chef watches the oven, you need to watch your model's training and validation metrics. This helps you catch problems before they waste time or resources. Today's best practices extend beyond just loss and perplexity—robust model evaluation includes accuracy, F1-score, BLEU, ROUGE, and even human-centric metrics, depending on your NLP task.

        Key metrics to monitor, depending on your task:

        - **Training Loss:** Measures how well the model fits your training data. Should decrease as learning progresses
        - **Validation Loss:** Indicates how well the model generalizes to new data. If validation loss rises while training loss falls, your model likely overfits
        - **Perplexity:** For language modeling, measures how well the model predicts the next word. Lower perplexity means better predictions
        - **Accuracy, Precision, Recall, F1-score:** For classification or sequence labeling tasks, these measure prediction quality
        - **BLEU, ROUGE:** For translation and summarization, these compare generated text to references
        - **Human or Task-Specific Metrics:** In production or research, human evaluation or domain-specific scores (e.g., faithfulness, toxicity) prove increasingly important

        To track these metrics during training, set up logging and visualization using the Hugging Face Trainer API, with support for modern experiment tracking tools and standardized metric computation.

        ### Logging Training Metrics with Trainer API and Experiment Tracking

        ```python
        from transformers import Trainer, TrainingArguments

        training_args = TrainingArguments(
            output_dir="./results",          # Save outputs here
            evaluation_strategy="steps",     # Evaluate every N steps
            eval_steps=500,                  # Evaluation interval
            logging_steps=100,               # Log every 100 steps
            save_steps=500,                  # Checkpoint interval
            per_device_train_batch_size=2,   # Batch size per GPU
            num_train_epochs=3,              # Number of training epochs
            report_to=["tensorboard", "wandb"], # Enable TensorBoard and Weights & Biases
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset
        )
        trainer.train()

        # To visualize in TensorBoard:
        #   tensorboard --logdir ./results
        # For Weights & Biases, login with wandb and view runs in the dashboard.

        ```

        The `report_to` parameter enables seamless integration with tools like TensorBoard and Weights & Biases (W&B), making it easy to visualize and compare experiments at scale. For even richer experiment management, you can use MLflow, Neptune, or other supported trackers.

        For standardized metric computation across tasks, use the Hugging Face `evaluate` library:

        ### Using the Hugging Face Evaluate Library

        ```python
        from evaluate import load

        # Load a metric appropriate for your task
        accuracy = load("accuracy")
        f1 = load("f1")
        bleu = load("bleu")

        # Example usage in your evaluation loop:
        predictions = [...]  # Model outputs
        references = [...]   # Ground truth labels
        result = accuracy.compute(predictions=predictions, references=references)
        print(result)

        ```

        Watch your logs for these patterns:

        - Both training and validation loss decrease: Good progress
        - Validation loss rises while training loss falls: Overfitting—consider early stopping or regularization
        - Loss plateaus or spikes: There may be a bug, data issue, or learning rate problem
        - Task-specific metrics stagnate or degrade: Review your data and evaluation setup

        **Tip:** Adjust `eval_steps` or `logging_steps` for the right monitoring granularity. For large models or datasets, consider distributed training with Hugging Face Accelerate or DeepSpeed (see Article 17).

        For more on evaluation metrics, troubleshooting, and distributed training, see Articles 10 and 17.

        ### Early Stopping, Checkpointing, and Distributed Training

        Don't waste resources by training longer than needed. Early stopping halts training when your model stops improving on the validation set, while checkpointing saves your progress so you can recover from interruptions. Both prove essential for robust, large-scale training workflows.

        Both remain simple to set up in the Hugging Face Trainer. Here's how to add early stopping:

        ### Adding Early Stopping Callback

        ```python
        from transformers import EarlyStoppingCallback

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop after 3 evaluations with no improvement
        )

        ```

        How it works:

        - **EarlyStoppingCallback:** Monitors validation loss (or your chosen metric) and stops training if there's no improvement for several evaluations
        - **Checkpointing:** The `save_steps` parameter in `TrainingArguments` saves your model every few steps. If training gets interrupted, reload the latest checkpoint and continue

        **Tip:** Monitor disk space and periodically delete old checkpoints you don't need, especially when working with large models.

        For large-scale or multi-GPU training, Hugging Face Accelerate, DeepSpeed, and FSDP (Fully Sharded Data Parallel) offer seamless scaling across hardware. These integrations support directly in Trainer workflows and prove essential for efficient, memory-optimized training of modern transformer models.

        See Article 17 for detailed guidance on distributed training and scaling.

        In business and research settings, these safeguards protect your investment in time and compute. For custom training workflows and advanced checkpoint management, see Article 10.

        ### Error Analysis and Iterative Improvement

        No model achieves perfection on the first try. The key to improvement involves iteration: analyze errors, adjust, and retrain. Modern workflows combine manual inspection with systematic error analysis tools.

        After training, look beyond metrics. Sample your model's outputs, compare them to the ground truth, and note where it fails. For example, if your legal language model stumbles on rare terms, you may need more examples of those in your data.

        Here's a quick way to sample outputs for manual review:

        ### Sampling Model Outputs for Error Analysis

        ```python
        from transformers import pipeline

        # Load your fine-tuned model
        text_generator = pipeline("text-generation", model="./results/checkpoint-1500")

        prompts = [
            "In accordance with the contract, the party of the first part shall",
            "The diagnosis was confirmed by the following procedure:"
        ]

        for prompt in prompts:
            output = text_generator(prompt, max_length=50, num_return_sequences=1)
            print(f"Prompt: {prompt}\\nGenerated: {output[0]['generated_text']}\\n")

        ```

        Step by step:

        1. Load your fine-tuned model with the Hugging Face `pipeline`
        2. Choose relevant prompts from your domain
        3. Generate and review outputs for errors

        Keep a log of frequent mistakes. Use these insights to clean your data, adjust hyperparameters (like learning rate or batch size), or even tweak your model's architecture. Hyperparameters control training settings—such as batch size, learning rate, and number of epochs.

        For systematic error analysis and human-in-the-loop annotation, consider using tools like Argilla (see Article 12). These platforms help teams label, review, and track errors across large datasets, accelerating the improvement cycle.

        Action checklist:

        - Review outputs for real errors, not just metric numbers
        - Add more data for weak spots
        - Adjust settings and retrain

        Each cycle brings your model closer to robust, reliable performance.

        For advanced fine-tuning, parameter-efficient adaptation (e.g., LoRA), and deployment, see Article 12.

        ### Key Takeaways

        - Monitor loss, perplexity, and task-specific metrics with robust tools like TensorBoard, Weights & Biases, and the Evaluate library
        - Use early stopping and checkpointing to protect your work and accelerate experimentation
        - For large models, leverage distributed training with Accelerate, DeepSpeed, or FSDP
        - Analyze errors using both manual review and advanced tools like Argilla, iterating for better results

        For deeper dives into evaluation, see Article 10. For advanced fine-tuning, error analysis, and deployment, continue to Articles 12 and 17.

        ## Summary and Key Takeaways

        You've just completed a critical step in your AI journey: learning how to curate data and train custom language models from scratch using the latest Hugging Face tools. Let's recap the essentials, highlight what matters most today, and see how these skills connect to advanced workflows and deployment.

        While training language models from scratch remains vital for research or highly specialized domains, most practical applications in 2025 start with pre-trained models and fine-tune them for specific tasks. Full training from scratch proves resource-intensive and less common for general business use cases. However, understanding the foundations prepares you for both scenarios brilliantly.

        Building a strong model resembles constructing a house: a solid foundation, careful planning, and ongoing adjustments all matter. In practice, this means focusing on high-quality data, scalable and private processing, and smart, reproducible model configuration.

        Let's break down each core idea, with practical steps and real-world relevance.

        ### 1. Data Quality: The Foundation

        Great models start with great data. Choose sources that reflect your target domain. Clean out noise, duplicates, and irrelevant material. For specialized tasks, label data carefully—often with human-in-the-loop workflows, where people review or annotate tricky cases for higher accuracy.

        **Example:** If you're building a finance chatbot, use finance documents—not just generic news. The right data shapes your model's strengths and reduces bias.

        Key steps:

        - **Select** data that matches your use case
        - **Clean** to remove errors and duplicates
        - **Label** with care, especially for nuanced tasks
        - **Tokenize** (split text into model-friendly pieces)

        For large datasets, prefer the Hugging Face `datasets` library for cleaning, mapping, and deduplication, as it's optimized for scalability and integrates seamlessly with modern LLM workflows.

        ### Sample Data Cleaning Pipeline with Hugging Face Datasets

        ```python
        import re
        from datasets import load_dataset

        def clean_text(example):
            example['text'] = re.sub(r'<.*?>', '', example['text'])  # Remove HTML tags
            example['text'] = re.sub(r'\\s+', ' ', example['text'])  # Normalize whitespace
            return example

        dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)
        cleaned_dataset = dataset.map(clean_text)
        # For deduplication, use .unique or filter as needed

        ```

        This pipeline uses the Hugging Face Datasets library for memory-efficient cleaning and mapping. It easily scales to massive datasets and proves preferred over pandas for large-scale NLP projects.

        ### 2. Efficient Processing, Privacy, and Reproducibility

        Big datasets demand efficient workflows. Streaming enables you to process data in batches without loading everything into memory—essential for scalability.

        If your data contains sensitive information (e.g., medical or personal records), always anonymize personal information and document every transformation. Privacy remains mandatory in regulated fields, and compliance with laws like GDPR or HIPAA proves non-negotiable.

        ### Streaming a Large Dataset with Hugging Face Datasets

        ```python
        from datasets import load_dataset

        dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)
        for i, example in enumerate(dataset):
            print(example['text'][:100])  # Show first 100 characters
            if i >= 2:
                break

        ```

        Here, you stream Wikipedia data—processing each example as needed, no matter the dataset size. This approach proves memory-efficient and production-ready.

        For reproducibility, document every data change, set random seeds, and track annotation rules. Tools like DVC, MLflow, Weights & Biases, and Hugging Face Hub versioning now represent standards for tracking data, experiments, and model artifacts. Dataset cards (see Article 14) help you document data provenance and quality.

        Always protect privacy: remove personal data, set access controls, and follow regulations. See Article 15 for secure deployment practices.

        ### 3. Model Configuration and Iterative Training

        Model setup matters deeply. While GPT-2 provides a classic example, most modern projects use architectures like Llama-2, Mistral, or Falcon for LLM training and fine-tuning. The Hugging Face `AutoModelForCausalLM` and `AutoConfig` APIs make it easy to load and configure these state-of-the-art models.

        ### Configuring a Modern LLM (e.g., Llama-2) for Fine-Tuning

        ```python
        from transformers import AutoConfig, AutoModelForCausalLM

        config = AutoConfig.from_pretrained("meta-llama/Llama-2-7b-hf")
        model = AutoModelForCausalLM.from_config(config)
        # For most tasks, you will load from pre-trained weights:
        # model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

        ```

        This example shows how to configure and load a modern LLM architecture like Llama-2. In practice, you'll usually start from pre-trained weights and fine-tune on your domain data for best results and efficiency.

        For most real-world projects, **parameter-efficient fine-tuning** methods such as LoRA and QLoRA prove recommended. These techniques allow you to adapt large models to new tasks with minimal compute and memory overhead, rather than retraining the entire model. See Article 12 for hands-on details.

        Modern training workflows remain configuration-driven and tracked for reproducibility. Instead of hardcoding parameters, use YAML or JSON files to define hyperparameters, model paths, and training arguments. This makes experiments easier to manage and share.

        ### Example Training Configuration (YAML)

        ```yaml
        model_name_or_path: "meta-llama/Llama-2-7b-hf"
        per_device_train_batch_size: 4
        num_train_epochs: 3
        learning_rate: 2e-5
        output_dir: "./results"
        fp16: true  # Enable mixed-precision for efficiency

        ```

        Using a configuration file like this, you can launch and reproduce experiments across different environments. Many experiment tracking tools (e.g., MLflow, Weights & Biases) integrate directly with these configs.

        During training, monitor your metrics closely. Use early stopping to halt if progress stalls, and checkpoint to save your work. Mixed-precision (fp16/bf16) training and gradient accumulation prove standard for optimizing speed and memory usage. Distributed training with Hugging Face Accelerate or DeepSpeed proves recommended for scaling up. Track experiments, metrics, and artifacts using MLflow, Weights & Biases, or Hugging Face Hub.

        ### Trainer Setup with Early Stopping, Checkpointing, and Mixed Precision

        ```python
        from transformers import Trainer, TrainingArguments, EarlyStoppingCallback

        training_args = TrainingArguments(
            output_dir="./results",
            evaluation_strategy="steps",
            eval_steps=500,
            save_steps=500,
            logging_steps=100,
            per_device_train_batch_size=4,
            num_train_epochs=3,
            report_to=["wandb"],  # or "mlflow", "hf_hub"
            fp16=True,             # Enable mixed-precision
            load_best_model_at_end=True
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        trainer.train()

        ```

        This setup logs progress, evaluates and saves checkpoints every 500 steps, uses mixed-precision for efficiency, and integrates with experiment tracking platforms. Early stopping prevents wasted compute when validation loss stops improving.

        After training, always review results and iterate. Even small tweaks in data, parameters, or fine-tuning strategies (like switching to QLoRA) can yield significant improvements.

        ### 4. Connecting to Advanced Skills

        Everything here prepares you for what's next: advanced fine-tuning (see Article 12, including LoRA/QLoRA), dataset management (Article 14), and secure deployment (Article 15). These foundations help you build, adapt, and scale models that deliver real business value.

        ### Key Takeaways

        - **Data quality drives results**
        - **Efficient, private, and reproducible workflows enable safe scaling**
        - **Modern model selection and parameter-efficient fine-tuning prove the norm**
        - **Experiment tracking and config-driven training ensure reliability**
        - **Iterate and refine for best performance**

        ### Glossary

        - **Tokenization:** Splitting text into model-ready pieces (tokens)
        - **Streaming:** Loading data in batches instead of all at once
        - **Checkpointing:** Saving your model's progress during training
        - **Early stopping:** Halting training when improvement stalls
        - **Parameter-efficient fine-tuning (PEFT):** Techniques like LoRA/QLoRA that update only a small subset of model parameters for efficiency
        - **Human-in-the-loop:** Involving people in labeling or reviewing data
        - **Experiment tracking:** Logging runs, configs, and metrics with tools like MLflow, Weights & Biases, or Hugging Face Hub

        ### Looking Ahead

        You now know how to curate data and train language models from scratch using modern, scalable tools. Next, you'll unlock advanced fine-tuning (Article 12), parameter-efficient adaptation, and best practices for deploying models securely and efficiently (Article 15). Review your own process—what can you improve? Keep building. Your AI journey continues.

        ## Summary

        This chapter provided a hands-on, practical roadmap for curating datasets and training language models from scratch with Hugging Face. From selecting and cleaning raw data, through scalable processing and privacy safeguards, to configuring and iterating on your model, you now possess the essential skills needed to create robust, custom AI solutions. These foundations prepare you for advanced fine-tuning, deployment, and responsible AI development covered in subsequent articles.
      metadata:
        extension: .md
        size_bytes: 61562
        language: markdown
    .yamlproject/config.yaml:
      content: |
        # Default configuration for YAML Project
        include_pattern: null
        exclude_pattern: null
        temp_dir: null
        backup_dir: null
        supported_extensions:
          .py: python
          .sh: bash
          .java: java
          .js: javascript
          .jsx: javascript
          .ts: typescript
          .tsx: typescript
          .html: html
          .css: css
          .md: markdown
          .yml: yaml
          .yaml: yaml
          .json: json
          .txt: text
          .go: go
          .rs: rust
          .rb: ruby
          .php: php
          .c: c
          .cpp: cpp
          .h: c
          .hpp: cpp
          .cs: csharp
          .toml: toml
          .xml: xml
          .sql: sql
          .kt: kotlin
          .swift: swift
          .dart: dart
          .r: r
          .scala: scala
          .pl: perl
          .lua: lua
          .ini: ini
          .cfg: ini
          .properties: properties
        forbidden_dirs:
          - __pycache__
          - node_modules
          - dist
          - cdk.out
          - env
          - venv
          - .venv
          - .idea
          - build
          - .git
          - .svn
          - .hg
          - .DS_Store
          - .vs
          - .vscode
          - target
          - bin
          - obj
          - out
          - Debug
          - Release
          - tmp
          - .tox
          - .pytest_cache
          - __MACOSX
          - .mypy_cache
          - tests
        outfile: project.yaml
        log_level: INFO
        max_file_size: 204800  # 200KB
        metadata_fields:
          - extension
          - size_bytes
          - language
        yaml_format:
          indent: 2
          width: 120
      metadata:
        extension: .yaml
        size_bytes: 1103
        language: yaml
    data/sample_medical_corpus.txt:
      content: |
        myocardial infarction is a serious condition requiring immediate treatment
        The patient presented with acute myocardial symptoms and chest pain
        Diagnosis confirmed myocardial damage through ECG and blood tests
        Treatment for myocardial conditions includes medication and surgery
        Post-myocardial care is essential for patient recovery
        Electrocardiogram showed ST-segment elevation indicating infarction
        Thrombolytic therapy is standard treatment for acute cases
        Cardiac catheterization revealed significant coronary stenosis
      metadata:
        extension: .txt
        size_bytes: 522
        language: text
    data/sample_corpus.txt:
      content: |
        myocardial infarction is a serious condition
        The patient presented with acute myocardial symptoms
        Diagnosis confirmed myocardial damage
        Treatment for myocardial conditions includes medication
        Post-myocardial care is essential
      metadata:
        extension: .txt
        size_bytes: 226
        language: text
    data/custom-tokenizer.json:
      content: |-
        {
          "version": "1.0",
          "truncation": null,
          "padding": null,
          "added_tokens": [
            {
              "id": 0,
              "content": "<pad>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 1,
              "content": "<unk>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 2,
              "content": "<s>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 3,
              "content": "</s>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            }
          ],
          "normalizer": null,
          "pre_tokenizer": {
            "type": "Whitespace"
          },
          "post_processor": null,
          "decoder": null,
          "model": {
            "type": "Unigram",
            "unk_id": null,
            "vocab": [
              [
                "<pad>",
                0.0
              ],
              [
                "<unk>",
                0.0
              ],
              [
                "<s>",
                0.0
              ],
              [
                "</s>",
                0.0
              ],
              [
                "m",
                -2.6789067733447025
              ],
              [
                "s",
                -2.8189253048375766
              ],
              [
                "o",
                -2.820233576833107
              ],
              [
                "i",
                -2.8450622934891587
              ],
              [
                "a",
                -2.8741913128367047
              ],
              [
                "l",
                -2.9610836998149748
              ],
              [
                "y",
                -3.127750366481642
              ],
              [
                "car",
                -3.1549257989027217
              ],
              [
                "di",
                -3.639655840209628
              ],
              [
                "e",
                -3.672632432014299
              ],
              [
                "d",
                -3.7412868976750993
              ],
              [
                "r",
                -3.792432872564746
              ],
              [
                "c",
                -3.7998109279752126
              ],
              [
                "p",
                -3.9110836997750162
              ],
              [
                "u",
                -3.9110836998149727
              ],
              [
                "t",
                -4.16344655877052
              ],
              [
                "f",
                -4.336757902556216
              ],
              [
                "ent",
                -4.337165321042806
              ],
              [
                "re",
                -4.404465203147301
              ],
              [
                "T",
                -4.4110836998149745
              ],
              [
                "h",
                -4.411083699814975
              ],
              [
                "ag",
                -4.411085215576343
              ],
              [
                "condition",
                -4.411213675053771
              ],
              [
                "tion",
                -4.41127040420999
              ],
              [
                "ia",
                -4.435930089492209
              ],
              [
                "sent",
                -4.491216327183718
              ],
              [
                "es",
                -4.537116471841484
              ],
              [
                "med",
                -4.541442727511527
              ],
              [
                "os",
                -4.758408164270939
              ],
              [
                "in",
                -5.217650555792403
              ],
              [
                "n",
                -5.274921940913284
              ],
              [
                "P",
                -5.411083699814972
              ],
              [
                "-",
                -5.411083699814975
              ],
              [
                "w",
                -5.411083699814975
              ],
              [
                "D",
                -5.411083699814975
              ],
              [
                "ca",
                -5.504062527005317
              ],
              [
                "ed",
                -5.526190559675131
              ],
              [
                "ati",
                -5.529397375851158
              ],
              [
                "te",
                -5.5577035708447236
              ],
              [
                "con",
                -5.580198476596903
              ],
              [
                "at",
                -5.591661811481362
              ],
              [
                "nf",
                -5.6269328596951675
              ],
              [
                "it",
                -5.753122731112787
              ],
              [
                "ar",
                -5.7564182748686985
              ],
              [
                "se",
                -5.879056860353778
              ],
              [
                "g",
                -6.4930569119767245
              ],
              [
                "io",
                -6.4930569119767245
              ]
            ],
            "byte_fallback": false
          }
        }
      metadata:
        extension: .json
        size_bytes: 3933
        language: json
    data/medical-tokenizer.json:
      content: |-
        {
          "version": "1.0",
          "truncation": null,
          "padding": null,
          "added_tokens": [
            {
              "id": 0,
              "content": "<pad>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 1,
              "content": "<unk>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 2,
              "content": "<s>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            },
            {
              "id": 3,
              "content": "</s>",
              "single_word": false,
              "lstrip": false,
              "rstrip": false,
              "normalized": false,
              "special": true
            }
          ],
          "normalizer": null,
          "pre_tokenizer": {
            "type": "Whitespace"
          },
          "post_processor": null,
          "decoder": null,
          "model": {
            "type": "Unigram",
            "unk_id": null,
            "vocab": [
              [
                "<pad>",
                0.0
              ],
              [
                "<unk>",
                0.0
              ],
              [
                "<s>",
                0.0
              ],
              [
                "</s>",
                0.0
              ],
              [
                "s",
                -2.8042668144044063
              ],
              [
                "m",
                -3.0401623154044186
              ],
              [
                "i",
                -3.182101589576122
              ],
              [
                "y",
                -3.1951877189795206
              ],
              [
                "o",
                -3.530761350066458
              ],
              [
                "c",
                -3.552115002574519
              ],
              [
                "re",
                -3.594905745289253
              ],
              [
                "r",
                -3.6539431346120814
              ],
              [
                "al",
                -3.6980105184713303
              ],
              [
                "t",
                -3.787321985906134
              ],
              [
                "ent",
                -3.8379074298790345
              ],
              [
                "ocardi",
                -3.8408571092552783
              ],
              [
                "u",
                -3.8649366341173454
              ],
              [
                "a",
                -3.898749962841752
              ],
              [
                "g",
                -3.9348876898074847
              ],
              [
                "in",
                -4.024810697535809
              ],
              [
                "f",
                -4.070483886450121
              ],
              [
                "tion",
                -4.155122282718086
              ],
              [
                "p",
                -4.191446531466092
              ],
              [
                "e",
                -4.2094141816060375
              ],
              [
                "l",
                -4.236837418913666
              ],
              [
                "ca",
                -4.287220840464651
              ],
              [
                "and",
                -4.290957839266917
              ],
              [
                "at",
                -4.380580024545052
              ],
              [
                "st",
                -4.4316798740846854
              ],
              [
                "te",
                -4.51329359621556
              ],
              [
                "or",
                -4.53141611521303
              ],
              [
                "d",
                -4.572361519061365
              ],
              [
                "ed",
                -4.605960253166077
              ],
              [
                "ar",
                -4.672924093756755
              ],
              [
                "T",
                -4.810926493700138
              ],
              [
                "h",
                -4.856546657211657
              ],
              [
                "es",
                -4.8940346201058835
              ],
              [
                "E",
                -5.124155972960129
              ],
              [
                "b",
                -5.124155972960131
              ],
              [
                "C",
                -5.124155973000082
              ],
              [
                "-",
                -5.124155973000085
              ],
              [
                "w",
                -5.124155973000085
              ],
              [
                "condition",
                -5.124157704557566
              ],
              [
                "ve",
                -5.13633874004362
              ],
              [
                "gn",
                -5.166966332689113
              ],
              [
                "ac",
                -5.234224608670597
              ],
              [
                "the",
                -5.236203779620462
              ],
              [
                "eri",
                -5.307745877519824
              ],
              [
                "sent",
                -5.355877265131485
              ],
              [
                "pa",
                -5.39378628551952
              ],
              [
                "medi",
                -5.448330869315797
              ],
              [
                "co",
                -5.467773750092115
              ],
              [
                "n",
                -5.514382839427488
              ],
              [
                "Th",
                -5.57610962875216
              ],
              [
                "si",
                -5.5791284532373595
              ],
              [
                "th",
                -5.58924559265877
              ],
              [
                "ti",
                -5.603174067936157
              ],
              [
                "ge",
                -5.6626310677575
              ],
              [
                "on",
                -5.723073133349362
              ],
              [
                "ra",
                -5.758937883354483
              ],
              [
                "ati",
                -5.769496936678133
              ],
              [
                "nt",
                -5.774637937656518
              ],
              [
                "se",
                -5.825444783464718
              ],
              [
                "dicati",
                -5.8593790079596575
              ],
              [
                "am",
                -5.90798882036945
              ],
              [
                "ou",
                -5.926580703063717
              ],
              [
                "os",
                -5.942547833388195
              ],
              [
                "med",
                -5.950533273390594
              ],
              [
                "fi",
                -6.000713170251337
              ],
              [
                "nf",
                -6.040626491965843
              ],
              [
                "om",
                -6.079538229871239
              ],
              [
                "D",
                -6.124155973000079
              ],
              [
                "P",
                -6.124155973000081
              ],
              [
                "q",
                -6.124155973000085
              ],
              [
                "G",
                -6.124155973000085
              ],
              [
                "S",
                -6.124155973000085
              ],
              [
                "z",
                -6.124155973000085
              ],
              [
                "ardia",
                -6.154263078988411
              ],
              [
                "ard",
                -6.168962111772203
              ],
              [
                "le",
                -6.195234482935753
              ],
              [
                "ev",
                -6.227120452355097
              ],
              [
                "ir",
                -6.235838093765628
              ],
              [
                "he",
                -6.241109300380828
              ],
              [
                "hro",
                -6.287790450557174
              ],
              [
                "ia",
                -6.315663816995935
              ],
              [
                "en",
                -6.4077096447750135
              ],
              [
                "con",
                -6.772274733455387
              ],
              [
                "est",
                -7.079894347614278
              ],
              [
                "v",
                -7.096108132002528
              ],
              [
                "tr",
                -7.096108132002528
              ]
            ],
            "byte_fallback": false
          }
        }
      metadata:
        extension: .json
        size_bytes: 6197
        language: json
    src/config.py:
      content: |
        """Configuration module for examples."""

        import os
        from pathlib import Path
        from typing import Optional, Literal
        from dotenv import load_dotenv
        import warnings

        # Load environment variables
        load_dotenv()

        # Project paths
        PROJECT_ROOT = Path(__file__).parent.parent
        DATA_DIR = PROJECT_ROOT / "data"
        MODELS_DIR = PROJECT_ROOT / "models"

        # Create directories if they don't exist
        DATA_DIR.mkdir(exist_ok=True)
        MODELS_DIR.mkdir(exist_ok=True)

        # Model configurations
        DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "bert-base-uncased")
        BATCH_SIZE = int(os.getenv("BATCH_SIZE", "8"))
        MAX_LENGTH = int(os.getenv("MAX_LENGTH", "512"))

        # API keys (if needed)
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
        HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")


        def validate_api_key(key_name: str, key_value: Optional[str], required: bool = False) -> bool:
            """
            Validate an API key.
            
            Args:
                key_name: Name of the API key (for error messages)
                key_value: The actual API key value
                required: Whether the key is required for operation
                
            Returns:
                bool: True if valid, False otherwise
                
            Raises:
                ValueError: If required key is missing or invalid
            """
            if not key_value:
                if required:
                    raise ValueError(f"{key_name} is required but not set in environment variables")
                else:
                    warnings.warn(f"{key_name} not found in environment variables", UserWarning)
                    return False
            
            # Basic validation - check if it's not just whitespace
            if not key_value.strip():
                if required:
                    raise ValueError(f"{key_name} is set but empty")
                return False
            
            # Check for placeholder values
            if key_value.lower() in ["your-api-key-here", "placeholder", "xxx", "todo"]:
                if required:
                    raise ValueError(f"{key_name} contains a placeholder value")
                warnings.warn(f"{key_name} contains a placeholder value", UserWarning)
                return False
            
            return True


        # Validate API keys on import (optional validation)
        validate_api_key("OPENAI_API_KEY", OPENAI_API_KEY, required=False)
        validate_api_key("ANTHROPIC_API_KEY", ANTHROPIC_API_KEY, required=False)
        validate_api_key("HUGGINGFACE_TOKEN", HF_TOKEN, required=False)


        # Device configuration
        import torch


        def get_device() -> Literal["mps", "cuda", "cpu"]:
            """
            Get the best available device for PyTorch computation.
            
            Returns:
                str: Device string - "mps" for Apple Silicon, "cuda" for NVIDIA GPU, or "cpu"
            """
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                

        DEVICE = get_device()
      metadata:
        extension: .py
        size_bytes: 2757
        language: python
    src/training_workflow.py:
      content: |-
        """Training Workflow Examples."""

        from transformers import (
            Trainer, TrainingArguments, EarlyStoppingCallback,
            AutoModelForCausalLM, AutoTokenizer,
            DataCollatorForLanguageModeling,
            pipeline
        )
        from datasets import Dataset
        import evaluate
        import torch
        from config import get_device, MODELS_DIR
        import numpy as np


        def create_sample_dataset():
            """Create a sample dataset for training."""
            # Sample medical texts
            texts = [
                "The patient presented with chest pain and shortness of breath.",
                "Diagnosis confirmed myocardial infarction based on ECG results.",
                "Treatment included aspirin and thrombolytic therapy.",
                "Post-operative care following cardiac surgery is essential.",
                "Regular monitoring of cardiac function recommended.",
                "Patient history includes hypertension and diabetes.",
                "Electrocardiogram showed ST-segment elevation.",
                "Cardiac catheterization revealed significant stenosis."
            ]
            
            return Dataset.from_dict({"text": texts})


        def basic_training_setup():
            """Basic training setup with logging."""
            print("Basic Training Setup...")
            
            # Load small model for demonstration
            model_name = "distilgpt2"
            model = AutoModelForCausalLM.from_pretrained(model_name)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # Set padding token
            tokenizer.pad_token = tokenizer.eos_token
            
            # Create dataset
            dataset = create_sample_dataset()
            
            # Tokenize dataset
            def tokenize_function(examples):
                return tokenizer(
                    examples["text"],
                    padding=True,
                    truncation=True,
                    max_length=128
                )
            
            tokenized_dataset = dataset.map(tokenize_function, batched=True)
            
            # Split into train/eval
            split_dataset = tokenized_dataset.train_test_split(test_size=0.2)
            
            # Data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False,  # Causal LM
            )
            
            # Training arguments - simplified for compatibility
            training_args = TrainingArguments(
                output_dir="./results",
                evaluation_strategy="steps",
                eval_steps=5,
                logging_steps=5,
                save_steps=10,
                per_device_train_batch_size=2,
                per_device_eval_batch_size=2,
                num_train_epochs=1,
                warmup_steps=10,
                logging_dir='./logs',
                report_to=[],  # Disable reporting for compatibility
                load_best_model_at_end=False,  # Simplify for demo
            )
            
            try:
                # Create trainer
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=split_dataset["train"],
                    eval_dataset=split_dataset["test"],
                    data_collator=data_collator,
                )
            except TypeError as e:
                print(f"Note: Trainer initialization issue (likely version mismatch): {e}")
                print("Creating a simplified training demo instead...")
                return None
            
            print("Training configuration:")
            print(f"  - Model: {model_name}")
            print(f"  - Train samples: {len(split_dataset['train'])}")
            print(f"  - Eval samples: {len(split_dataset['test'])}")
            print(f"  - Device: {get_device()}")
            
            return trainer


        def training_with_metrics():
            """Training with custom metrics and evaluation."""
            print("Training with Metrics...")
            
            # Setup trainer
            trainer = basic_training_setup()
            
            if trainer is None:
                print("Skipping metrics example due to trainer initialization issue")
                return
            
            # Load evaluation metrics
            accuracy = evaluate.load("accuracy")
            perplexity = evaluate.load("perplexity")
            
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                
                # Calculate perplexity
                loss = predictions[0] if isinstance(predictions, tuple) else predictions
                perplexity_score = np.exp(np.mean(loss))
                
                return {
                    "perplexity": perplexity_score
                }
            
            # Update trainer with metrics
            trainer.compute_metrics = compute_metrics
            
            print("\nStarting training with metrics...")
            # Note: We'll do a minimal training for demonstration
            trainer.args.num_train_epochs = 1
            trainer.args.max_steps = 10
            
            # Train
            train_result = trainer.train()
            
            print("\nTraining completed!")
            print(f"Final loss: {train_result.training_loss:.4f}")
            
            # Evaluate
            eval_results = trainer.evaluate()
            print(f"Eval loss: {eval_results['eval_loss']:.4f}")
            if 'eval_perplexity' in eval_results:
                print(f"Eval perplexity: {eval_results['eval_perplexity']:.4f}")


        def early_stopping_example():
            """Training with early stopping."""
            print("Early Stopping Example...")
            
            # Setup trainer
            trainer = basic_training_setup()
            
            if trainer is None:
                print("Skipping early stopping example due to trainer initialization issue")
                return
            
            # Add early stopping callback
            trainer.add_callback(
                EarlyStoppingCallback(
                    early_stopping_patience=3,
                    early_stopping_threshold=0.001
                )
            )
            
            print("\nTraining with early stopping (patience=3)...")
            trainer.args.num_train_epochs = 10  # Set high to test early stopping
            trainer.args.max_steps = 20
            
            # Train
            trainer.train()
            
            print("\nTraining completed (may have stopped early)")


        def mixed_precision_training():
            """Training with mixed precision for efficiency."""
            print("Mixed Precision Training...")
            
            # Check if GPU supports mixed precision
            device = get_device()
            fp16_available = device == "cuda"  # Only CUDA supports fp16 in transformers
            
            if not fp16_available:
                print(f"Mixed precision (fp16) not available on {device}. Using fp32.")
                print("Note: fp16 is only supported on CUDA devices in transformers")
            
            # Training arguments with mixed precision
            training_args = TrainingArguments(
                output_dir="./results_fp16",
                evaluation_strategy="steps",
                eval_steps=5,
                save_steps=10,
                per_device_train_batch_size=4,
                num_train_epochs=1,
                fp16=fp16_available,  # Enable mixed precision only if CUDA available
                logging_steps=5,
                report_to=[],  # Disable reporting for this example
                max_steps=10,  # Limit steps for demonstration
            )
            
            # Load model and tokenizer
            model = AutoModelForCausalLM.from_pretrained("distilgpt2")
            tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
            tokenizer.pad_token = tokenizer.eos_token
            
            # Create simple dataset
            dataset = create_sample_dataset()
            tokenized = dataset.map(
                lambda x: tokenizer(x["text"], padding=True, truncation=True, max_length=128),
                batched=True
            )
            
            # Trainer
            try:
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=tokenized,
                    tokenizer=tokenizer,
                )
                
                print(f"\nTraining with fp16={fp16_available}")
                trainer.train()
                
                print("Mixed precision training completed!")
            except Exception as e:
                print(f"Note: Training failed due to: {e}")
                print("This is likely due to version compatibility issues")


        def error_analysis():
            """Sampling model outputs for error analysis."""
            print("Error Analysis Example...")
            
            # Train a small model first
            trainer = basic_training_setup()
            
            if trainer is None:
                print("Skipping error analysis due to trainer initialization issue")
                print("Using pre-trained model for generation example instead...")
                
                # Use pre-trained model directly
                model_name = "distilgpt2"
                text_generator = pipeline(
                    "text-generation",
                    model=model_name,
                    device=0 if get_device() == "cuda" else -1
                )
            else:
                trainer.args.max_steps = 10
                trainer.train()
                
                # Save the model
                output_dir = "./results/checkpoint-final"
                trainer.save_model(output_dir)
                
                # Load for generation
                text_generator = pipeline(
                    "text-generation",
                    model=output_dir,
                    tokenizer=trainer.tokenizer,
                    device=0 if get_device() == "cuda" else -1
                )
            
            # Test prompts
            prompts = [
                "The patient presented with",
                "Diagnosis confirmed",
                "Treatment included"
            ]
            
            print("\nGenerating samples for error analysis:")
            for prompt in prompts:
                output = text_generator(
                    prompt,
                    max_length=50,
                    num_return_sequences=1,
                    temperature=0.8,
                    pad_token_id=text_generator.tokenizer.eos_token_id
                )
                print(f"\nPrompt: {prompt}")
                print(f"Generated: {output[0]['generated_text']}")
            
            print("\nError analysis tips:")
            print("- Check for repetition or nonsensical output")
            print("- Verify domain-specific terms are used correctly")
            print("- Note any biases or inappropriate content")
            print("- Consider if more training data is needed")


        def run_training_workflow_examples():
            """Run all training workflow examples."""
            
            print("1. Basic Training Setup")
            print("-" * 40)
            trainer = basic_training_setup()
            
            print("\n\n2. Training with Metrics")
            print("-" * 40)
            training_with_metrics()
            
            print("\n\n3. Early Stopping")
            print("-" * 40)
            early_stopping_example()
            
            print("\n\n4. Mixed Precision Training")
            print("-" * 40)
            mixed_precision_training()
            
            print("\n\n5. Error Analysis")
            print("-" * 40)
            error_analysis()


        if __name__ == "__main__":
            run_training_workflow_examples()
      metadata:
        extension: .py
        size_bytes: 9826
        language: python
    src/chain_of_thought.py:
      content: |-
        """Chain of Thought Examples."""

        from transformers import pipeline
        from config import get_device


        def basic_chain_of_thought():
            """Basic chain of thought reasoning example."""
            print("Basic Chain of Thought Example...")
            
            generator = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if get_device() == "cuda" else -1
            )
            
            # Math problem with step-by-step reasoning
            prompt = """Solve step by step:

        Q: If a shirt costs $20 and is on sale for 25% off, what is the final price?
        A: Let me solve this step by step:
        1. The shirt costs $20
        2. The discount is 25% of $20 = 0.25 × $20 = $5
        3. The final price = $20 - $5 = $15
        Therefore, the final price is $15.

        Q: A recipe needs 3 cups of flour for 12 cookies. How much flour is needed for 20 cookies?
        A: Let me solve this step by step:
        1. For 12 cookies, we need 3 cups of flour
        2. For 1 cookie, we need 3/12 = 0.25 cups of flour
        3. For 20 cookies, we need 0.25 × 20 = 5 cups of flour
        Therefore, we need 5 cups of flour for 20 cookies.

        Q: If a train travels 60 miles in 45 minutes, what is its speed in miles per hour?
        A: Let me solve this step by step:"""

            output = generator(
                prompt,
                max_new_tokens=100,
                temperature=0.3,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Chain of Thought Prompt:")
            print(prompt)
            print("\nModel's reasoning:", output[0]['generated_text'][len(prompt):])


        def medical_reasoning_cot():
            """Chain of thought for medical reasoning."""
            print("Medical Reasoning with Chain of Thought...")
            
            generator = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if get_device() == "cuda" else -1
            )
            
            prompt = """Diagnose based on symptoms using step-by-step reasoning:

        Patient: 45-year-old male presenting with chest pain and shortness of breath
        Analysis: Let me evaluate step by step:
        1. Key symptoms: chest pain + shortness of breath
        2. These are cardinal symptoms of cardiac issues
        3. Age (45) puts patient in risk category for heart disease
        4. Most likely: Acute coronary syndrome
        5. Immediate actions needed: ECG, cardiac enzymes, oxygen
        Conclusion: Possible myocardial infarction, requires immediate cardiac evaluation.

        Patient: 28-year-old female with severe headache, fever, and neck stiffness
        Analysis: Let me evaluate step by step:"""

            output = generator(
                prompt,
                max_new_tokens=120,
                temperature=0.3,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Medical Chain of Thought:")
            print(prompt)
            print("\nModel's analysis:", output[0]['generated_text'][len(prompt):])


        def logical_reasoning_cot():
            """Chain of thought for logical reasoning problems."""
            print("Logical Reasoning with Chain of Thought...")
            
            generator = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if get_device() == "cuda" else -1
            )
            
            prompt = """Solve the logic puzzle step by step:

        Puzzle: Three friends (Alice, Bob, Carol) have different pets (cat, dog, bird). 
        - Alice doesn't have the dog
        - The person with the cat lives next to Bob
        - Carol doesn't have the bird

        Solution: Let me work through this step by step:
        1. Alice doesn't have the dog, so Alice has either cat or bird
        2. Carol doesn't have the bird, so Carol has either cat or dog
        3. The person with the cat lives next to Bob, so Bob doesn't have the cat
        4. From step 3: Bob has either dog or bird
        5. If Carol had the cat, that would work with clue 2
        6. This means: Carol has cat, Bob has bird, Alice has dog
        Wait, but Alice can't have the dog (clue 1)
        7. So Carol must have the dog, Bob has the bird, Alice has the cat
        Final answer: Alice-cat, Bob-bird, Carol-dog

        Puzzle: Four people are in a line. Sarah is not first or last. Tom is somewhere after Sarah. Mike is not next to Tom. Where is everyone?

        Solution: Let me work through this step by step:"""

            output = generator(
                prompt,
                max_new_tokens=150,
                temperature=0.3,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Logical Reasoning Chain of Thought:")
            print(prompt)
            print("\nModel's solution:", output[0]['generated_text'][len(prompt):])


        def run_chain_of_thought_examples():
            """Run all chain of thought examples."""
            
            print("Running chain of thought examples...")
            print("These show step-by-step reasoning patterns.\n")
            
            basic_chain_of_thought()
            print("\n" + "-" * 60 + "\n")
            
            medical_reasoning_cot()
            print("\n" + "-" * 60 + "\n")
            
            logical_reasoning_cot()


        if __name__ == "__main__":
            run_chain_of_thought_examples()
      metadata:
        extension: .py
        size_bytes: 4683
        language: python
    src/model_configuration.py:
      content: |-
        """Model Configuration and Initialization Examples."""

        from transformers import (
            GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast,
            AutoConfig, AutoModelForCausalLM, AutoTokenizer
        )
        import torch
        from config import get_device, MODELS_DIR

        # Handle optional dependencies
        try:
            from transformers import BitsAndBytesConfig
            HAS_BITSANDBYTES = True
        except ImportError:
            HAS_BITSANDBYTES = False
            
        try:
            from peft import LoraConfig, get_peft_model, TaskType
            HAS_PEFT = True
        except ImportError:
            HAS_PEFT = False


        def configure_gpt2_from_scratch():
            """Configure a GPT-2 model from scratch."""
            print("Configuring GPT-2 from Scratch...")
            
            # Use modern config parameter names
            config = GPT2Config(
                vocab_size=30000,                # Match your tokenizer's vocab size
                max_position_embeddings=512,     # Max sequence length
                n_embd=768,                      # Embedding size
                n_layer=12,                      # Number of transformer layers
                n_head=12,                       # Number of attention heads
                use_cache=True                   # Enable caching for faster generation
            )
            
            model = GPT2LMHeadModel(config)
            
            # Sanity check: vocab size should match embedding matrix
            assert config.vocab_size == model.transformer.wte.weight.shape[0], "Vocab size mismatch!"
            
            print(f"Model initialized with:")
            print(f"  - Vocab size: {config.vocab_size}")
            print(f"  - Max position embeddings: {config.max_position_embeddings}")
            print(f"  - Hidden size: {config.n_embd}")
            print(f"  - Layers: {config.n_layer}")
            print(f"  - Attention heads: {config.n_head}")
            print(f"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}")
            
            return model, config


        def load_and_adapt_pretrained():
            """Load and adapt a pre-trained GPT-2 model."""
            print("Loading and Adapting Pre-trained GPT-2...")
            
            tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
            model = GPT2LMHeadModel.from_pretrained("gpt2")
            
            print(f"Original vocab size: {len(tokenizer)}")
            
            # Add new domain-specific tokens
            new_tokens = ["<medical>", "<diagnosis>", "<treatment>", "<patient>"]
            num_added = tokenizer.add_tokens(new_tokens)
            
            if num_added > 0:
                model.resize_token_embeddings(len(tokenizer))
                print(f"Added {num_added} new tokens")
                print(f"New vocab size: {len(tokenizer)}")
            
            # Test the new tokens
            test_text = "<patient> presented with <diagnosis> requiring <treatment>"
            tokens = tokenizer.tokenize(test_text)
            print(f"\nTest text: {test_text}")
            print(f"Tokens: {tokens}")
            
            return model, tokenizer


        def configure_modern_llm():
            """Configure a modern LLM (like Llama-2) for fine-tuning."""
            print("Configuring Modern LLM...")
            
            # For demonstration, we'll use a smaller model
            model_name = "microsoft/phi-2"
            
            print(f"Loading configuration for {model_name}...")
            config = AutoConfig.from_pretrained(model_name)
            
            print("\nModel Architecture:")
            print(f"  - Model type: {config.model_type}")
            print(f"  - Hidden size: {config.hidden_size}")
            print(f"  - Num layers: {config.num_hidden_layers}")
            print(f"  - Num attention heads: {config.num_attention_heads}")
            print(f"  - Vocab size: {config.vocab_size}")
            
            # In practice, load with pre-trained weights
            print("\nFor fine-tuning, load with pre-trained weights:")
            print(f'model = AutoModelForCausalLM.from_pretrained("{model_name}")')
            
            return config


        def parameter_efficient_finetuning():
            """Demonstrate parameter-efficient fine-tuning with LoRA."""
            print("Parameter-Efficient Fine-Tuning with LoRA...")
            
            if not HAS_PEFT:
                print("Skipping: PEFT library not available")
                print("Note: bitsandbytes requires scipy which may have installation issues")
                return
            
            # Use a small model for demonstration
            model_name = "gpt2"
            
            print(f"Loading {model_name}...")
            model = AutoModelForCausalLM.from_pretrained(model_name)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # Count original parameters
            original_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            print(f"\nOriginal model:")
            print(f"  - Total parameters: {original_params:,}")
            print(f"  - Trainable parameters: {trainable_params:,}")
            
            # Configure LoRA
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=8,  # LoRA rank
                lora_alpha=32,
                lora_dropout=0.1,
                target_modules=["c_attn", "c_proj"]  # GPT-2 attention layers
            )
            
            # Apply LoRA
            model = get_peft_model(model, peft_config)
            
            # Count LoRA parameters
            lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            print(f"\nWith LoRA:")
            print(f"  - Trainable parameters: {lora_params:,}")
            print(f"  - Reduction: {(1 - lora_params/original_params)*100:.2f}%")
            
            # Show trainable parameters
            model.print_trainable_parameters()
            
            return model, tokenizer


        def inspect_model_architecture():
            """Inspect model parameters and architecture."""
            print("Inspecting Model Architecture...")
            
            model = GPT2LMHeadModel.from_pretrained("gpt2")
            
            print("Model Architecture:")
            print(model)
            
            print("\n\nEmbedding Shapes:")
            print(f"Token embeddings: {model.transformer.wte.weight.shape}")
            print(f"Position embeddings: {model.transformer.wpe.weight.shape}")
            
            # Verify alignment
            config = model.config
            assert model.transformer.wte.weight.shape == (config.vocab_size, config.n_embd), \
                "Token embedding shape mismatch!"
            assert model.transformer.wpe.weight.shape == (config.n_positions, config.n_embd), \
                "Position embedding shape mismatch!"
            
            print("\nShape verification passed!")


        def run_model_configuration_examples():
            """Run all model configuration examples."""
            
            print("1. Configure GPT-2 from Scratch")
            print("-" * 40)
            configure_gpt2_from_scratch()
            
            print("\n\n2. Load and Adapt Pre-trained Model")
            print("-" * 40)
            load_and_adapt_pretrained()
            
            print("\n\n3. Configure Modern LLM")
            print("-" * 40)
            configure_modern_llm()
            
            print("\n\n4. Parameter-Efficient Fine-Tuning")
            print("-" * 40)
            parameter_efficient_finetuning()
            
            print("\n\n5. Inspect Model Architecture")
            print("-" * 40)
            inspect_model_architecture()


        if __name__ == "__main__":
            run_model_configuration_examples()
      metadata:
        extension: .py
        size_bytes: 6688
        language: python
    src/__init__.py:
      content: |
        """
        Chapter 11 Examples: Advanced Transformer Techniques
        """

        __version__ = "0.1.0"
      metadata:
        extension: .py
        size_bytes: 84
        language: python
    src/tokenization.py:
      content: |-
        """Tokenization and Vocabulary Creation Examples."""

        from transformers import AutoTokenizer, PreTrainedTokenizerFast
        from tokenizers import Tokenizer, models, pre_tokenizers, trainers
        from config import DATA_DIR
        import os
        from pathlib import Path
        from typing import List, Optional


        def train_custom_tokenizer():
            """Train a custom SentencePiece Unigram tokenizer."""
            print("Training Custom Tokenizer...")
            
            # Validate DATA_DIR exists
            if not DATA_DIR.exists():
                raise FileNotFoundError(f"Data directory does not exist: {DATA_DIR}")
            
            # Create sample corpus
            corpus_path = DATA_DIR / "sample_corpus.txt"
            sample_texts = [
                "myocardial infarction is a serious condition",
                "The patient presented with acute myocardial symptoms",
                "Diagnosis confirmed myocardial damage",
                "Treatment for myocardial conditions includes medication",
                "Post-myocardial care is essential"
            ]
            
            # Validate sample texts
            if not sample_texts:
                raise ValueError("Sample texts cannot be empty")
            
            # Write sample corpus with validation
            try:
                with open(corpus_path, 'w', encoding='utf-8') as f:
                    for text in sample_texts:
                        if not isinstance(text, str):
                            raise TypeError(f"Expected string, got {type(text)}")
                        f.write(text + '\n')
            except IOError as e:
                raise IOError(f"Failed to write corpus file: {e}")
            
            # Initialize a Unigram model
            tokenizer = Tokenizer(models.Unigram())
            tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
            
            # Configure trainer
            trainer = trainers.UnigramTrainer(
                vocab_size=1000,
                special_tokens=["<pad>", "<unk>", "<s>", "</s>"]
            )
            
            # Train tokenizer with validation
            if not corpus_path.exists():
                raise FileNotFoundError(f"Corpus file not found: {corpus_path}")
            
            tokenizer.train([str(corpus_path)], trainer)
            
            # Save tokenizer with validation
            tokenizer_path = DATA_DIR / "custom-tokenizer.json"
            
            try:
                tokenizer.save(str(tokenizer_path))
                print(f"Tokenizer saved to: {tokenizer_path}")
            except Exception as e:
                raise IOError(f"Failed to save tokenizer: {e}")
            
            # Load into Hugging Face
            hf_tokenizer = PreTrainedTokenizerFast(
                tokenizer_file=str(tokenizer_path),
                unk_token="<unk>",
                pad_token="<pad>",
                cls_token="<s>",
                sep_token="</s>",
                mask_token="<mask>"
            )
            
            # Test tokenization
            test_text = "myocardial infarction treatment"
            tokens = hf_tokenizer.tokenize(test_text)
            print(f"\nTest text: {test_text}")
            print(f"Tokens: {tokens}")
            
            return hf_tokenizer


        def compare_tokenizers():
            """Compare pre-trained vs custom tokenizer."""
            print("Comparing Tokenizers...")
            
            # Validate DATA_DIR exists
            if not DATA_DIR.exists():
                raise FileNotFoundError(f"Data directory does not exist: {DATA_DIR}")
            
            # Load pre-trained tokenizer
            pretrained = AutoTokenizer.from_pretrained("bert-base-uncased")
            
            # Load existing custom tokenizer if available, otherwise train new one
            tokenizer_path = DATA_DIR / "custom-tokenizer.json"
            if tokenizer_path.exists() and tokenizer_path.is_file():
                print("Loading existing custom tokenizer...")
                try:
                    custom = PreTrainedTokenizerFast(
                        tokenizer_file=str(tokenizer_path),
                        unk_token="<unk>",
                        pad_token="<pad>",
                        cls_token="<s>",
                        sep_token="</s>",
                        mask_token="<mask>"
                    )
                except Exception as e:
                    print(f"Failed to load custom tokenizer: {e}")
                    print("Training new tokenizer...")
                    custom = train_custom_tokenizer()
            else:
                custom = train_custom_tokenizer()
            
            # Medical terms to test
            test_sentences = [
                "myocardial infarction",
                "cardiac treatment",
                "patient diagnosis"
            ]
            
            print("\n\nTokenizer Comparison:")
            print("-" * 60)
            
            for sentence in test_sentences:
                print(f"\nText: {sentence}")
                print(f"BERT tokens: {pretrained.tokenize(sentence)}")
                print(f"Custom tokens: {custom.tokenize(sentence)}")


        def vocabulary_analysis():
            """Analyze vocabulary coverage."""
            print("Vocabulary Analysis...")
            
            # Create domain-specific corpus
            medical_terms = [
                "myocardial", "infarction", "electrocardiogram",
                "thrombolytic", "angioplasty", "stenosis",
                "arrhythmia", "bradycardia", "tachycardia"
            ]
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            
            print("\nDomain term tokenization:")
            unknown_count = 0
            
            for term in medical_terms:
                tokens = tokenizer.tokenize(term)
                token_ids = tokenizer.convert_tokens_to_ids(tokens)
                
                # Check for unknown tokens
                unk_id = tokenizer.unk_token_id
                has_unk = unk_id in token_ids
                
                if has_unk:
                    unknown_count += 1
                    
                print(f"{term:20} -> {tokens}")
                if has_unk:
                    print(f"{'':20}    Contains [UNK] token!")
            
            print(f"\nVocabulary coverage: {len(medical_terms) - unknown_count}/{len(medical_terms)} terms")
            print("Recommendation: Train custom tokenizer for better domain coverage")


        def run_tokenization_examples():
            """Run all tokenization examples."""
            
            print("1. Training Custom Tokenizer")
            print("-" * 40)
            train_custom_tokenizer()
            
            print("\n\n2. Comparing Tokenizers")
            print("-" * 40)
            compare_tokenizers()
            
            print("\n\n3. Vocabulary Analysis")
            print("-" * 40)
            vocabulary_analysis()


        if __name__ == "__main__":
            run_tokenization_examples()
      metadata:
        extension: .py
        size_bytes: 5842
        language: python
    src/constitutional_ai.py:
      content: |-
        """Constitutional AI Examples - Placeholder for future implementation."""

        def run_constitutional_ai_examples():
            """Run constitutional AI examples."""
            print("Constitutional AI examples are planned for future implementation.")
            print("This would demonstrate:")
            print("- AI safety principles")
            print("- Self-correction mechanisms")
            print("- Ethical constraints in generation")
            print("- Harmlessness and helpfulness balance")
            

        if __name__ == "__main__":
            run_constitutional_ai_examples()
      metadata:
        extension: .py
        size_bytes: 519
        language: python
    src/prompt_engineering.py:
      content: |
        """Prompt Engineering implementation."""

        from transformers import pipeline, AutoTokenizer, AutoModel
        import torch
        from config import get_device, DEFAULT_MODEL

        def run_prompt_engineering_examples():
            """Run prompt engineering examples."""
            
            print(f"Loading model: {DEFAULT_MODEL}")
            device = get_device()
            print(f"Using device: {device}")
            
            # Example implementation
            tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)
            model = AutoModel.from_pretrained(DEFAULT_MODEL)
            
            # Example text
            text = "Hugging Face Transformers make NLP accessible to everyone!"
            
            # Tokenize
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            
            print(f"\nInput text: {text}")
            print(f"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())}")
            print(f"Token IDs: {inputs['input_ids'][0].tolist()}")
            
            # Get model outputs
            with torch.no_grad():
                outputs = model(**inputs)
            
            print(f"\nModel output shape: {outputs.last_hidden_state.shape}")
            print("Example completed successfully!")
            
        if __name__ == "__main__":
            print("=== Prompt Engineering Examples ===\n")
            run_prompt_engineering_examples()
      metadata:
        extension: .py
        size_bytes: 1219
        language: python
    src/data_curation.py:
      content: |-
        """Data Curation and Cleaning Examples."""

        import re
        import unicodedata
        from config import DATA_DIR

        # Handle import issues gracefully
        try:
            from datasets import load_dataset, get_dataset_config_names, Dataset
            HAS_DATASETS = True
        except (ImportError, AttributeError) as e:
            print(f"Warning: datasets library import issue: {e}")
            HAS_DATASETS = False
            
        try:
            from langdetect import detect
            HAS_LANGDETECT = True
        except ImportError:
            print("Warning: langdetect not available")
            HAS_LANGDETECT = False


        def basic_data_cleaning():
            """Basic data cleaning with Hugging Face Datasets."""
            print("Loading customer logs dataset...")
            
            if not HAS_DATASETS:
                print("Skipping: datasets library not available")
                return
            
            # Create sample data for demonstration
            sample_data = {
                "text": [
                    "<p>Customer complaint: Product <b>broken</b></p>   Multiple   spaces!",
                    "<div>Great service!</div>\n\n\nExtra newlines",
                    "Normal text without HTML"
                ]
            }
            
            dataset = Dataset.from_dict(sample_data)
            
            def clean_text(example):
                # Remove HTML tags
                text = re.sub(r'<.*?>', '', example["text"])
                # Replace multiple spaces/newlines with a single space
                text = re.sub(r'\s+', ' ', text)
                # Strip leading/trailing whitespace
                text = text.strip()
                return {"text": text}
            
            cleaned_dataset = dataset.map(clean_text)
            
            print("\nOriginal vs Cleaned:")
            for i in range(len(dataset)):
                print(f"Original: {dataset[i]['text']}")
                print(f"Cleaned:  {cleaned_dataset[i]['text']}\n")


        def scalable_text_cleaning():
            """Scalable text cleaning and deduplication."""
            print("Scalable text cleaning example...")
            
            if not HAS_DATASETS:
                print("Skipping: datasets library not available")
                return
            
            # Create sample data that simulates Wikipedia-like content
            sample_wiki_data = {
                "text": [
                    "Python is a <b>high-level</b> programming language. Visit https://python.org for more info.",
                    "Machine   learning   involves    multiple    spaces and <i>algorithms</i>.",
                    "Natural language processing (NLP) is a field of AI. See https://example.com/nlp",
                ]
            }
            
            try:
                dataset = Dataset.from_dict(sample_wiki_data)
            except Exception as e:
                print(f"Error creating dataset: {e}")
                return
            
            def clean_text(example):
                text = unicodedata.normalize('NFKC', example['text'])  # Unicode normalization
                text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
                text = re.sub(r'https?://\S+', '', text)  # Remove URLs
                text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
                text = text.strip()
                return {"text": text}
            
            # Apply cleaning to dataset
            cleaned_dataset = dataset.map(clean_text)
            
            print("\nCleaning examples:")
            for i in range(len(dataset)):
                print(f"\nExample {i+1}:")
                print(f"Original: {dataset[i]['text']}")
                print(f"Cleaned:  {cleaned_dataset[i]['text']}")


        def language_detection_filtering():
            """Automated language detection and filtering."""
            print("Demonstrating language detection...")
            
            if not HAS_DATASETS:
                print("Skipping: datasets library not available")
                return
                
            if not HAS_LANGDETECT:
                print("Skipping: langdetect not available")
                return
            
            # Sample multilingual data
            sample_texts = [
                "This is an English sentence.",
                "Ceci est une phrase en français.",
                "Dies ist ein deutscher Satz.",
                "This is another English example.",
                "これは日本語の文です。"
            ]
            
            dataset = Dataset.from_dict({"text": sample_texts})
            
            def filter_english(example):
                try:
                    return detect(example['text']) == 'en'
                except (ImportError, LookupError, ValueError) as e:
                    # LookupError: langdetect couldn't detect language
                    # ValueError: invalid input text
                    print(f"Language detection error: {e}")
                    return False
            
            # Filter for English only
            english_dataset = dataset.filter(filter_english)
            
            print("\nOriginal texts:")
            for text in dataset['text']:
                try:
                    lang = detect(text)
                except (ImportError, LookupError, ValueError):
                    lang = "unknown"
                print(f"  [{lang}] {text}")
            
            print("\nFiltered (English only):")
            for text in english_dataset['text']:
                print(f"  {text}")


        def pii_redaction():
            """Simple PII redaction example."""
            print("PII Redaction Example...")
            
            def redact_pii(text):
                # Apply patterns in specific order to avoid conflicts
                
                # SSN pattern (xxx-xx-xxxx) - do this before phone numbers
                text = re.sub(
                    r'\b\d{3}-\d{2}-\d{4}\b',
                    '[SSN]',
                    text
                )
                
                # Credit card patterns (basic - 16 digits with optional spaces/dashes)
                text = re.sub(
                    r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
                    '[CREDIT_CARD]',
                    text
                )
                
                # Improved email pattern - handles more complex email formats
                text = re.sub(
                    r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                    '[EMAIL]',
                    text
                )
                
                # Improved phone patterns - handles various formats
                # US phone numbers: (xxx) xxx-xxxx, xxx-xxx-xxxx, xxx.xxx.xxxx, etc.
                text = re.sub(
                    r'(\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b',
                    '[PHONE]',
                    text
                )
                
                # International phone formats (more specific to avoid catching SSN/CC)
                text = re.sub(
                    r'\+[0-9]{1,3}[-.\s]?\(?[0-9]{1,4}\)?[-.\s]?[0-9]{1,4}[-.\s]?[0-9]{1,9}',
                    '[PHONE]',
                    text
                )
                
                # Improved name patterns - handles more titles and name formats
                # Titles with first and last names
                text = re.sub(
                    r'\b(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.|Rev\.)\s+[A-Z][a-z]+\s+[A-Z][a-z]+\b',
                    '[NAME]',
                    text
                )
                
                # Single title with name
                text = re.sub(
                    r'\b(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.|Rev\.)\s+[A-Z][a-z]+\b',
                    '[NAME]',
                    text
                )
                
                return text
            
            samples = [
                "Contact Dr. Smith at dr.smith@example.com or 555-123-4567.",
                "Please email john.doe@company.com for support.",
                "Ms. Johnson can be reached at 123-456-7890.",
                "Prof. John Williams called from +1 (800) 555-1234.",
                "SSN: 123-45-6789 and card: 4532-1234-5678-9012"
            ]
            
            print("\nBefore and after PII redaction:")
            for sample in samples:
                print(f"Original: {sample}")
                print(f"Redacted: {redact_pii(sample)}\n")


        def streaming_batch_processing():
            """Streaming and batch processing example."""
            print("Streaming and Batch Processing Example...")
            
            if not HAS_DATASETS:
                print("Skipping: datasets library not available")
                return
            
            # Create a larger sample dataset
            large_sample = {
                "text": [
                    "This is a long text about machine learning. " * 50,  # Repeated text
                    "Natural language processing involves many techniques. " * 50,
                    "Deep learning has revolutionized AI. " * 50,
                    "Transformers are powerful models. " * 50,
                    "Data science requires many skills. " * 50,
                ]
            }
            
            dataset = Dataset.from_dict(large_sample)
            
            def process_batch(batch):
                # Example batch processing (e.g., truncating text)
                return {"processed_text": [t[:200] for t in batch["text"]]}
            
            # Process data in batches
            processed = dataset.map(process_batch, batched=True, batch_size=2)
            
            print("\nProcessing examples (truncated to 200 chars):")
            for i in range(min(3, len(processed))):
                print(f"\nExample {i+1}:")
                print(f"Original length: {len(dataset[i]['text'])} chars")
                print(f"Processed: {processed[i]['processed_text']}...")


        def run_data_curation_examples():
            """Run all data curation examples."""
            
            print("1. Basic Data Cleaning")
            print("-" * 40)
            basic_data_cleaning()
            
            print("\n\n2. Scalable Text Cleaning")
            print("-" * 40)
            scalable_text_cleaning()
            
            print("\n\n3. Language Detection and Filtering")
            print("-" * 40)
            language_detection_filtering()
            
            print("\n\n4. PII Redaction")
            print("-" * 40)
            pii_redaction()
            
            print("\n\n5. Streaming and Batch Processing")
            print("-" * 40)
            streaming_batch_processing()


        if __name__ == "__main__":
            run_data_curation_examples()
      metadata:
        extension: .py
        size_bytes: 8841
        language: python
    src/main.py:
      content: |
        """Main entry point for all examples."""

        import sys
        from pathlib import Path

        # Add src to path
        sys.path.append(str(Path(__file__).parent))

        from prompt_engineering import run_prompt_engineering_examples
        from few_shot_learning import run_few_shot_learning_examples
        from chain_of_thought import run_chain_of_thought_examples
        from constitutional_ai import run_constitutional_ai_examples
        from data_curation import run_data_curation_examples
        from tokenization import run_tokenization_examples
        from model_configuration import run_model_configuration_examples
        from training_workflow import run_training_workflow_examples

        def print_section(title: str):
            """Print a formatted section header."""
            print("\n" + "=" * 60)
            print(f"  {title}")
            print("=" * 60 + "\n")

        def main():
            """Run all examples."""
            print_section("CHAPTER 11: DATASET CURATION & TRAINING LLMs")
            print("Welcome! This script demonstrates building custom language models.")
            print("From data curation to training workflows.\n")
            
            print_section("1. DATA CURATION & CLEANING")
            run_data_curation_examples()
            
            print_section("2. TOKENIZATION & VOCABULARY")
            run_tokenization_examples()
            
            print_section("3. MODEL CONFIGURATION")
            run_model_configuration_examples()
            
            print_section("4. TRAINING WORKFLOWS")
            run_training_workflow_examples()
            
            print_section("5. PROMPT ENGINEERING")
            run_prompt_engineering_examples()
            
            print_section("6. FEW SHOT LEARNING")
            run_few_shot_learning_examples()
            
            print_section("7. CHAIN OF THOUGHT")
            run_chain_of_thought_examples()
            
            print_section("CONCLUSION")
            print("You've learned the complete pipeline from data to deployment!")
            print("Next steps: Try fine-tuning on your own domain-specific data.")

        if __name__ == "__main__":
            main()
      metadata:
        extension: .py
        size_bytes: 1839
        language: python
    src/few_shot_learning.py:
      content: |-
        """Few-shot Learning Examples."""

        from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
        import torch
        from config import get_device


        def basic_few_shot_example():
            """Basic few-shot learning example with GPT-2."""
            print("Basic Few-Shot Learning Example...")
            
            # Load model
            model_name = "gpt2"
            generator = pipeline(
                "text-generation",
                model=model_name,
                device=0 if get_device() == "cuda" else -1
            )
            
            # Few-shot prompt for sentiment classification
            prompt = """Classify the sentiment of these reviews as positive or negative.

        Review: The product arrived quickly and works perfectly.
        Sentiment: positive

        Review: Terrible quality, broke after one day.
        Sentiment: negative

        Review: Amazing value for money, highly recommend!
        Sentiment: positive

        Review: Complete waste of money, very disappointed.
        Sentiment:"""

            # Generate
            output = generator(
                prompt,
                max_new_tokens=5,
                temperature=0.1,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Few-shot prompt:")
            print(prompt)
            print("\nModel output:", output[0]['generated_text'][len(prompt):].strip())


        def domain_specific_few_shot():
            """Few-shot learning for domain-specific tasks."""
            print("Domain-Specific Few-Shot Learning...")
            
            generator = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if get_device() == "cuda" else -1
            )
            
            # Medical diagnosis few-shot example
            prompt = """Based on symptoms, suggest possible conditions:

        Symptoms: Chest pain, shortness of breath, sweating
        Possible condition: Myocardial infarction (heart attack)

        Symptoms: Frequent urination, excessive thirst, fatigue
        Possible condition: Diabetes mellitus

        Symptoms: Severe headache, stiff neck, sensitivity to light
        Possible condition: Meningitis

        Symptoms: Persistent cough, fever, difficulty breathing
        Possible condition:"""

            output = generator(
                prompt,
                max_new_tokens=10,
                temperature=0.3,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Medical few-shot prompt:")
            print(prompt)
            print("\nModel suggestion:", output[0]['generated_text'][len(prompt):].strip())


        def structured_output_few_shot():
            """Few-shot learning for structured output generation."""
            print("Structured Output Few-Shot Example...")
            
            generator = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if get_device() == "cuda" else -1
            )
            
            # JSON generation few-shot
            prompt = """Convert natural language to JSON format:

        Input: John Smith is 30 years old and lives in New York
        Output: {"name": "John Smith", "age": 30, "city": "New York"}

        Input: The product costs $49.99 and weighs 2.5 kg
        Output: {"price": 49.99, "weight": 2.5, "unit": "kg"}

        Input: Meeting scheduled for March 15 at 2:30 PM in Room 101
        Output:"""

            output = generator(
                prompt,
                max_new_tokens=30,
                temperature=0.2,
                pad_token_id=generator.tokenizer.eos_token_id
            )
            
            print("Structured output prompt:")
            print(prompt)
            print("\nGenerated JSON:", output[0]['generated_text'][len(prompt):].strip())


        def run_few_shot_learning_examples():
            """Run all few-shot learning examples."""
            
            print("Running few-shot learning examples...")
            print("These demonstrate in-context learning without fine-tuning.\n")
            
            basic_few_shot_example()
            print("\n" + "-" * 60 + "\n")
            
            domain_specific_few_shot()
            print("\n" + "-" * 60 + "\n")
            
            structured_output_few_shot()


        if __name__ == "__main__":
            run_few_shot_learning_examples()
      metadata:
        extension: .py
        size_bytes: 3678
        language: python
